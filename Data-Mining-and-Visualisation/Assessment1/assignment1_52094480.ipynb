{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A1_DMV.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjSgPr3n7T2O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4636104f-9f71-4696-ff9b-64a551ed3433"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import os #dealing with directories \n",
        "from random import shuffle #to shuffle the data randomly\n",
        "import cv2 #for resizing the images\n",
        "from PIL import Image\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import categorical_crossentropy\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBXgVwm1y4QW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6ef518a-84d1-4a6b-dd99-80df776dbc42"
      },
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "def load_images(directory):\n",
        "    covid_images = []\n",
        "    for filename in os.listdir(covid_19):\n",
        "        img = cv2.imread(os.path.join(covid_19,filename))\n",
        "        if img is not None:\n",
        "            covid_images.append(img)\n",
        "               \n",
        "    return covid_images\n",
        "       \n",
        "covid_19 = \"/content/drive/MyDrive/Masters/Semester 2/Data Mining & Visualisation/Assessments/Assessment1/Code/covid19_lungs_dataset/covid_19\"\n",
        "covid_images = load_images(covid_19)\n",
        "covid_images"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[176, 176, 176],\n",
              "         [173, 173, 173],\n",
              "         [173, 173, 173],\n",
              "         ...,\n",
              "         [135, 135, 135],\n",
              "         [134, 134, 134],\n",
              "         [133, 133, 133]],\n",
              " \n",
              "        [[178, 178, 178],\n",
              "         [179, 179, 179],\n",
              "         [179, 179, 179],\n",
              "         ...,\n",
              "         [137, 137, 137],\n",
              "         [136, 136, 136],\n",
              "         [135, 135, 135]],\n",
              " \n",
              "        [[178, 178, 178],\n",
              "         [179, 179, 179],\n",
              "         [179, 179, 179],\n",
              "         ...,\n",
              "         [137, 137, 137],\n",
              "         [136, 136, 136],\n",
              "         [135, 135, 135]]], dtype=uint8), array([[[12, 12, 12],\n",
              "         [12, 12, 12],\n",
              "         [12, 12, 12],\n",
              "         ...,\n",
              "         [42, 42, 42],\n",
              "         [42, 42, 42],\n",
              "         [40, 40, 40]],\n",
              " \n",
              "        [[12, 12, 12],\n",
              "         [12, 12, 12],\n",
              "         [12, 12, 12],\n",
              "         ...,\n",
              "         [43, 43, 43],\n",
              "         [43, 43, 43],\n",
              "         [40, 40, 40]],\n",
              " \n",
              "        [[12, 12, 12],\n",
              "         [12, 12, 12],\n",
              "         [12, 12, 12],\n",
              "         ...,\n",
              "         [44, 44, 44],\n",
              "         [44, 44, 44],\n",
              "         [42, 42, 42]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         ...,\n",
              "         [ 8,  8,  8],\n",
              "         [ 6,  6,  6],\n",
              "         [ 8,  8,  8]],\n",
              " \n",
              "        [[ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         ...,\n",
              "         [ 3,  3,  3],\n",
              "         [ 1,  1,  1],\n",
              "         [ 3,  3,  3]],\n",
              " \n",
              "        [[ 2,  2,  2],\n",
              "         [ 2,  2,  2],\n",
              "         [ 2,  2,  2],\n",
              "         ...,\n",
              "         [ 2,  2,  2],\n",
              "         [ 1,  1,  1],\n",
              "         [ 0,  0,  0]]], dtype=uint8), array([[[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]]], dtype=uint8), array([[[148, 148, 148],\n",
              "         [156, 156, 156],\n",
              "         [164, 164, 164],\n",
              "         ...,\n",
              "         [ 74,  74,  74],\n",
              "         [106, 106, 106],\n",
              "         [ 98,  98,  98]],\n",
              " \n",
              "        [[139, 139, 139],\n",
              "         [148, 148, 148],\n",
              "         [148, 148, 148],\n",
              "         ...,\n",
              "         [ 82,  82,  82],\n",
              "         [ 90,  90,  90],\n",
              "         [ 98,  98,  98]],\n",
              " \n",
              "        [[139, 139, 139],\n",
              "         [123, 123, 123],\n",
              "         [139, 139, 139],\n",
              "         ...,\n",
              "         [ 65,  65,  65],\n",
              "         [ 49,  49,  49],\n",
              "         [106, 106, 106]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[ 24,  24,  24],\n",
              "         [  8,   8,   8],\n",
              "         [  8,   8,   8],\n",
              "         ...,\n",
              "         [ 24,  24,  24],\n",
              "         [ 16,  16,  16],\n",
              "         [ 41,  41,  41]],\n",
              " \n",
              "        [[  8,   8,   8],\n",
              "         [  0,   0,   0],\n",
              "         [  8,   8,   8],\n",
              "         ...,\n",
              "         [ 24,  24,  24],\n",
              "         [ 16,  16,  16],\n",
              "         [ 24,  24,  24]],\n",
              " \n",
              "        [[ 32,  32,  32],\n",
              "         [ 32,  32,  32],\n",
              "         [ 24,  24,  24],\n",
              "         ...,\n",
              "         [ 41,  41,  41],\n",
              "         [ 41,  41,  41],\n",
              "         [ 49,  49,  49]]], dtype=uint8), array([[[  0,   0,   0],\n",
              "         [  3,   3,   3],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [  1,   1,   1],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[  3,   3,   3],\n",
              "         [  0,   0,   0],\n",
              "         [  7,   7,   7],\n",
              "         ...,\n",
              "         [  4,   4,   4],\n",
              "         [  0,   0,   0],\n",
              "         [  1,   1,   1]],\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  9,   9,   9],\n",
              "         [114, 114, 114],\n",
              "         ...,\n",
              "         [  8,   8,   8],\n",
              "         [  1,   1,   1],\n",
              "         [  2,   2,   2]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[  5,   5,   5],\n",
              "         [  4,   4,   4],\n",
              "         [ 19,  19,  19],\n",
              "         ...,\n",
              "         [ 12,  12,  12],\n",
              "         [  1,   1,   1],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  4,   4,   4],\n",
              "         ...,\n",
              "         [  6,   6,   6],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[  3,   3,   3],\n",
              "         [  0,   0,   0],\n",
              "         [  4,   4,   4],\n",
              "         ...,\n",
              "         [  2,   2,   2],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]]], dtype=uint8), array([[[  1,   1,   1],\n",
              "         [  1,   1,   1],\n",
              "         [  1,   1,   1],\n",
              "         ...,\n",
              "         [  0,   0,   1],\n",
              "         [  0,   0,   1],\n",
              "         [  0,   0,   1]],\n",
              " \n",
              "        [[  1,   1,   1],\n",
              "         [  1,   1,   1],\n",
              "         [  1,   1,   1],\n",
              "         ...,\n",
              "         [  0,   0,   1],\n",
              "         [  0,   0,   1],\n",
              "         [  0,   0,   1]],\n",
              " \n",
              "        [[  1,   1,   1],\n",
              "         [  1,   1,   1],\n",
              "         [  1,   1,   1],\n",
              "         ...,\n",
              "         [  0,   0,   1],\n",
              "         [  0,   0,   1],\n",
              "         [  0,   0,   1]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[ 27,  27,  27],\n",
              "         [ 29,  29,  29],\n",
              "         [ 33,  33,  33],\n",
              "         ...,\n",
              "         [178, 177, 179],\n",
              "         [180, 179, 181],\n",
              "         [102, 101, 103]],\n",
              " \n",
              "        [[ 27,  27,  27],\n",
              "         [ 29,  29,  29],\n",
              "         [ 33,  33,  33],\n",
              "         ...,\n",
              "         [180, 179, 181],\n",
              "         [182, 181, 183],\n",
              "         [104, 103, 105]],\n",
              " \n",
              "        [[ 26,  26,  26],\n",
              "         [ 30,  30,  30],\n",
              "         [ 34,  34,  34],\n",
              "         ...,\n",
              "         [182, 181, 183],\n",
              "         [183, 182, 184],\n",
              "         [105, 104, 106]]], dtype=uint8), array([[[  2,   2,   2],\n",
              "         [  2,   2,   2],\n",
              "         [  1,   1,   1],\n",
              "         ...,\n",
              "         [ 38,  38,  38],\n",
              "         [ 38,  38,  38],\n",
              "         [ 38,  38,  38]],\n",
              " \n",
              "        [[  3,   3,   3],\n",
              "         [  2,   2,   2],\n",
              "         [  1,   1,   1],\n",
              "         ...,\n",
              "         [ 40,  40,  40],\n",
              "         [ 40,  40,  40],\n",
              "         [ 40,  40,  40]],\n",
              " \n",
              "        [[  3,   3,   3],\n",
              "         [  3,   3,   3],\n",
              "         [  2,   2,   2],\n",
              "         ...,\n",
              "         [ 43,  43,  43],\n",
              "         [ 42,  42,  42],\n",
              "         [ 43,  43,  43]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[145, 145, 145],\n",
              "         [144, 144, 144],\n",
              "         [144, 144, 144],\n",
              "         ...,\n",
              "         [121, 121, 121],\n",
              "         [119, 119, 119],\n",
              "         [118, 118, 118]],\n",
              " \n",
              "        [[146, 146, 146],\n",
              "         [146, 146, 146],\n",
              "         [145, 145, 145],\n",
              "         ...,\n",
              "         [120, 120, 120],\n",
              "         [119, 119, 119],\n",
              "         [119, 119, 119]],\n",
              " \n",
              "        [[146, 146, 146],\n",
              "         [146, 146, 146],\n",
              "         [145, 145, 145],\n",
              "         ...,\n",
              "         [120, 120, 120],\n",
              "         [119, 119, 119],\n",
              "         [119, 119, 119]]], dtype=uint8), array([[[1, 1, 1],\n",
              "         [1, 1, 1],\n",
              "         [1, 1, 1],\n",
              "         ...,\n",
              "         [2, 2, 2],\n",
              "         [2, 2, 2],\n",
              "         [2, 2, 2]],\n",
              " \n",
              "        [[1, 1, 1],\n",
              "         [1, 1, 1],\n",
              "         [1, 1, 1],\n",
              "         ...,\n",
              "         [2, 2, 2],\n",
              "         [2, 2, 2],\n",
              "         [2, 2, 2]],\n",
              " \n",
              "        [[1, 1, 1],\n",
              "         [1, 1, 1],\n",
              "         [1, 1, 1],\n",
              "         ...,\n",
              "         [2, 2, 2],\n",
              "         [2, 2, 2],\n",
              "         [2, 2, 2]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [1, 1, 1],\n",
              "         [2, 2, 2],\n",
              "         ...,\n",
              "         [1, 1, 1],\n",
              "         [1, 1, 1],\n",
              "         [1, 1, 1]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [1, 1, 1],\n",
              "         ...,\n",
              "         [1, 1, 1],\n",
              "         [1, 1, 1],\n",
              "         [1, 1, 1]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [1, 1, 1],\n",
              "         [1, 1, 1],\n",
              "         [1, 1, 1]]], dtype=uint8), array([[[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  1,   1,   1]],\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [  1,   1,   1],\n",
              "         [  1,   1,   1],\n",
              "         [  1,   1,   1]],\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[160, 160, 160],\n",
              "         [164, 164, 164],\n",
              "         [164, 164, 164],\n",
              "         ...,\n",
              "         [156, 156, 156],\n",
              "         [154, 154, 154],\n",
              "         [151, 151, 151]],\n",
              " \n",
              "        [[160, 160, 160],\n",
              "         [164, 164, 164],\n",
              "         [165, 165, 165],\n",
              "         ...,\n",
              "         [159, 159, 159],\n",
              "         [157, 157, 157],\n",
              "         [155, 155, 155]],\n",
              " \n",
              "        [[159, 159, 159],\n",
              "         [164, 164, 164],\n",
              "         [166, 166, 166],\n",
              "         ...,\n",
              "         [161, 161, 161],\n",
              "         [159, 159, 159],\n",
              "         [157, 157, 157]]], dtype=uint8), array([[[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[1, 1, 1],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        [[1, 1, 1],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        [[1, 1, 1],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]]], dtype=uint8), array([[[ 64,  44,  19],\n",
              "         [ 34,  17,   0],\n",
              "         [ 33,  20,   4],\n",
              "         ...,\n",
              "         [ 71,  71,  71],\n",
              "         [ 66,  66,  66],\n",
              "         [ 64,  64,  64]],\n",
              " \n",
              "        [[ 65,  45,  20],\n",
              "         [ 36,  19,   0],\n",
              "         [ 35,  22,   6],\n",
              "         ...,\n",
              "         [ 71,  71,  71],\n",
              "         [ 68,  68,  68],\n",
              "         [ 66,  66,  66]],\n",
              " \n",
              "        [[ 66,  46,  21],\n",
              "         [ 38,  21,   0],\n",
              "         [ 39,  26,  10],\n",
              "         ...,\n",
              "         [ 75,  75,  75],\n",
              "         [ 73,  73,  73],\n",
              "         [ 71,  71,  71]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[ 66,  45,  23],\n",
              "         [ 82,  65,  46],\n",
              "         [178, 165, 151],\n",
              "         ...,\n",
              "         [157, 157, 157],\n",
              "         [156, 156, 156],\n",
              "         [151, 151, 151]],\n",
              " \n",
              "        [[ 65,  44,  22],\n",
              "         [ 80,  63,  44],\n",
              "         [173, 160, 146],\n",
              "         ...,\n",
              "         [156, 156, 156],\n",
              "         [158, 158, 158],\n",
              "         [150, 150, 150]],\n",
              " \n",
              "        [[ 64,  43,  21],\n",
              "         [ 78,  61,  42],\n",
              "         [170, 157, 143],\n",
              "         ...,\n",
              "         [156, 156, 156],\n",
              "         [159, 159, 159],\n",
              "         [149, 149, 149]]], dtype=uint8), array([[[ 17,  17,  17],\n",
              "         [ 10,  10,  10],\n",
              "         [  7,   7,   7],\n",
              "         ...,\n",
              "         [  3,   3,   3],\n",
              "         [  4,   4,   4],\n",
              "         [  2,   2,   2]],\n",
              " \n",
              "        [[ 17,  17,  17],\n",
              "         [ 10,  10,  10],\n",
              "         [  7,   7,   7],\n",
              "         ...,\n",
              "         [  3,   3,   3],\n",
              "         [  4,   4,   4],\n",
              "         [  2,   2,   2]],\n",
              " \n",
              "        [[ 16,  16,  16],\n",
              "         [ 11,  11,  11],\n",
              "         [  7,   7,   7],\n",
              "         ...,\n",
              "         [  3,   3,   3],\n",
              "         [  4,   4,   4],\n",
              "         [  2,   2,   2]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[255, 255, 255],\n",
              "         [255, 255, 255],\n",
              "         [255, 255, 255],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[255, 255, 255],\n",
              "         [255, 255, 255],\n",
              "         [255, 255, 255],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[255, 255, 255],\n",
              "         [255, 255, 255],\n",
              "         [255, 255, 255],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]]], dtype=uint8), array([[[ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         ...,\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0]],\n",
              " \n",
              "        [[ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         ...,\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0]],\n",
              " \n",
              "        [[ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         ...,\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         ...,\n",
              "         [39, 39, 39],\n",
              "         [32, 32, 32],\n",
              "         [21, 21, 21]],\n",
              " \n",
              "        [[ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         ...,\n",
              "         [39, 39, 39],\n",
              "         [32, 32, 32],\n",
              "         [22, 22, 22]],\n",
              " \n",
              "        [[ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         ...,\n",
              "         [39, 39, 39],\n",
              "         [32, 32, 32],\n",
              "         [22, 22, 22]]], dtype=uint8), array([[[253, 253, 253],\n",
              "         [253, 253, 253],\n",
              "         [247, 247, 247],\n",
              "         ...,\n",
              "         [  4,   4,   4],\n",
              "         [  4,   4,   4],\n",
              "         [  5,   5,   5]],\n",
              " \n",
              "        [[254, 254, 254],\n",
              "         [255, 255, 255],\n",
              "         [237, 237, 237],\n",
              "         ...,\n",
              "         [  4,   4,   4],\n",
              "         [  4,   4,   4],\n",
              "         [  5,   5,   5]],\n",
              " \n",
              "        [[255, 255, 255],\n",
              "         [240, 240, 240],\n",
              "         [186, 186, 186],\n",
              "         ...,\n",
              "         [  4,   4,   4],\n",
              "         [  4,   4,   4],\n",
              "         [  5,   5,   5]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[ 59,  59,  59],\n",
              "         [ 59,  59,  59],\n",
              "         [ 58,  58,  58],\n",
              "         ...,\n",
              "         [ 76,  76,  76],\n",
              "         [ 71,  71,  71],\n",
              "         [ 71,  71,  71]],\n",
              " \n",
              "        [[ 59,  59,  59],\n",
              "         [ 59,  59,  59],\n",
              "         [ 58,  58,  58],\n",
              "         ...,\n",
              "         [ 80,  80,  80],\n",
              "         [ 73,  73,  73],\n",
              "         [ 72,  72,  72]],\n",
              " \n",
              "        [[ 59,  59,  59],\n",
              "         [ 59,  59,  59],\n",
              "         [ 58,  58,  58],\n",
              "         ...,\n",
              "         [ 82,  82,  82],\n",
              "         [ 75,  75,  75],\n",
              "         [ 73,  73,  73]]], dtype=uint8), array([[[13, 13, 13],\n",
              "         [15, 15, 15],\n",
              "         [15, 15, 15],\n",
              "         ...,\n",
              "         [18, 18, 18],\n",
              "         [13, 13, 13],\n",
              "         [14, 14, 14]],\n",
              " \n",
              "        [[ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 1,  1,  1],\n",
              "         ...,\n",
              "         [ 1,  1,  1],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0]],\n",
              " \n",
              "        [[ 0,  0,  0],\n",
              "         [ 3,  3,  3],\n",
              "         [ 2,  2,  2],\n",
              "         ...,\n",
              "         [ 5,  5,  5],\n",
              "         [ 0,  0,  0],\n",
              "         [ 1,  1,  1]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[ 0,  0,  0],\n",
              "         [ 5,  5,  5],\n",
              "         [ 4,  4,  4],\n",
              "         ...,\n",
              "         [30, 30, 30],\n",
              "         [22, 22, 22],\n",
              "         [ 2,  2,  2]],\n",
              " \n",
              "        [[ 0,  0,  0],\n",
              "         [ 2,  2,  2],\n",
              "         [ 4,  4,  4],\n",
              "         ...,\n",
              "         [28, 28, 28],\n",
              "         [18, 18, 18],\n",
              "         [ 0,  0,  0]],\n",
              " \n",
              "        [[ 0,  0,  0],\n",
              "         [ 1,  1,  1],\n",
              "         [ 0,  0,  0],\n",
              "         ...,\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 1,  1,  1]]], dtype=uint8), array([[[ 55,  55,  55],\n",
              "         [ 53,  53,  53],\n",
              "         [ 50,  50,  50],\n",
              "         ...,\n",
              "         [ 48,  48,  48],\n",
              "         [ 48,  48,  48],\n",
              "         [ 48,  48,  48]],\n",
              " \n",
              "        [[ 56,  56,  56],\n",
              "         [ 54,  54,  54],\n",
              "         [ 51,  51,  51],\n",
              "         ...,\n",
              "         [ 48,  48,  48],\n",
              "         [ 48,  48,  48],\n",
              "         [ 48,  48,  48]],\n",
              " \n",
              "        [[ 56,  56,  56],\n",
              "         [ 54,  54,  54],\n",
              "         [ 51,  51,  51],\n",
              "         ...,\n",
              "         [ 49,  49,  49],\n",
              "         [ 49,  49,  49],\n",
              "         [ 49,  49,  49]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[168, 168, 168],\n",
              "         [169, 169, 169],\n",
              "         [170, 170, 170],\n",
              "         ...,\n",
              "         [162, 162, 162],\n",
              "         [161, 161, 161],\n",
              "         [160, 160, 160]],\n",
              " \n",
              "        [[168, 168, 168],\n",
              "         [169, 169, 169],\n",
              "         [170, 170, 170],\n",
              "         ...,\n",
              "         [161, 161, 161],\n",
              "         [160, 160, 160],\n",
              "         [160, 160, 160]],\n",
              " \n",
              "        [[168, 168, 168],\n",
              "         [169, 169, 169],\n",
              "         [170, 170, 170],\n",
              "         ...,\n",
              "         [162, 162, 162],\n",
              "         [162, 162, 162],\n",
              "         [162, 162, 162]]], dtype=uint8), array([[[226, 226, 226],\n",
              "         [226, 226, 226],\n",
              "         [226, 226, 226],\n",
              "         ...,\n",
              "         [219, 219, 219],\n",
              "         [236, 236, 236],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[226, 226, 226],\n",
              "         [226, 226, 226],\n",
              "         [226, 226, 226],\n",
              "         ...,\n",
              "         [219, 219, 219],\n",
              "         [236, 236, 236],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[226, 226, 226],\n",
              "         [226, 226, 226],\n",
              "         [226, 226, 226],\n",
              "         ...,\n",
              "         [219, 219, 219],\n",
              "         [236, 236, 236],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[218, 218, 218],\n",
              "         [215, 215, 215],\n",
              "         [216, 216, 216],\n",
              "         ...,\n",
              "         [206, 206, 206],\n",
              "         [234, 234, 234],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[235, 235, 235],\n",
              "         [232, 232, 232],\n",
              "         [233, 233, 233],\n",
              "         ...,\n",
              "         [226, 226, 226],\n",
              "         [246, 246, 246],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  3,   3,   3],\n",
              "         [  0,   0,   0]]], dtype=uint8), array([[[  2,   2,   2],\n",
              "         [  0,   0,   0],\n",
              "         [  2,   2,   2],\n",
              "         ...,\n",
              "         [  2,   2,   2],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  3,   3,   3],\n",
              "         ...,\n",
              "         [  6,   6,   6],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[  3,   3,   3],\n",
              "         [  2,   2,   2],\n",
              "         [ 11,  11,  11],\n",
              "         ...,\n",
              "         [ 10,  10,  10],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  8,   8,   8],\n",
              "         [115, 115, 115],\n",
              "         ...,\n",
              "         [ 21,  21,  21],\n",
              "         [  1,   1,   1],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[  2,   2,   2],\n",
              "         [  0,   0,   0],\n",
              "         [ 10,  10,  10],\n",
              "         ...,\n",
              "         [  8,   8,   8],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  5,   5,   5],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [  2,   2,   2],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]]], dtype=uint8), array([[[138, 132, 113],\n",
              "         [151, 144, 127],\n",
              "         [160, 153, 136],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[138, 131, 112],\n",
              "         [151, 144, 125],\n",
              "         [158, 151, 134],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[134, 127, 107],\n",
              "         [147, 140, 121],\n",
              "         [155, 148, 129],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[  5,   1,   0],\n",
              "         [  7,   2,   1],\n",
              "         [  9,   1,   1],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[  6,   2,   1],\n",
              "         [  8,   3,   2],\n",
              "         [ 10,   2,   2],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[  8,   3,   0],\n",
              "         [ 11,   3,   3],\n",
              "         [ 11,   3,   3],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]]], dtype=uint8), array([[[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]]], dtype=uint8), array([[[  0,   4,   0],\n",
              "         [  4,   5,   3],\n",
              "         [  5,   0,   3],\n",
              "         ...,\n",
              "         [  7,   7,   7],\n",
              "         [  0,   0,   0],\n",
              "         [  2,   2,   2]],\n",
              " \n",
              "        [[  0,   3,   0],\n",
              "         [  0,   1,   0],\n",
              "         [ 12,   5,  10],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  3,   3,   3]],\n",
              " \n",
              "        [[  0,   3,   0],\n",
              "         [  5,   9,   4],\n",
              "         [189, 184, 186],\n",
              "         ...,\n",
              "         [  9,   9,   9],\n",
              "         [  2,   2,   2],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [ 11,  11,  11],\n",
              "         [107, 107, 107],\n",
              "         ...,\n",
              "         [  2,   1,   0],\n",
              "         [  3,   4,   0],\n",
              "         [  5,   8,   0]],\n",
              " \n",
              "        [[  3,   3,   3],\n",
              "         [  0,   0,   0],\n",
              "         [  4,   4,   4],\n",
              "         ...,\n",
              "         [  4,   1,   0],\n",
              "         [  2,   2,   0],\n",
              "         [  4,   5,   0]],\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  5,   5,   5],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [  4,   1,   0],\n",
              "         [  9,   8,   4],\n",
              "         [  2,   1,   0]]], dtype=uint8), array([[[ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         ...,\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0]],\n",
              " \n",
              "        [[ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         ...,\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0]],\n",
              " \n",
              "        [[ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         ...,\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[11, 11, 11],\n",
              "         [11, 11, 11],\n",
              "         [12, 12, 12],\n",
              "         ...,\n",
              "         [13, 13, 13],\n",
              "         [ 7,  7,  7],\n",
              "         [ 2,  2,  2]],\n",
              " \n",
              "        [[11, 11, 11],\n",
              "         [11, 11, 11],\n",
              "         [12, 12, 12],\n",
              "         ...,\n",
              "         [13, 13, 13],\n",
              "         [ 7,  7,  7],\n",
              "         [ 2,  2,  2]],\n",
              " \n",
              "        [[11, 11, 11],\n",
              "         [11, 11, 11],\n",
              "         [11, 11, 11],\n",
              "         ...,\n",
              "         [13, 13, 13],\n",
              "         [ 7,  7,  7],\n",
              "         [ 2,  2,  2]]], dtype=uint8), array([[[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]]], dtype=uint8), array([[[  0,   0,   0],\n",
              "         [ 22,  22,  22],\n",
              "         [ 61,  61,  61],\n",
              "         ...,\n",
              "         [  2,   2,   0],\n",
              "         [ 12,   2,   0],\n",
              "         [ 40,  26,  14]],\n",
              " \n",
              "        [[  2,   2,   2],\n",
              "         [ 26,  26,  26],\n",
              "         [ 66,  66,  66],\n",
              "         ...,\n",
              "         [  3,   3,   0],\n",
              "         [ 12,   2,   0],\n",
              "         [ 40,  26,  14]],\n",
              " \n",
              "        [[  5,   5,   5],\n",
              "         [ 31,  31,  31],\n",
              "         [ 73,  73,  73],\n",
              "         ...,\n",
              "         [  6,   6,   0],\n",
              "         [ 12,   2,   0],\n",
              "         [ 40,  26,  14]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[ 40,  24,  11],\n",
              "         [ 18,   2,   0],\n",
              "         [ 38,  22,   9],\n",
              "         ...,\n",
              "         [ 37,  19,   2],\n",
              "         [ 46,  17,   0],\n",
              "         [ 72,  40,  17]],\n",
              " \n",
              "        [[ 86,  49,  11],\n",
              "         [ 86,  49,  11],\n",
              "         [ 86,  49,  11],\n",
              "         ...,\n",
              "         [ 88,  44,  20],\n",
              "         [ 99,  45,  15],\n",
              "         [102,  45,  14]],\n",
              " \n",
              "        [[ 99,  52,   1],\n",
              "         [ 99,  52,   1],\n",
              "         [ 99,  52,   1],\n",
              "         ...,\n",
              "         [100,  44,  15],\n",
              "         [111,  44,  11],\n",
              "         [115,  44,  10]]], dtype=uint8), array([[[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]]], dtype=uint8), array([[[ 71,  71,  71],\n",
              "         [ 72,  72,  72],\n",
              "         [ 74,  74,  74],\n",
              "         ...,\n",
              "         [ 69,  69,  69],\n",
              "         [ 71,  71,  71],\n",
              "         [ 71,  71,  71]],\n",
              " \n",
              "        [[ 77,  77,  77],\n",
              "         [ 77,  77,  77],\n",
              "         [ 76,  76,  76],\n",
              "         ...,\n",
              "         [ 73,  73,  73],\n",
              "         [ 73,  73,  73],\n",
              "         [ 73,  73,  73]],\n",
              " \n",
              "        [[ 83,  83,  83],\n",
              "         [ 81,  81,  81],\n",
              "         [ 78,  78,  78],\n",
              "         ...,\n",
              "         [ 79,  79,  79],\n",
              "         [ 70,  70,  70],\n",
              "         [ 70,  70,  70]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[187, 187, 187],\n",
              "         [188, 188, 188],\n",
              "         [188, 188, 188],\n",
              "         ...,\n",
              "         [133, 133, 133],\n",
              "         [134, 134, 134],\n",
              "         [134, 134, 134]],\n",
              " \n",
              "        [[186, 186, 186],\n",
              "         [187, 187, 187],\n",
              "         [187, 187, 187],\n",
              "         ...,\n",
              "         [135, 135, 135],\n",
              "         [131, 131, 131],\n",
              "         [131, 131, 131]],\n",
              " \n",
              "        [[186, 186, 186],\n",
              "         [186, 186, 186],\n",
              "         [186, 186, 186],\n",
              "         ...,\n",
              "         [133, 133, 133],\n",
              "         [129, 129, 129],\n",
              "         [129, 129, 129]]], dtype=uint8), array([[[ 92,  92,  92],\n",
              "         [ 96,  96,  96],\n",
              "         [102, 102, 102],\n",
              "         ...,\n",
              "         [ 74,  74,  74],\n",
              "         [ 66,  66,  66],\n",
              "         [ 66,  66,  66]],\n",
              " \n",
              "        [[ 93,  93,  93],\n",
              "         [ 96,  96,  96],\n",
              "         [100, 100, 100],\n",
              "         ...,\n",
              "         [ 77,  77,  77],\n",
              "         [ 74,  74,  74],\n",
              "         [ 74,  74,  74]],\n",
              " \n",
              "        [[ 95,  95,  95],\n",
              "         [ 95,  95,  95],\n",
              "         [ 96,  96,  96],\n",
              "         ...,\n",
              "         [ 84,  84,  84],\n",
              "         [ 82,  82,  82],\n",
              "         [ 82,  82,  82]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[121, 121, 121],\n",
              "         [122, 122, 122],\n",
              "         [125, 125, 125],\n",
              "         ...,\n",
              "         [120, 120, 120],\n",
              "         [120, 120, 120],\n",
              "         [120, 120, 120]],\n",
              " \n",
              "        [[121, 121, 121],\n",
              "         [122, 122, 122],\n",
              "         [125, 125, 125],\n",
              "         ...,\n",
              "         [120, 120, 120],\n",
              "         [120, 120, 120],\n",
              "         [120, 120, 120]],\n",
              " \n",
              "        [[121, 121, 121],\n",
              "         [122, 122, 122],\n",
              "         [125, 125, 125],\n",
              "         ...,\n",
              "         [120, 120, 120],\n",
              "         [120, 120, 120],\n",
              "         [120, 120, 120]]], dtype=uint8), array([[[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  4,   4,   4],\n",
              "         ...,\n",
              "         [131, 131, 131],\n",
              "         [124, 124, 124],\n",
              "         [125, 125, 125]],\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  2,   2,   2],\n",
              "         [  3,   3,   3],\n",
              "         ...,\n",
              "         [130, 130, 130],\n",
              "         [123, 123, 123],\n",
              "         [124, 124, 124]],\n",
              " \n",
              "        [[  3,   3,   3],\n",
              "         [  3,   3,   3],\n",
              "         [  2,   2,   2],\n",
              "         ...,\n",
              "         [130, 130, 130],\n",
              "         [125, 125, 125],\n",
              "         [127, 127, 127]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[136, 136, 136],\n",
              "         [134, 134, 134],\n",
              "         [131, 131, 131],\n",
              "         ...,\n",
              "         [162, 162, 162],\n",
              "         [163, 163, 163],\n",
              "         [167, 167, 167]],\n",
              " \n",
              "        [[136, 136, 136],\n",
              "         [134, 134, 134],\n",
              "         [131, 131, 131],\n",
              "         ...,\n",
              "         [162, 162, 162],\n",
              "         [163, 163, 163],\n",
              "         [167, 167, 167]],\n",
              " \n",
              "        [[136, 136, 136],\n",
              "         [134, 134, 134],\n",
              "         [131, 131, 131],\n",
              "         ...,\n",
              "         [163, 163, 163],\n",
              "         [164, 164, 164],\n",
              "         [168, 168, 168]]], dtype=uint8), array([[[ 52,  52,  52],\n",
              "         [ 52,  52,  52],\n",
              "         [ 52,  52,  52],\n",
              "         ...,\n",
              "         [ 50,  50,  50],\n",
              "         [ 50,  50,  50],\n",
              "         [ 50,  50,  50]],\n",
              " \n",
              "        [[ 52,  52,  52],\n",
              "         [ 52,  52,  52],\n",
              "         [ 52,  52,  52],\n",
              "         ...,\n",
              "         [ 50,  50,  50],\n",
              "         [ 50,  50,  50],\n",
              "         [ 50,  50,  50]],\n",
              " \n",
              "        [[ 52,  52,  52],\n",
              "         [ 52,  52,  52],\n",
              "         [ 51,  51,  51],\n",
              "         ...,\n",
              "         [ 50,  50,  50],\n",
              "         [ 49,  49,  49],\n",
              "         [ 49,  49,  49]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[171, 171, 171],\n",
              "         [171, 171, 171],\n",
              "         [171, 171, 171],\n",
              "         ...,\n",
              "         [150, 150, 150],\n",
              "         [151, 151, 151],\n",
              "         [151, 151, 151]],\n",
              " \n",
              "        [[171, 171, 171],\n",
              "         [171, 171, 171],\n",
              "         [171, 171, 171],\n",
              "         ...,\n",
              "         [150, 150, 150],\n",
              "         [151, 151, 151],\n",
              "         [151, 151, 151]],\n",
              " \n",
              "        [[170, 170, 170],\n",
              "         [170, 170, 170],\n",
              "         [170, 170, 170],\n",
              "         ...,\n",
              "         [151, 151, 151],\n",
              "         [151, 151, 151],\n",
              "         [151, 151, 151]]], dtype=uint8), array([[[ 52,  83,  98],\n",
              "         [ 51,  82,  97],\n",
              "         [ 51,  82,  97],\n",
              "         ...,\n",
              "         [ 77, 112, 125],\n",
              "         [ 76, 111, 124],\n",
              "         [ 77, 112, 125]],\n",
              " \n",
              "        [[ 52,  83,  98],\n",
              "         [ 51,  82,  97],\n",
              "         [ 51,  82,  97],\n",
              "         ...,\n",
              "         [ 78, 113, 126],\n",
              "         [ 77, 112, 125],\n",
              "         [ 77, 112, 125]],\n",
              " \n",
              "        [[ 52,  83,  98],\n",
              "         [ 50,  81,  96],\n",
              "         [ 50,  81,  96],\n",
              "         ...,\n",
              "         [ 81, 116, 129],\n",
              "         [ 81, 116, 129],\n",
              "         [ 81, 116, 129]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[172, 178, 185],\n",
              "         [171, 177, 184],\n",
              "         [180, 186, 193],\n",
              "         ...,\n",
              "         [204, 214, 214],\n",
              "         [205, 216, 214],\n",
              "         [208, 219, 217]],\n",
              " \n",
              "        [[178, 184, 191],\n",
              "         [169, 175, 182],\n",
              "         [179, 185, 192],\n",
              "         ...,\n",
              "         [206, 216, 216],\n",
              "         [207, 218, 216],\n",
              "         [209, 220, 218]],\n",
              " \n",
              "        [[182, 188, 195],\n",
              "         [169, 175, 182],\n",
              "         [177, 183, 190],\n",
              "         ...,\n",
              "         [208, 218, 218],\n",
              "         [209, 220, 218],\n",
              "         [211, 222, 220]]], dtype=uint8), array([[[ 12,  12,  12],\n",
              "         [ 14,  14,  14],\n",
              "         [ 16,  16,  16],\n",
              "         ...,\n",
              "         [ 25,  27,  28],\n",
              "         [ 30,  32,  33],\n",
              "         [ 22,  24,  25]],\n",
              " \n",
              "        [[ 26,  26,  26],\n",
              "         [ 26,  26,  26],\n",
              "         [ 25,  25,  25],\n",
              "         ...,\n",
              "         [ 40,  42,  43],\n",
              "         [ 43,  45,  46],\n",
              "         [ 34,  36,  37]],\n",
              " \n",
              "        [[ 36,  36,  36],\n",
              "         [ 34,  34,  34],\n",
              "         [ 31,  31,  31],\n",
              "         ...,\n",
              "         [ 47,  49,  50],\n",
              "         [ 49,  51,  52],\n",
              "         [ 38,  40,  41]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[140, 145, 144],\n",
              "         [150, 155, 154],\n",
              "         [133, 138, 137],\n",
              "         ...,\n",
              "         [ 87,  89,  90],\n",
              "         [ 86,  88,  89],\n",
              "         [ 65,  67,  68]],\n",
              " \n",
              "        [[152, 155, 159],\n",
              "         [143, 146, 150],\n",
              "         [127, 130, 134],\n",
              "         ...,\n",
              "         [ 89,  82,  95],\n",
              "         [ 85,  84,  93],\n",
              "         [ 62,  67,  68]],\n",
              " \n",
              "        [[147, 150, 154],\n",
              "         [141, 144, 148],\n",
              "         [127, 130, 134],\n",
              "         ...,\n",
              "         [ 88,  83,  92],\n",
              "         [ 84,  85,  89],\n",
              "         [ 61,  67,  66]]], dtype=uint8), array([[[ 54,  54,  54],\n",
              "         [ 56,  56,  56],\n",
              "         [ 54,  54,  54],\n",
              "         ...,\n",
              "         [114, 114, 114],\n",
              "         [ 87,  87,  87],\n",
              "         [ 68,  68,  68]],\n",
              " \n",
              "        [[ 54,  54,  54],\n",
              "         [ 57,  57,  57],\n",
              "         [ 57,  57,  57],\n",
              "         ...,\n",
              "         [ 90,  90,  90],\n",
              "         [ 74,  74,  74],\n",
              "         [ 67,  67,  67]],\n",
              " \n",
              "        [[ 54,  54,  54],\n",
              "         [ 58,  58,  58],\n",
              "         [ 59,  59,  59],\n",
              "         ...,\n",
              "         [ 72,  72,  72],\n",
              "         [ 69,  69,  69],\n",
              "         [ 75,  75,  75]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]]], dtype=uint8), array([[[ 26,  26,  26],\n",
              "         [ 14,  14,  14],\n",
              "         [ 14,  14,  14],\n",
              "         ...,\n",
              "         [ 13,  13,  13],\n",
              "         [ 12,  12,  12],\n",
              "         [115, 115, 115]],\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [ 93,  93,  93]],\n",
              " \n",
              "        [[ 13,  13,  13],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [107, 107, 107]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[208, 208, 208],\n",
              "         [215, 215, 215],\n",
              "         [202, 202, 202],\n",
              "         ...,\n",
              "         [179, 179, 179],\n",
              "         [144, 144, 144],\n",
              "         [185, 185, 185]],\n",
              " \n",
              "        [[202, 202, 202],\n",
              "         [206, 206, 206],\n",
              "         [193, 193, 193],\n",
              "         ...,\n",
              "         [165, 165, 165],\n",
              "         [128, 128, 128],\n",
              "         [176, 176, 176]],\n",
              " \n",
              "        [[227, 227, 227],\n",
              "         [226, 226, 226],\n",
              "         [222, 222, 222],\n",
              "         ...,\n",
              "         [206, 206, 206],\n",
              "         [188, 188, 188],\n",
              "         [212, 212, 212]]], dtype=uint8), array([[[37, 37, 37],\n",
              "         [38, 38, 38],\n",
              "         [49, 49, 49],\n",
              "         ...,\n",
              "         [43, 43, 43],\n",
              "         [45, 45, 45],\n",
              "         [45, 45, 45]],\n",
              " \n",
              "        [[40, 40, 40],\n",
              "         [38, 38, 38],\n",
              "         [46, 46, 46],\n",
              "         ...,\n",
              "         [42, 42, 42],\n",
              "         [44, 44, 44],\n",
              "         [44, 44, 44]],\n",
              " \n",
              "        [[42, 42, 42],\n",
              "         [37, 37, 37],\n",
              "         [42, 42, 42],\n",
              "         ...,\n",
              "         [42, 42, 42],\n",
              "         [44, 44, 44],\n",
              "         [44, 44, 44]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         ...,\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0]],\n",
              " \n",
              "        [[ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         ...,\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0]],\n",
              " \n",
              "        [[ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         ...,\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0]]], dtype=uint8), array([[[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [  4,   4,   4],\n",
              "         [  4,   4,   4],\n",
              "         [  5,   5,   5]],\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [  5,   5,   5],\n",
              "         [  5,   5,   5],\n",
              "         [  5,   5,   5]],\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [  5,   5,   5],\n",
              "         [  5,   5,   5],\n",
              "         [  5,   5,   5]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[166, 166, 166],\n",
              "         [172, 172, 172],\n",
              "         [178, 178, 178],\n",
              "         ...,\n",
              "         [153, 153, 153],\n",
              "         [153, 153, 153],\n",
              "         [152, 152, 152]],\n",
              " \n",
              "        [[166, 166, 166],\n",
              "         [172, 172, 172],\n",
              "         [178, 178, 178],\n",
              "         ...,\n",
              "         [155, 155, 155],\n",
              "         [154, 154, 154],\n",
              "         [154, 154, 154]],\n",
              " \n",
              "        [[167, 167, 167],\n",
              "         [171, 171, 171],\n",
              "         [177, 177, 177],\n",
              "         ...,\n",
              "         [155, 155, 155],\n",
              "         [155, 155, 155],\n",
              "         [155, 155, 155]]], dtype=uint8), array([[[ 87,  87,  87],\n",
              "         [ 88,  88,  88],\n",
              "         [ 91,  91,  91],\n",
              "         ...,\n",
              "         [102, 102, 102],\n",
              "         [103, 103, 103],\n",
              "         [ 98,  98,  98]],\n",
              " \n",
              "        [[ 86,  86,  86],\n",
              "         [ 88,  88,  88],\n",
              "         [ 90,  90,  90],\n",
              "         ...,\n",
              "         [ 99,  99,  99],\n",
              "         [ 98,  98,  98],\n",
              "         [ 94,  94,  94]],\n",
              " \n",
              "        [[ 86,  86,  86],\n",
              "         [ 88,  88,  88],\n",
              "         [ 89,  89,  89],\n",
              "         ...,\n",
              "         [ 93,  93,  93],\n",
              "         [ 92,  92,  92],\n",
              "         [ 88,  88,  88]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[  2,   2,   2],\n",
              "         [  3,   3,   3],\n",
              "         [  4,   4,   4],\n",
              "         ...,\n",
              "         [  1,   1,   1],\n",
              "         [  1,   1,   1],\n",
              "         [  1,   1,   1]],\n",
              " \n",
              "        [[  1,   1,   1],\n",
              "         [  2,   2,   2],\n",
              "         [  3,   3,   3],\n",
              "         ...,\n",
              "         [  1,   1,   1],\n",
              "         [  1,   1,   1],\n",
              "         [  1,   1,   1]],\n",
              " \n",
              "        [[  1,   1,   1],\n",
              "         [  2,   2,   2],\n",
              "         [  3,   3,   3],\n",
              "         ...,\n",
              "         [  1,   1,   1],\n",
              "         [  1,   1,   1],\n",
              "         [  1,   1,   1]]], dtype=uint8), array([[[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[173, 173, 173],\n",
              "         [173, 173, 173],\n",
              "         [173, 173, 173],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[173, 173, 173],\n",
              "         [173, 173, 173],\n",
              "         [173, 173, 173],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[173, 173, 173],\n",
              "         [173, 173, 173],\n",
              "         [174, 174, 174],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]]], dtype=uint8), array([[[102,  97,  98],\n",
              "         [102,  97,  98],\n",
              "         [102,  97,  98],\n",
              "         ...,\n",
              "         [ 89,  89,  89],\n",
              "         [ 97,  97,  97],\n",
              "         [129, 129, 129]],\n",
              " \n",
              "        [[  9,   4,   5],\n",
              "         [  9,   4,   5],\n",
              "         [  9,   4,   5],\n",
              "         ...,\n",
              "         [  1,   1,   1],\n",
              "         [  8,   8,   8],\n",
              "         [ 41,  41,  41]],\n",
              " \n",
              "        [[  4,   0,   0],\n",
              "         [  4,   0,   0],\n",
              "         [  4,   0,   0],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [ 32,  32,  32]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[194, 189, 190],\n",
              "         [196, 191, 192],\n",
              "         [198, 193, 194],\n",
              "         ...,\n",
              "         [208, 208, 208],\n",
              "         [210, 210, 210],\n",
              "         [212, 212, 212]],\n",
              " \n",
              "        [[194, 189, 190],\n",
              "         [196, 191, 192],\n",
              "         [198, 193, 194],\n",
              "         ...,\n",
              "         [208, 208, 208],\n",
              "         [210, 210, 210],\n",
              "         [212, 212, 212]],\n",
              " \n",
              "        [[194, 189, 190],\n",
              "         [196, 191, 192],\n",
              "         [198, 193, 194],\n",
              "         ...,\n",
              "         [208, 208, 208],\n",
              "         [210, 210, 210],\n",
              "         [212, 212, 212]]], dtype=uint8), array([[[  4,   0,   0],\n",
              "         [  4,   0,   0],\n",
              "         [  4,   0,   0],\n",
              "         ...,\n",
              "         [  4,   0,   0],\n",
              "         [  8,   3,   4],\n",
              "         [  4,   0,   0]],\n",
              " \n",
              "        [[  4,   0,   0],\n",
              "         [  4,   0,   0],\n",
              "         [  6,   1,   2],\n",
              "         ...,\n",
              "         [  4,   0,   0],\n",
              "         [  8,   3,   4],\n",
              "         [  4,   0,   0]],\n",
              " \n",
              "        [[  4,   0,   0],\n",
              "         [  6,   1,   2],\n",
              "         [  8,   3,   4],\n",
              "         ...,\n",
              "         [  4,   0,   0],\n",
              "         [  8,   3,   4],\n",
              "         [  4,   0,   0]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[163, 160, 156],\n",
              "         [162, 159, 155],\n",
              "         [168, 165, 161],\n",
              "         ...,\n",
              "         [252, 252, 252],\n",
              "         [251, 251, 251],\n",
              "         [249, 249, 249]],\n",
              " \n",
              "        [[163, 160, 156],\n",
              "         [162, 159, 155],\n",
              "         [168, 165, 161],\n",
              "         ...,\n",
              "         [252, 252, 252],\n",
              "         [251, 251, 251],\n",
              "         [249, 249, 249]],\n",
              " \n",
              "        [[163, 160, 156],\n",
              "         [162, 159, 155],\n",
              "         [168, 165, 161],\n",
              "         ...,\n",
              "         [252, 252, 252],\n",
              "         [251, 251, 251],\n",
              "         [249, 249, 249]]], dtype=uint8), array([[[ 6,  6,  6],\n",
              "         [ 6,  6,  6],\n",
              "         [ 6,  6,  6],\n",
              "         ...,\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0]],\n",
              " \n",
              "        [[ 6,  6,  6],\n",
              "         [ 7,  7,  7],\n",
              "         [ 7,  7,  7],\n",
              "         ...,\n",
              "         [ 3,  3,  3],\n",
              "         [ 4,  4,  4],\n",
              "         [ 2,  2,  2]],\n",
              " \n",
              "        [[ 7,  7,  7],\n",
              "         [ 7,  7,  7],\n",
              "         [ 7,  7,  7],\n",
              "         ...,\n",
              "         [ 4,  4,  4],\n",
              "         [ 5,  5,  5],\n",
              "         [ 5,  5,  5]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[10, 10, 10],\n",
              "         [ 9,  9,  9],\n",
              "         [10, 10, 10],\n",
              "         ...,\n",
              "         [ 7,  7,  7],\n",
              "         [ 7,  7,  7],\n",
              "         [ 8,  8,  8]],\n",
              " \n",
              "        [[10, 10, 10],\n",
              "         [10, 10, 10],\n",
              "         [10, 10, 10],\n",
              "         ...,\n",
              "         [ 7,  7,  7],\n",
              "         [ 7,  7,  7],\n",
              "         [ 3,  3,  3]],\n",
              " \n",
              "        [[10, 10, 10],\n",
              "         [ 9,  9,  9],\n",
              "         [10, 10, 10],\n",
              "         ...,\n",
              "         [ 5,  5,  5],\n",
              "         [ 5,  5,  5],\n",
              "         [ 5,  5,  5]]], dtype=uint8), array([[[  6,   6,   6],\n",
              "         [  6,   6,   6],\n",
              "         [  6,   6,   6],\n",
              "         ...,\n",
              "         [  2,   2,   2],\n",
              "         [  3,   3,   3],\n",
              "         [  2,   2,   2]],\n",
              " \n",
              "        [[  6,   6,   6],\n",
              "         [  7,   7,   7],\n",
              "         [  7,   7,   7],\n",
              "         ...,\n",
              "         [  3,   3,   3],\n",
              "         [  3,   3,   3],\n",
              "         [  3,   3,   3]],\n",
              " \n",
              "        [[  6,   6,   6],\n",
              "         [  7,   7,   7],\n",
              "         [  7,   7,   7],\n",
              "         ...,\n",
              "         [  3,   3,   3],\n",
              "         [  3,   3,   3],\n",
              "         [  3,   3,   3]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[175, 175, 175],\n",
              "         [ 82,  82,  82],\n",
              "         [121, 121, 121],\n",
              "         ...,\n",
              "         [ 29,  29,  29],\n",
              "         [ 88,  88,  88],\n",
              "         [ 70,  70,  70]],\n",
              " \n",
              "        [[163, 163, 163],\n",
              "         [  5,   5,   5],\n",
              "         [ 88,  88,  88],\n",
              "         ...,\n",
              "         [ 18,  18,  18],\n",
              "         [ 34,  34,  34],\n",
              "         [ 19,  19,  19]],\n",
              " \n",
              "        [[ 96,  96,  96],\n",
              "         [  0,   0,   0],\n",
              "         [ 77,  77,  77],\n",
              "         ...,\n",
              "         [ 11,  11,  11],\n",
              "         [  8,   8,   8],\n",
              "         [ 10,  10,  10]]], dtype=uint8), array([[[  4,   0,   0],\n",
              "         [ 10,   5,   6],\n",
              "         [  4,   0,   0],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[  4,   0,   0],\n",
              "         [ 10,   5,   6],\n",
              "         [  4,   0,   0],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[  4,   0,   0],\n",
              "         [ 10,   5,   6],\n",
              "         [  4,   0,   0],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[218, 213, 214],\n",
              "         [217, 212, 213],\n",
              "         [223, 218, 219],\n",
              "         ...,\n",
              "         [251, 248, 244],\n",
              "         [251, 248, 244],\n",
              "         [251, 248, 244]],\n",
              " \n",
              "        [[218, 213, 214],\n",
              "         [217, 212, 213],\n",
              "         [223, 218, 219],\n",
              "         ...,\n",
              "         [251, 248, 244],\n",
              "         [251, 248, 244],\n",
              "         [251, 248, 244]],\n",
              " \n",
              "        [[218, 213, 214],\n",
              "         [217, 212, 213],\n",
              "         [223, 218, 219],\n",
              "         ...,\n",
              "         [251, 248, 244],\n",
              "         [251, 248, 244],\n",
              "         [251, 248, 244]]], dtype=uint8), array([[[  1,   1,   1],\n",
              "         [  1,   1,   1],\n",
              "         [  1,   1,   1],\n",
              "         ...,\n",
              "         [ 15,   7,   0],\n",
              "         [ 54,  27,   0],\n",
              "         [ 93,  50,   7]],\n",
              " \n",
              "        [[  1,   1,   1],\n",
              "         [  1,   1,   1],\n",
              "         [  1,   1,   1],\n",
              "         ...,\n",
              "         [ 15,   7,   0],\n",
              "         [ 54,  27,   0],\n",
              "         [ 93,  50,   7]],\n",
              " \n",
              "        [[  1,   1,   1],\n",
              "         [  1,   1,   1],\n",
              "         [  1,   1,   1],\n",
              "         ...,\n",
              "         [ 18,   8,   1],\n",
              "         [ 57,  28,   1],\n",
              "         [ 94,  51,   8]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[ 83,  88,  87],\n",
              "         [ 93,  98,  97],\n",
              "         [106, 111, 110],\n",
              "         ...,\n",
              "         [ 42,  33,  36],\n",
              "         [ 50,  21,   6],\n",
              "         [ 96,  53,   4]],\n",
              " \n",
              "        [[ 83,  88,  87],\n",
              "         [ 94,  99,  98],\n",
              "         [109, 114, 113],\n",
              "         ...,\n",
              "         [ 43,  34,  37],\n",
              "         [ 51,  22,   7],\n",
              "         [ 96,  53,   4]],\n",
              " \n",
              "        [[ 84,  89,  88],\n",
              "         [ 94,  99,  98],\n",
              "         [110, 115, 114],\n",
              "         ...,\n",
              "         [ 44,  35,  38],\n",
              "         [ 52,  23,   8],\n",
              "         [ 96,  53,   4]]], dtype=uint8), array([[[ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         ...,\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0]],\n",
              " \n",
              "        [[ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         ...,\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0]],\n",
              " \n",
              "        [[ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         ...,\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[ 0,  0,  0],\n",
              "         [ 1,  1,  1],\n",
              "         [30, 30, 30],\n",
              "         ...,\n",
              "         [55, 55, 55],\n",
              "         [ 7,  7,  7],\n",
              "         [ 0,  0,  0]],\n",
              " \n",
              "        [[ 0,  0,  0],\n",
              "         [ 1,  1,  1],\n",
              "         [29, 29, 29],\n",
              "         ...,\n",
              "         [55, 55, 55],\n",
              "         [ 7,  7,  7],\n",
              "         [ 0,  0,  0]],\n",
              " \n",
              "        [[ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [29, 29, 29],\n",
              "         ...,\n",
              "         [55, 55, 55],\n",
              "         [ 7,  7,  7],\n",
              "         [ 0,  0,  0]]], dtype=uint8), array([[[ 21,  20,  22],\n",
              "         [ 23,  22,  24],\n",
              "         [ 25,  24,  26],\n",
              "         ...,\n",
              "         [ 27,  26,  30],\n",
              "         [ 16,  25,  29],\n",
              "         [ 16,  25,  29]],\n",
              " \n",
              "        [[ 23,  22,  24],\n",
              "         [ 25,  24,  26],\n",
              "         [ 27,  26,  28],\n",
              "         ...,\n",
              "         [ 28,  27,  31],\n",
              "         [ 23,  29,  34],\n",
              "         [ 23,  29,  34]],\n",
              " \n",
              "        [[ 24,  23,  25],\n",
              "         [ 27,  26,  28],\n",
              "         [ 29,  28,  30],\n",
              "         ...,\n",
              "         [ 29,  28,  32],\n",
              "         [ 26,  29,  34],\n",
              "         [ 25,  28,  33]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[ 83,  83,  83],\n",
              "         [ 87,  87,  87],\n",
              "         [ 94,  94,  94],\n",
              "         ...,\n",
              "         [ 79,  79,  79],\n",
              "         [ 71,  70,  72],\n",
              "         [ 71,  70,  74]],\n",
              " \n",
              "        [[ 89,  89,  89],\n",
              "         [ 90,  90,  90],\n",
              "         [ 96,  96,  96],\n",
              "         ...,\n",
              "         [ 81,  81,  81],\n",
              "         [ 72,  71,  73],\n",
              "         [ 74,  73,  77]],\n",
              " \n",
              "        [[ 96,  96,  96],\n",
              "         [ 95,  95,  95],\n",
              "         [102, 102, 102],\n",
              "         ...,\n",
              "         [ 82,  82,  82],\n",
              "         [ 73,  72,  74],\n",
              "         [ 76,  75,  79]]], dtype=uint8), array([[[ 48,  48,  48],\n",
              "         [ 45,  45,  45],\n",
              "         [ 38,  38,  38],\n",
              "         ...,\n",
              "         [204, 204, 204],\n",
              "         [208, 208, 208],\n",
              "         [229, 229, 229]],\n",
              " \n",
              "        [[ 46,  46,  46],\n",
              "         [ 46,  46,  46],\n",
              "         [ 42,  42,  42],\n",
              "         ...,\n",
              "         [188, 188, 188],\n",
              "         [185, 185, 185],\n",
              "         [191, 191, 191]],\n",
              " \n",
              "        [[ 46,  46,  46],\n",
              "         [ 48,  48,  48],\n",
              "         [ 47,  47,  47],\n",
              "         ...,\n",
              "         [212, 212, 212],\n",
              "         [205, 205, 205],\n",
              "         [217, 217, 217]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]]], dtype=uint8), array([[[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [ 69,  69,  69],\n",
              "         [ 67,  67,  67],\n",
              "         [ 67,  67,  67]],\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [ 70,  70,  70],\n",
              "         [ 69,  69,  69],\n",
              "         [ 68,  68,  68]],\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [ 66,  66,  66],\n",
              "         [ 68,  68,  68],\n",
              "         [ 69,  69,  69]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[205, 205, 205],\n",
              "         [207, 207, 207],\n",
              "         [211, 211, 211],\n",
              "         ...,\n",
              "         [206, 206, 206],\n",
              "         [204, 204, 204],\n",
              "         [203, 203, 203]],\n",
              " \n",
              "        [[207, 207, 207],\n",
              "         [208, 208, 208],\n",
              "         [211, 211, 211],\n",
              "         ...,\n",
              "         [205, 205, 205],\n",
              "         [204, 204, 204],\n",
              "         [204, 204, 204]],\n",
              " \n",
              "        [[209, 209, 209],\n",
              "         [210, 210, 210],\n",
              "         [211, 211, 211],\n",
              "         ...,\n",
              "         [204, 204, 204],\n",
              "         [204, 204, 204],\n",
              "         [205, 205, 205]]], dtype=uint8), array([[[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [ 89,  89,  89],\n",
              "         [ 96,  96,  96],\n",
              "         [129, 129, 129]],\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [ 97,  97,  97],\n",
              "         [110, 110, 110],\n",
              "         [126, 126, 126]],\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [103, 103, 103],\n",
              "         [106, 106, 106],\n",
              "         [119, 119, 119]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [136, 136, 136],\n",
              "         [137, 137, 137],\n",
              "         [121, 121, 121]],\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [137, 137, 137],\n",
              "         [138, 138, 138],\n",
              "         [123, 123, 123]],\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [134, 134, 134],\n",
              "         [136, 136, 136],\n",
              "         [127, 127, 127]]], dtype=uint8), array([[[102,  97,  98],\n",
              "         [102,  97,  98],\n",
              "         [102,  97,  98],\n",
              "         ...,\n",
              "         [ 89,  89,  89],\n",
              "         [ 97,  97,  97],\n",
              "         [129, 129, 129]],\n",
              " \n",
              "        [[  9,   4,   5],\n",
              "         [  9,   4,   5],\n",
              "         [  9,   4,   5],\n",
              "         ...,\n",
              "         [  1,   1,   1],\n",
              "         [  8,   8,   8],\n",
              "         [ 41,  41,  41]],\n",
              " \n",
              "        [[  4,   0,   0],\n",
              "         [  4,   0,   0],\n",
              "         [  4,   0,   0],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [ 32,  32,  32]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[194, 189, 190],\n",
              "         [196, 191, 192],\n",
              "         [198, 193, 194],\n",
              "         ...,\n",
              "         [208, 208, 208],\n",
              "         [210, 210, 210],\n",
              "         [212, 212, 212]],\n",
              " \n",
              "        [[194, 189, 190],\n",
              "         [196, 191, 192],\n",
              "         [198, 193, 194],\n",
              "         ...,\n",
              "         [208, 208, 208],\n",
              "         [210, 210, 210],\n",
              "         [212, 212, 212]],\n",
              " \n",
              "        [[194, 189, 190],\n",
              "         [196, 191, 192],\n",
              "         [198, 193, 194],\n",
              "         ...,\n",
              "         [208, 208, 208],\n",
              "         [210, 210, 210],\n",
              "         [212, 212, 212]]], dtype=uint8), array([[[ 3,  3,  3],\n",
              "         [ 3,  3,  3],\n",
              "         [ 3,  3,  3],\n",
              "         ...,\n",
              "         [ 6,  6,  6],\n",
              "         [ 6,  6,  6],\n",
              "         [ 6,  6,  6]],\n",
              " \n",
              "        [[12, 12, 12],\n",
              "         [12, 12, 12],\n",
              "         [12, 12, 12],\n",
              "         ...,\n",
              "         [10, 10, 10],\n",
              "         [10, 10, 10],\n",
              "         [10, 10, 10]],\n",
              " \n",
              "        [[16, 16, 16],\n",
              "         [16, 16, 16],\n",
              "         [16, 16, 16],\n",
              "         ...,\n",
              "         [14, 14, 14],\n",
              "         [14, 14, 14],\n",
              "         [14, 14, 14]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[11, 11, 11],\n",
              "         [11, 11, 11],\n",
              "         [11, 11, 11],\n",
              "         ...,\n",
              "         [43, 43, 43],\n",
              "         [41, 41, 41],\n",
              "         [40, 40, 40]],\n",
              " \n",
              "        [[ 6,  6,  6],\n",
              "         [ 6,  6,  6],\n",
              "         [ 6,  6,  6],\n",
              "         ...,\n",
              "         [24, 24, 24],\n",
              "         [23, 23, 23],\n",
              "         [23, 23, 23]],\n",
              " \n",
              "        [[ 2,  2,  2],\n",
              "         [ 2,  2,  2],\n",
              "         [ 2,  2,  2],\n",
              "         ...,\n",
              "         [ 4,  4,  4],\n",
              "         [ 4,  4,  4],\n",
              "         [ 4,  4,  4]]], dtype=uint8), array([[[7, 7, 7],\n",
              "         [7, 7, 7],\n",
              "         [7, 7, 7],\n",
              "         ...,\n",
              "         [7, 7, 7],\n",
              "         [7, 7, 7],\n",
              "         [7, 7, 7]],\n",
              " \n",
              "        [[7, 7, 7],\n",
              "         [7, 7, 7],\n",
              "         [7, 7, 7],\n",
              "         ...,\n",
              "         [7, 7, 7],\n",
              "         [7, 7, 7],\n",
              "         [7, 7, 7]],\n",
              " \n",
              "        [[7, 7, 7],\n",
              "         [7, 7, 7],\n",
              "         [7, 7, 7],\n",
              "         ...,\n",
              "         [7, 7, 7],\n",
              "         [7, 7, 7],\n",
              "         [7, 7, 7]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[6, 6, 6],\n",
              "         [6, 6, 6],\n",
              "         [6, 6, 6],\n",
              "         ...,\n",
              "         [7, 7, 7],\n",
              "         [7, 7, 7],\n",
              "         [7, 7, 7]],\n",
              " \n",
              "        [[6, 6, 6],\n",
              "         [6, 6, 6],\n",
              "         [6, 6, 6],\n",
              "         ...,\n",
              "         [7, 7, 7],\n",
              "         [7, 7, 7],\n",
              "         [7, 7, 7]],\n",
              " \n",
              "        [[6, 6, 6],\n",
              "         [6, 6, 6],\n",
              "         [6, 6, 6],\n",
              "         ...,\n",
              "         [7, 7, 7],\n",
              "         [7, 7, 7],\n",
              "         [7, 7, 7]]], dtype=uint8), array([[[  0,   0,   0],\n",
              "         [ 22,  22,  22],\n",
              "         [ 61,  61,  61],\n",
              "         ...,\n",
              "         [  2,   2,   0],\n",
              "         [ 12,   2,   0],\n",
              "         [ 40,  26,  14]],\n",
              " \n",
              "        [[  2,   2,   2],\n",
              "         [ 26,  26,  26],\n",
              "         [ 66,  66,  66],\n",
              "         ...,\n",
              "         [  3,   3,   0],\n",
              "         [ 12,   2,   0],\n",
              "         [ 40,  26,  14]],\n",
              " \n",
              "        [[  5,   5,   5],\n",
              "         [ 31,  31,  31],\n",
              "         [ 73,  73,  73],\n",
              "         ...,\n",
              "         [  6,   6,   0],\n",
              "         [ 12,   2,   0],\n",
              "         [ 40,  26,  14]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[ 40,  24,  11],\n",
              "         [ 18,   2,   0],\n",
              "         [ 38,  22,   9],\n",
              "         ...,\n",
              "         [ 37,  19,   2],\n",
              "         [ 46,  17,   0],\n",
              "         [ 72,  40,  17]],\n",
              " \n",
              "        [[ 86,  49,  11],\n",
              "         [ 86,  49,  11],\n",
              "         [ 86,  49,  11],\n",
              "         ...,\n",
              "         [ 88,  44,  20],\n",
              "         [ 99,  45,  15],\n",
              "         [102,  45,  14]],\n",
              " \n",
              "        [[ 99,  52,   1],\n",
              "         [ 99,  52,   1],\n",
              "         [ 99,  52,   1],\n",
              "         ...,\n",
              "         [100,  44,  15],\n",
              "         [111,  44,  11],\n",
              "         [115,  44,  10]]], dtype=uint8), array([[[ 19,  19,  19],\n",
              "         [ 19,  19,  19],\n",
              "         [ 20,  20,  20],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[ 19,  19,  19],\n",
              "         [ 19,  19,  19],\n",
              "         [ 20,  20,  20],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[ 20,  20,  20],\n",
              "         [ 20,  20,  20],\n",
              "         [ 20,  20,  20],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[183, 183, 183],\n",
              "         [184, 184, 184],\n",
              "         [185, 185, 185],\n",
              "         ...,\n",
              "         [148, 148, 148],\n",
              "         [146, 146, 146],\n",
              "         [144, 144, 144]],\n",
              " \n",
              "        [[186, 186, 186],\n",
              "         [186, 186, 186],\n",
              "         [187, 187, 187],\n",
              "         ...,\n",
              "         [148, 148, 148],\n",
              "         [146, 146, 146],\n",
              "         [144, 144, 144]],\n",
              " \n",
              "        [[188, 188, 188],\n",
              "         [188, 188, 188],\n",
              "         [189, 189, 189],\n",
              "         ...,\n",
              "         [148, 148, 148],\n",
              "         [146, 146, 146],\n",
              "         [144, 144, 144]]], dtype=uint8), array([[[ 28,  19,  10],\n",
              "         [ 54,  45,  36],\n",
              "         [ 38,  29,  25],\n",
              "         ...,\n",
              "         [ 35,  29,  24],\n",
              "         [ 46,  34,  24],\n",
              "         [ 41,  25,  12]],\n",
              " \n",
              "        [[ 52,  43,  34],\n",
              "         [ 98,  89,  80],\n",
              "         [ 95,  86,  82],\n",
              "         ...,\n",
              "         [ 89,  83,  78],\n",
              "         [ 79,  67,  57],\n",
              "         [ 51,  35,  22]],\n",
              " \n",
              "        [[ 35,  26,  17],\n",
              "         [ 95,  86,  77],\n",
              "         [100,  91,  87],\n",
              "         ...,\n",
              "         [ 94,  88,  83],\n",
              "         [ 82,  70,  60],\n",
              "         [ 44,  28,  15]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[ 46,  40,  35],\n",
              "         [141, 137, 132],\n",
              "         [133, 128, 125],\n",
              "         ...,\n",
              "         [ 89,  84,  81],\n",
              "         [ 79,  70,  61],\n",
              "         [ 42,  30,  18]],\n",
              " \n",
              "        [[ 45,  39,  34],\n",
              "         [136, 132, 127],\n",
              "         [126, 121, 118],\n",
              "         ...,\n",
              "         [ 86,  81,  78],\n",
              "         [ 77,  68,  59],\n",
              "         [ 41,  29,  17]],\n",
              " \n",
              "        [[ 41,  35,  30],\n",
              "         [129, 125, 120],\n",
              "         [117, 112, 109],\n",
              "         ...,\n",
              "         [ 79,  74,  71],\n",
              "         [ 72,  63,  54],\n",
              "         [ 37,  25,  13]]], dtype=uint8), array([[[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[159, 159, 159],\n",
              "         [158, 158, 158],\n",
              "         [156, 156, 156],\n",
              "         ...,\n",
              "         [130, 130, 130],\n",
              "         [131, 131, 131],\n",
              "         [132, 132, 132]],\n",
              " \n",
              "        [[160, 160, 160],\n",
              "         [159, 159, 159],\n",
              "         [157, 157, 157],\n",
              "         ...,\n",
              "         [130, 130, 130],\n",
              "         [130, 130, 130],\n",
              "         [131, 131, 131]],\n",
              " \n",
              "        [[161, 161, 161],\n",
              "         [160, 160, 160],\n",
              "         [158, 158, 158],\n",
              "         ...,\n",
              "         [131, 131, 131],\n",
              "         [131, 131, 131],\n",
              "         [132, 132, 132]]], dtype=uint8), array([[[  4,   0,   0],\n",
              "         [ 10,   5,   6],\n",
              "         [  4,   0,   0],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[  4,   0,   0],\n",
              "         [ 10,   5,   6],\n",
              "         [  4,   0,   0],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[  4,   0,   0],\n",
              "         [ 10,   5,   6],\n",
              "         [  4,   0,   0],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[218, 213, 214],\n",
              "         [217, 212, 213],\n",
              "         [223, 218, 219],\n",
              "         ...,\n",
              "         [251, 248, 244],\n",
              "         [251, 248, 244],\n",
              "         [251, 248, 244]],\n",
              " \n",
              "        [[218, 213, 214],\n",
              "         [217, 212, 213],\n",
              "         [223, 218, 219],\n",
              "         ...,\n",
              "         [251, 248, 244],\n",
              "         [251, 248, 244],\n",
              "         [251, 248, 244]],\n",
              " \n",
              "        [[218, 213, 214],\n",
              "         [217, 212, 213],\n",
              "         [223, 218, 219],\n",
              "         ...,\n",
              "         [251, 248, 244],\n",
              "         [251, 248, 244],\n",
              "         [251, 248, 244]]], dtype=uint8), array([[[  0,   0,   0],\n",
              "         [  4,   4,   4],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [ 34,  29,  30],\n",
              "         [ 34,  29,  30],\n",
              "         [ 33,  28,  29]],\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  3,   3,   3],\n",
              "         [ 14,  14,  14],\n",
              "         ...,\n",
              "         [ 63,  58,  59],\n",
              "         [ 60,  55,  56],\n",
              "         [ 58,  53,  54]],\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [ 23,  23,  23],\n",
              "         ...,\n",
              "         [ 76,  71,  72],\n",
              "         [ 70,  65,  66],\n",
              "         [ 67,  62,  63]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[  4,   0,   0],\n",
              "         [  4,   0,   0],\n",
              "         [  8,   3,   4],\n",
              "         ...,\n",
              "         [106, 103,  99],\n",
              "         [ 98,  95,  91],\n",
              "         [ 90,  87,  83]],\n",
              " \n",
              "        [[  4,   0,   0],\n",
              "         [  4,   0,   0],\n",
              "         [  8,   3,   4],\n",
              "         ...,\n",
              "         [105, 102,  98],\n",
              "         [ 98,  95,  91],\n",
              "         [ 89,  86,  82]],\n",
              " \n",
              "        [[  4,   0,   0],\n",
              "         [  4,   0,   0],\n",
              "         [  8,   3,   4],\n",
              "         ...,\n",
              "         [105, 102,  98],\n",
              "         [ 97,  94,  90],\n",
              "         [ 88,  85,  81]]], dtype=uint8), array([[[ 56,  56,  56],\n",
              "         [ 56,  56,  56],\n",
              "         [ 56,  56,  56],\n",
              "         ...,\n",
              "         [117, 117, 117],\n",
              "         [117, 117, 117],\n",
              "         [116, 116, 116]],\n",
              " \n",
              "        [[ 58,  58,  58],\n",
              "         [ 58,  58,  58],\n",
              "         [ 58,  58,  58],\n",
              "         ...,\n",
              "         [117, 117, 117],\n",
              "         [117, 117, 117],\n",
              "         [116, 116, 116]],\n",
              " \n",
              "        [[ 61,  61,  61],\n",
              "         [ 61,  61,  61],\n",
              "         [ 61,  61,  61],\n",
              "         ...,\n",
              "         [117, 117, 117],\n",
              "         [117, 117, 117],\n",
              "         [117, 117, 117]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[137, 137, 137],\n",
              "         [138, 138, 138],\n",
              "         [140, 140, 140],\n",
              "         ...,\n",
              "         [124, 124, 124],\n",
              "         [124, 124, 124],\n",
              "         [123, 123, 123]],\n",
              " \n",
              "        [[137, 137, 137],\n",
              "         [138, 138, 138],\n",
              "         [140, 140, 140],\n",
              "         ...,\n",
              "         [124, 124, 124],\n",
              "         [124, 124, 124],\n",
              "         [123, 123, 123]],\n",
              " \n",
              "        [[137, 137, 137],\n",
              "         [138, 138, 138],\n",
              "         [140, 140, 140],\n",
              "         ...,\n",
              "         [124, 124, 124],\n",
              "         [124, 124, 124],\n",
              "         [123, 123, 123]]], dtype=uint8), array([[[125, 125, 125],\n",
              "         [126, 126, 126],\n",
              "         [126, 126, 126],\n",
              "         ...,\n",
              "         [148, 148, 148],\n",
              "         [147, 147, 147],\n",
              "         [147, 147, 147]],\n",
              " \n",
              "        [[131, 131, 131],\n",
              "         [130, 130, 130],\n",
              "         [128, 128, 128],\n",
              "         ...,\n",
              "         [148, 148, 148],\n",
              "         [146, 146, 146],\n",
              "         [146, 146, 146]],\n",
              " \n",
              "        [[135, 135, 135],\n",
              "         [132, 132, 132],\n",
              "         [128, 128, 128],\n",
              "         ...,\n",
              "         [141, 141, 141],\n",
              "         [137, 137, 137],\n",
              "         [137, 137, 137]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[160, 160, 160],\n",
              "         [165, 165, 165],\n",
              "         [165, 165, 165],\n",
              "         ...,\n",
              "         [141, 141, 141],\n",
              "         [136, 136, 136],\n",
              "         [134, 134, 134]],\n",
              " \n",
              "        [[160, 160, 160],\n",
              "         [165, 165, 165],\n",
              "         [165, 165, 165],\n",
              "         ...,\n",
              "         [141, 141, 141],\n",
              "         [136, 136, 136],\n",
              "         [134, 134, 134]],\n",
              " \n",
              "        [[161, 161, 161],\n",
              "         [166, 166, 166],\n",
              "         [164, 164, 164],\n",
              "         ...,\n",
              "         [140, 140, 140],\n",
              "         [134, 134, 134],\n",
              "         [132, 132, 132]]], dtype=uint8), array([[[ 13,  13,  13],\n",
              "         [  0,   0,   0],\n",
              "         [  1,   1,   1],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[ 13,  13,  13],\n",
              "         [  0,   0,   0],\n",
              "         [  1,   1,   1],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[ 13,  13,  13],\n",
              "         [  0,   0,   0],\n",
              "         [  1,   1,   1],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[153, 153, 153],\n",
              "         [158, 158, 158],\n",
              "         [165, 165, 165],\n",
              "         ...,\n",
              "         [ 58,  58,  58],\n",
              "         [ 55,  55,  55],\n",
              "         [ 43,  43,  43]],\n",
              " \n",
              "        [[153, 153, 153],\n",
              "         [158, 158, 158],\n",
              "         [167, 166, 167],\n",
              "         ...,\n",
              "         [ 57,  57,  57],\n",
              "         [ 51,  51,  51],\n",
              "         [ 41,  41,  41]],\n",
              " \n",
              "        [[153, 153, 153],\n",
              "         [158, 158, 158],\n",
              "         [169, 169, 169],\n",
              "         ...,\n",
              "         [ 55,  55,  55],\n",
              "         [ 47,  47,  47],\n",
              "         [ 38,  38,  38]]], dtype=uint8), array([[[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [2, 2, 2],\n",
              "         [2, 2, 2],\n",
              "         [2, 2, 2]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [2, 2, 2],\n",
              "         [2, 2, 2],\n",
              "         [2, 2, 2]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [2, 2, 2],\n",
              "         [2, 2, 2],\n",
              "         [2, 2, 2]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]]], dtype=uint8), array([[[253, 253, 253],\n",
              "         [253, 253, 253],\n",
              "         [ 22,  22,  22],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[253, 253, 253],\n",
              "         [253, 253, 253],\n",
              "         [ 22,  22,  22],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[253, 253, 253],\n",
              "         [253, 253, 253],\n",
              "         [ 22,  22,  22],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[163, 163, 163],\n",
              "         [164, 164, 164],\n",
              "         [ 82,  82,  82],\n",
              "         ...,\n",
              "         [124, 124, 124],\n",
              "         [122, 122, 122],\n",
              "         [121, 121, 121]],\n",
              " \n",
              "        [[163, 163, 163],\n",
              "         [164, 164, 164],\n",
              "         [ 82,  82,  82],\n",
              "         ...,\n",
              "         [123, 123, 123],\n",
              "         [122, 122, 122],\n",
              "         [120, 120, 120]],\n",
              " \n",
              "        [[164, 164, 164],\n",
              "         [164, 164, 164],\n",
              "         [ 82,  82,  82],\n",
              "         ...,\n",
              "         [123, 123, 123],\n",
              "         [121, 121, 121],\n",
              "         [120, 120, 120]]], dtype=uint8), array([[[  1,   1,   1],\n",
              "         [  0,   0,   0],\n",
              "         [ 16,  16,  16],\n",
              "         ...,\n",
              "         [  7,   7,   7],\n",
              "         [  5,   5,   5],\n",
              "         [  4,   4,   4]],\n",
              " \n",
              "        [[  5,   5,   5],\n",
              "         [  2,   2,   2],\n",
              "         [ 29,  29,  29],\n",
              "         ...,\n",
              "         [ 21,  21,  21],\n",
              "         [ 20,  20,  20],\n",
              "         [ 18,  18,  18]],\n",
              " \n",
              "        [[  2,   2,   2],\n",
              "         [  3,   3,   3],\n",
              "         [ 35,  35,  35],\n",
              "         ...,\n",
              "         [ 26,  26,  26],\n",
              "         [ 24,  24,  24],\n",
              "         [ 23,  23,  23]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [131, 131, 131],\n",
              "         ...,\n",
              "         [ 11,  11,  11],\n",
              "         [ 11,  11,  11],\n",
              "         [ 11,  11,  11]],\n",
              " \n",
              "        [[ 14,  14,  14],\n",
              "         [ 14,  14,  14],\n",
              "         [120, 120, 120],\n",
              "         ...,\n",
              "         [  9,   9,   9],\n",
              "         [  9,   9,   9],\n",
              "         [  9,   9,   9]],\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [ 40,  40,  40],\n",
              "         ...,\n",
              "         [  6,   6,   6],\n",
              "         [  6,   6,   6],\n",
              "         [  6,   6,   6]]], dtype=uint8), array([[[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]]], dtype=uint8), array([[[ 41,  41,  41],\n",
              "         [  9,   9,   9],\n",
              "         [ 27,  27,  27],\n",
              "         ...,\n",
              "         [ 25,  25,  25],\n",
              "         [ 23,  23,  23],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[ 13,  13,  13],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  2,   2,   2],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[ 29,  29,  29],\n",
              "         [  0,   0,   0],\n",
              "         [ 17,  17,  17],\n",
              "         ...,\n",
              "         [ 12,  12,  12],\n",
              "         [ 11,  11,  11],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[129, 129, 129],\n",
              "         [116, 116, 116],\n",
              "         [131, 131, 131],\n",
              "         ...,\n",
              "         [147, 147, 147],\n",
              "         [144, 144, 144],\n",
              "         [124, 124, 124]],\n",
              " \n",
              "        [[129, 129, 129],\n",
              "         [114, 114, 114],\n",
              "         [131, 131, 131],\n",
              "         ...,\n",
              "         [143, 143, 143],\n",
              "         [142, 142, 142],\n",
              "         [124, 124, 124]],\n",
              " \n",
              "        [[110, 110, 110],\n",
              "         [ 96,  96,  96],\n",
              "         [118, 118, 118],\n",
              "         ...,\n",
              "         [135, 135, 135],\n",
              "         [130, 130, 130],\n",
              "         [110, 110, 110]]], dtype=uint8), array([[[ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         ...,\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0]],\n",
              " \n",
              "        [[ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         ...,\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0]],\n",
              " \n",
              "        [[ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         ...,\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         ...,\n",
              "         [85, 85, 85],\n",
              "         [84, 84, 84],\n",
              "         [84, 84, 84]],\n",
              " \n",
              "        [[ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         ...,\n",
              "         [83, 83, 83],\n",
              "         [84, 84, 84],\n",
              "         [85, 85, 85]],\n",
              " \n",
              "        [[ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         ...,\n",
              "         [83, 83, 83],\n",
              "         [84, 84, 84],\n",
              "         [85, 85, 85]]], dtype=uint8), array([[[222, 222, 222],\n",
              "         [223, 223, 223],\n",
              "         [223, 223, 223],\n",
              "         ...,\n",
              "         [229, 229, 229],\n",
              "         [181, 181, 181],\n",
              "         [206, 206, 206]],\n",
              " \n",
              "        [[223, 223, 223],\n",
              "         [223, 223, 223],\n",
              "         [222, 222, 222],\n",
              "         ...,\n",
              "         [241, 241, 241],\n",
              "         [169, 169, 169],\n",
              "         [166, 166, 166]],\n",
              " \n",
              "        [[224, 224, 224],\n",
              "         [223, 223, 223],\n",
              "         [221, 221, 221],\n",
              "         ...,\n",
              "         [216, 216, 216],\n",
              "         [173, 173, 173],\n",
              "         [191, 191, 191]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[227, 227, 227],\n",
              "         [225, 225, 225],\n",
              "         [224, 224, 224],\n",
              "         ...,\n",
              "         [217, 217, 217],\n",
              "         [214, 214, 214],\n",
              "         [209, 209, 209]],\n",
              " \n",
              "        [[229, 229, 229],\n",
              "         [226, 226, 226],\n",
              "         [224, 224, 224],\n",
              "         ...,\n",
              "         [217, 217, 217],\n",
              "         [215, 215, 215],\n",
              "         [210, 210, 210]],\n",
              " \n",
              "        [[230, 230, 230],\n",
              "         [226, 226, 226],\n",
              "         [225, 225, 225],\n",
              "         ...,\n",
              "         [215, 215, 215],\n",
              "         [213, 213, 213],\n",
              "         [209, 209, 209]]], dtype=uint8), array([[[109, 109, 109],\n",
              "         [ 43,  43,  43],\n",
              "         [ 16,  16,  16],\n",
              "         ...,\n",
              "         [  2,   2,   2],\n",
              "         [  2,   2,   2],\n",
              "         [  6,   6,   6]],\n",
              " \n",
              "        [[107, 107, 107],\n",
              "         [ 42,  42,  42],\n",
              "         [ 15,  15,  15],\n",
              "         ...,\n",
              "         [  1,   1,   1],\n",
              "         [  2,   2,   2],\n",
              "         [  6,   6,   6]],\n",
              " \n",
              "        [[105, 105, 105],\n",
              "         [ 42,  42,  42],\n",
              "         [ 15,  15,  15],\n",
              "         ...,\n",
              "         [  1,   1,   1],\n",
              "         [  2,   2,   2],\n",
              "         [  6,   6,   6]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[  5,   5,   5],\n",
              "         [  3,   3,   3],\n",
              "         [  3,   3,   3],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  2,   2,   2],\n",
              "         [  6,   6,   6]],\n",
              " \n",
              "        [[  2,   2,   2],\n",
              "         [  3,   3,   3],\n",
              "         [  3,   3,   3],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  2,   2,   2],\n",
              "         [  6,   6,   6]],\n",
              " \n",
              "        [[ 41,  41,  41],\n",
              "         [ 40,  40,  40],\n",
              "         [ 41,  41,  41],\n",
              "         ...,\n",
              "         [ 40,  40,  40],\n",
              "         [ 35,  35,  35],\n",
              "         [ 49,  49,  49]]], dtype=uint8), array([[[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [  4,   0,   0],\n",
              "         [  4,   0,   0],\n",
              "         [  4,   0,   0]],\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [  4,   0,   0],\n",
              "         [  4,   0,   0],\n",
              "         [  4,   0,   0]],\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [  4,   0,   0],\n",
              "         [  4,   0,   0],\n",
              "         [  4,   0,   0]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[129, 129, 129],\n",
              "         [117, 117, 117],\n",
              "         [127, 127, 127],\n",
              "         ...,\n",
              "         [168, 163, 164],\n",
              "         [168, 163, 164],\n",
              "         [169, 164, 165]],\n",
              " \n",
              "        [[129, 129, 129],\n",
              "         [117, 117, 117],\n",
              "         [127, 127, 127],\n",
              "         ...,\n",
              "         [168, 163, 164],\n",
              "         [168, 163, 164],\n",
              "         [169, 164, 165]],\n",
              " \n",
              "        [[129, 129, 129],\n",
              "         [117, 117, 117],\n",
              "         [127, 127, 127],\n",
              "         ...,\n",
              "         [168, 163, 164],\n",
              "         [168, 163, 164],\n",
              "         [169, 164, 165]]], dtype=uint8), array([[[ 5,  5,  5],\n",
              "         [ 5,  5,  5],\n",
              "         [ 5,  5,  5],\n",
              "         ...,\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0]],\n",
              " \n",
              "        [[ 5,  5,  5],\n",
              "         [ 5,  5,  5],\n",
              "         [ 5,  5,  5],\n",
              "         ...,\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0]],\n",
              " \n",
              "        [[ 5,  5,  5],\n",
              "         [ 5,  5,  5],\n",
              "         [ 5,  5,  5],\n",
              "         ...,\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[82, 82, 82],\n",
              "         [87, 87, 87],\n",
              "         [89, 89, 89],\n",
              "         ...,\n",
              "         [35, 35, 35],\n",
              "         [34, 34, 34],\n",
              "         [33, 33, 33]],\n",
              " \n",
              "        [[82, 82, 82],\n",
              "         [87, 87, 87],\n",
              "         [89, 89, 89],\n",
              "         ...,\n",
              "         [34, 34, 34],\n",
              "         [33, 33, 33],\n",
              "         [32, 32, 32]],\n",
              " \n",
              "        [[81, 81, 81],\n",
              "         [87, 87, 87],\n",
              "         [89, 89, 89],\n",
              "         ...,\n",
              "         [34, 34, 34],\n",
              "         [33, 33, 33],\n",
              "         [32, 32, 32]]], dtype=uint8), array([[[ 13,  10,   6],\n",
              "         [ 12,   9,   5],\n",
              "         [ 10,   7,   3],\n",
              "         ...,\n",
              "         [ 10,  10,  10],\n",
              "         [  1,   1,   1],\n",
              "         [ 27,  27,  27]],\n",
              " \n",
              "        [[ 13,  10,   6],\n",
              "         [ 12,   9,   5],\n",
              "         [ 10,   7,   3],\n",
              "         ...,\n",
              "         [ 10,  10,  10],\n",
              "         [  2,   2,   2],\n",
              "         [ 28,  28,  28]],\n",
              " \n",
              "        [[ 13,  10,   6],\n",
              "         [ 12,   9,   5],\n",
              "         [ 10,   7,   3],\n",
              "         ...,\n",
              "         [ 10,  10,  10],\n",
              "         [  2,   2,   2],\n",
              "         [ 28,  28,  28]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[128, 123, 124],\n",
              "         [130, 125, 126],\n",
              "         [133, 128, 129],\n",
              "         ...,\n",
              "         [170, 170, 170],\n",
              "         [167, 167, 167],\n",
              "         [169, 169, 169]],\n",
              " \n",
              "        [[128, 123, 124],\n",
              "         [130, 125, 126],\n",
              "         [133, 128, 129],\n",
              "         ...,\n",
              "         [170, 170, 170],\n",
              "         [167, 167, 167],\n",
              "         [169, 169, 169]],\n",
              " \n",
              "        [[128, 123, 124],\n",
              "         [130, 125, 126],\n",
              "         [133, 128, 129],\n",
              "         ...,\n",
              "         [170, 170, 170],\n",
              "         [167, 167, 167],\n",
              "         [169, 169, 169]]], dtype=uint8), array([[[ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         ...,\n",
              "         [ 9,  9,  9],\n",
              "         [ 7,  7,  7],\n",
              "         [ 9,  9,  9]],\n",
              " \n",
              "        [[ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         ...,\n",
              "         [ 7,  7,  7],\n",
              "         [ 7,  7,  7],\n",
              "         [ 9,  9,  9]],\n",
              " \n",
              "        [[ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         ...,\n",
              "         [ 7,  7,  7],\n",
              "         [ 6,  6,  6],\n",
              "         [ 8,  8,  8]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[74, 74, 74],\n",
              "         [72, 72, 72],\n",
              "         [72, 72, 72],\n",
              "         ...,\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0]],\n",
              " \n",
              "        [[70, 70, 70],\n",
              "         [71, 71, 71],\n",
              "         [73, 73, 73],\n",
              "         ...,\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0]],\n",
              " \n",
              "        [[70, 70, 70],\n",
              "         [73, 73, 73],\n",
              "         [73, 73, 73],\n",
              "         ...,\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0]]], dtype=uint8), array([[[76, 75, 77],\n",
              "         [40, 39, 41],\n",
              "         [43, 42, 44],\n",
              "         ...,\n",
              "         [60, 58, 47],\n",
              "         [66, 66, 50],\n",
              "         [71, 70, 50]],\n",
              " \n",
              "        [[60, 59, 61],\n",
              "         [ 3,  2,  4],\n",
              "         [ 0,  0,  1],\n",
              "         ...,\n",
              "         [26, 24,  0],\n",
              "         [33, 32,  4],\n",
              "         [38, 38,  8]],\n",
              " \n",
              "        [[80, 79, 81],\n",
              "         [ 4,  3,  5],\n",
              "         [ 0,  0,  1],\n",
              "         ...,\n",
              "         [27, 23,  0],\n",
              "         [31, 28,  0],\n",
              "         [35, 33,  3]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[76, 76, 76],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         ...,\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0]],\n",
              " \n",
              "        [[76, 76, 76],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         ...,\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0]],\n",
              " \n",
              "        [[76, 76, 76],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         ...,\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0]]], dtype=uint8), array([[[ 4,  0,  0],\n",
              "         [ 4,  0,  0],\n",
              "         [ 4,  0,  0],\n",
              "         ...,\n",
              "         [ 0,  0,  0],\n",
              "         [ 2,  2,  2],\n",
              "         [ 0,  0,  0]],\n",
              " \n",
              "        [[ 4,  0,  0],\n",
              "         [ 4,  0,  0],\n",
              "         [ 4,  0,  0],\n",
              "         ...,\n",
              "         [ 0,  0,  0],\n",
              "         [ 2,  2,  2],\n",
              "         [ 0,  0,  0]],\n",
              " \n",
              "        [[ 4,  0,  0],\n",
              "         [ 4,  0,  0],\n",
              "         [ 4,  0,  0],\n",
              "         ...,\n",
              "         [ 0,  0,  0],\n",
              "         [ 2,  2,  2],\n",
              "         [ 0,  0,  0]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[37, 34, 30],\n",
              "         [45, 42, 38],\n",
              "         [58, 55, 51],\n",
              "         ...,\n",
              "         [ 0,  0,  0],\n",
              "         [ 2,  2,  2],\n",
              "         [ 0,  0,  0]],\n",
              " \n",
              "        [[39, 36, 32],\n",
              "         [47, 44, 40],\n",
              "         [59, 56, 52],\n",
              "         ...,\n",
              "         [ 0,  0,  0],\n",
              "         [ 2,  2,  2],\n",
              "         [ 0,  0,  0]],\n",
              " \n",
              "        [[40, 37, 33],\n",
              "         [48, 45, 41],\n",
              "         [60, 57, 53],\n",
              "         ...,\n",
              "         [ 0,  0,  0],\n",
              "         [ 2,  2,  2],\n",
              "         [ 0,  0,  0]]], dtype=uint8), array([[[  4,   4,   4],\n",
              "         [  4,   4,   4],\n",
              "         [  4,   4,   4],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[  6,   6,   6],\n",
              "         [  6,   6,   6],\n",
              "         [  6,   6,   6],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[  9,   9,   9],\n",
              "         [  9,   9,   9],\n",
              "         [  9,   9,   9],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[123, 123, 123],\n",
              "         [124, 124, 124],\n",
              "         [126, 126, 126],\n",
              "         ...,\n",
              "         [117, 117, 117],\n",
              "         [115, 115, 115],\n",
              "         [114, 114, 114]],\n",
              " \n",
              "        [[125, 125, 125],\n",
              "         [127, 127, 127],\n",
              "         [129, 129, 129],\n",
              "         ...,\n",
              "         [119, 119, 119],\n",
              "         [118, 118, 118],\n",
              "         [117, 117, 117]],\n",
              " \n",
              "        [[125, 125, 125],\n",
              "         [127, 127, 127],\n",
              "         [129, 129, 129],\n",
              "         ...,\n",
              "         [120, 120, 120],\n",
              "         [120, 120, 120],\n",
              "         [120, 120, 120]]], dtype=uint8), array([[[  1,   1,   1],\n",
              "         [  1,   1,   1],\n",
              "         [  1,   1,   1],\n",
              "         ...,\n",
              "         [  0,   0,   1],\n",
              "         [  0,   0,   1],\n",
              "         [  0,   0,   3]],\n",
              " \n",
              "        [[  1,   1,   1],\n",
              "         [  1,   1,   1],\n",
              "         [  1,   1,   1],\n",
              "         ...,\n",
              "         [  0,   0,   1],\n",
              "         [  0,   0,   1],\n",
              "         [  0,   0,   3]],\n",
              " \n",
              "        [[  1,   1,   1],\n",
              "         [  1,   1,   1],\n",
              "         [  1,   1,   1],\n",
              "         ...,\n",
              "         [  0,   0,   1],\n",
              "         [  0,   0,   1],\n",
              "         [  0,   0,   3]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[ 25,  25,  25],\n",
              "         [ 28,  28,  28],\n",
              "         [ 32,  32,  32],\n",
              "         ...,\n",
              "         [185, 184, 186],\n",
              "         [108, 107, 109],\n",
              "         [  0,   0,   4]],\n",
              " \n",
              "        [[ 24,  24,  24],\n",
              "         [ 28,  28,  28],\n",
              "         [ 32,  32,  32],\n",
              "         ...,\n",
              "         [187, 186, 188],\n",
              "         [109, 108, 110],\n",
              "         [  0,   0,   4]],\n",
              " \n",
              "        [[ 17,  19,  30],\n",
              "         [ 19,  21,  32],\n",
              "         [ 23,  25,  36],\n",
              "         ...,\n",
              "         [193, 201, 201],\n",
              "         [103, 105, 106],\n",
              "         [  4,   2,   2]]], dtype=uint8), array([[[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]]], dtype=uint8), array([[[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]]], dtype=uint8), array([[[105, 105, 105],\n",
              "         [104, 104, 104],\n",
              "         [103, 103, 103],\n",
              "         ...,\n",
              "         [103, 103, 103],\n",
              "         [103, 103, 103],\n",
              "         [103, 103, 103]],\n",
              " \n",
              "        [[104, 104, 104],\n",
              "         [103, 103, 103],\n",
              "         [102, 102, 102],\n",
              "         ...,\n",
              "         [104, 104, 104],\n",
              "         [104, 104, 104],\n",
              "         [104, 104, 104]],\n",
              " \n",
              "        [[103, 103, 103],\n",
              "         [102, 102, 102],\n",
              "         [101, 101, 101],\n",
              "         ...,\n",
              "         [105, 105, 105],\n",
              "         [105, 105, 105],\n",
              "         [105, 105, 105]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[184, 184, 184],\n",
              "         [185, 185, 185],\n",
              "         [186, 186, 186],\n",
              "         ...,\n",
              "         [195, 195, 195],\n",
              "         [198, 198, 198],\n",
              "         [200, 200, 200]],\n",
              " \n",
              "        [[184, 184, 184],\n",
              "         [185, 185, 185],\n",
              "         [186, 186, 186],\n",
              "         ...,\n",
              "         [195, 195, 195],\n",
              "         [198, 198, 198],\n",
              "         [200, 200, 200]],\n",
              " \n",
              "        [[184, 184, 184],\n",
              "         [185, 185, 185],\n",
              "         [186, 186, 186],\n",
              "         ...,\n",
              "         [195, 195, 195],\n",
              "         [198, 198, 198],\n",
              "         [200, 200, 200]]], dtype=uint8), array([[[ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         ...,\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0]],\n",
              " \n",
              "        [[ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         ...,\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0]],\n",
              " \n",
              "        [[ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         ...,\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         ...,\n",
              "         [51, 51, 51],\n",
              "         [47, 47, 47],\n",
              "         [43, 43, 43]],\n",
              " \n",
              "        [[ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         ...,\n",
              "         [49, 49, 49],\n",
              "         [45, 45, 45],\n",
              "         [41, 41, 41]],\n",
              " \n",
              "        [[ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         ...,\n",
              "         [48, 48, 48],\n",
              "         [44, 44, 44],\n",
              "         [40, 40, 40]]], dtype=uint8), array([[[ 6,  6,  6],\n",
              "         [ 0,  0,  0],\n",
              "         [ 4,  4,  4],\n",
              "         ...,\n",
              "         [ 2,  2,  2],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0]],\n",
              " \n",
              "        [[ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 5,  5,  5],\n",
              "         ...,\n",
              "         [ 7,  7,  7],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0]],\n",
              " \n",
              "        [[ 2,  2,  2],\n",
              "         [ 2,  2,  2],\n",
              "         [16, 16, 16],\n",
              "         ...,\n",
              "         [12, 12, 12],\n",
              "         [ 1,  1,  1],\n",
              "         [ 0,  0,  0]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[ 3,  3,  3],\n",
              "         [ 8,  8,  8],\n",
              "         [14, 14, 14],\n",
              "         ...,\n",
              "         [10, 10, 10],\n",
              "         [ 4,  4,  4],\n",
              "         [ 1,  1,  1]],\n",
              " \n",
              "        [[ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 1,  1,  1],\n",
              "         ...,\n",
              "         [ 3,  3,  3],\n",
              "         [ 1,  1,  1],\n",
              "         [ 0,  0,  0]],\n",
              " \n",
              "        [[ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         ...,\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 0,  0,  0]]], dtype=uint8), array([[[ 71,  71,  71],\n",
              "         [ 75,  75,  75],\n",
              "         [ 71,  71,  71],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[ 76,  76,  76],\n",
              "         [ 75,  75,  75],\n",
              "         [ 72,  72,  72],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[ 78,  78,  78],\n",
              "         [ 79,  79,  79],\n",
              "         [ 82,  82,  82],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[154, 154, 154],\n",
              "         [155, 155, 155],\n",
              "         [149, 149, 149],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[151, 151, 151],\n",
              "         [151, 151, 151],\n",
              "         [152, 152, 152],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[152, 152, 152],\n",
              "         [152, 152, 152],\n",
              "         [150, 150, 150],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]]], dtype=uint8), array([[[ 54,  54,  54],\n",
              "         [ 56,  56,  56],\n",
              "         [ 54,  54,  54],\n",
              "         ...,\n",
              "         [114, 114, 114],\n",
              "         [ 87,  87,  87],\n",
              "         [ 68,  68,  68]],\n",
              " \n",
              "        [[ 54,  54,  54],\n",
              "         [ 57,  57,  57],\n",
              "         [ 57,  57,  57],\n",
              "         ...,\n",
              "         [ 90,  90,  90],\n",
              "         [ 74,  74,  74],\n",
              "         [ 67,  67,  67]],\n",
              " \n",
              "        [[ 54,  54,  54],\n",
              "         [ 58,  58,  58],\n",
              "         [ 59,  59,  59],\n",
              "         ...,\n",
              "         [ 72,  72,  72],\n",
              "         [ 69,  69,  69],\n",
              "         [ 75,  75,  75]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]]], dtype=uint8), array([[[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [214, 214, 214],\n",
              "         [216, 216, 216],\n",
              "         [ 28,  28,  28]],\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [213, 213, 213],\n",
              "         [215, 215, 215],\n",
              "         [ 27,  27,  27]],\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [211, 211, 211],\n",
              "         [214, 214, 214],\n",
              "         [ 26,  26,  26]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [177, 177, 177],\n",
              "         [174, 174, 174],\n",
              "         [ 23,  23,  23]],\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [177, 177, 177],\n",
              "         [174, 174, 174],\n",
              "         [ 24,  24,  24]],\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [176, 176, 176],\n",
              "         [174, 174, 174],\n",
              "         [ 24,  24,  24]]], dtype=uint8), array([[[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [ 31,  28,  24],\n",
              "         [ 30,  27,  23],\n",
              "         [ 29,  26,  22]],\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [ 33,  30,  26],\n",
              "         [ 32,  29,  25],\n",
              "         [ 32,  29,  25]],\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [ 36,  33,  29],\n",
              "         [ 36,  33,  29],\n",
              "         [ 35,  32,  28]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[220, 220, 220],\n",
              "         [220, 220, 220],\n",
              "         [220, 220, 220],\n",
              "         ...,\n",
              "         [151, 148, 144],\n",
              "         [147, 144, 140],\n",
              "         [145, 142, 138]],\n",
              " \n",
              "        [[218, 218, 218],\n",
              "         [218, 218, 218],\n",
              "         [218, 218, 218],\n",
              "         ...,\n",
              "         [152, 149, 145],\n",
              "         [148, 145, 141],\n",
              "         [145, 142, 138]],\n",
              " \n",
              "        [[217, 217, 217],\n",
              "         [218, 218, 218],\n",
              "         [219, 219, 219],\n",
              "         ...,\n",
              "         [152, 149, 145],\n",
              "         [149, 146, 142],\n",
              "         [146, 143, 139]]], dtype=uint8), array([[[ 19,  22,  26],\n",
              "         [ 24,  27,  31],\n",
              "         [ 27,  30,  34],\n",
              "         ...,\n",
              "         [ 27,  29,  30],\n",
              "         [ 27,  29,  30],\n",
              "         [ 27,  29,  30]],\n",
              " \n",
              "        [[ 19,  22,  26],\n",
              "         [ 24,  27,  31],\n",
              "         [ 27,  30,  34],\n",
              "         ...,\n",
              "         [ 27,  29,  30],\n",
              "         [ 27,  29,  30],\n",
              "         [ 27,  29,  30]],\n",
              " \n",
              "        [[ 19,  22,  26],\n",
              "         [ 24,  27,  31],\n",
              "         [ 27,  30,  34],\n",
              "         ...,\n",
              "         [ 27,  29,  30],\n",
              "         [ 27,  29,  30],\n",
              "         [ 27,  29,  30]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[ 56,  58,  59],\n",
              "         [126, 128, 129],\n",
              "         [127, 128, 132],\n",
              "         ...,\n",
              "         [ 96,  97, 101],\n",
              "         [ 85,  86,  90],\n",
              "         [ 74,  75,  79]],\n",
              " \n",
              "        [[ 61,  63,  64],\n",
              "         [126, 128, 129],\n",
              "         [123, 124, 128],\n",
              "         ...,\n",
              "         [ 97,  98, 102],\n",
              "         [ 87,  88,  92],\n",
              "         [ 78,  79,  83]],\n",
              " \n",
              "        [[ 28,  30,  31],\n",
              "         [ 87,  89,  90],\n",
              "         [ 78,  79,  83],\n",
              "         ...,\n",
              "         [ 55,  56,  60],\n",
              "         [ 46,  47,  51],\n",
              "         [ 39,  40,  44]]], dtype=uint8), array([[[77, 77, 77],\n",
              "         [78, 78, 78],\n",
              "         [80, 80, 80],\n",
              "         ...,\n",
              "         [64, 64, 64],\n",
              "         [61, 61, 61],\n",
              "         [60, 60, 60]],\n",
              " \n",
              "        [[81, 81, 81],\n",
              "         [83, 83, 83],\n",
              "         [84, 84, 84],\n",
              "         ...,\n",
              "         [71, 71, 71],\n",
              "         [68, 68, 68],\n",
              "         [66, 66, 66]],\n",
              " \n",
              "        [[85, 85, 85],\n",
              "         [89, 89, 89],\n",
              "         [89, 89, 89],\n",
              "         ...,\n",
              "         [75, 75, 75],\n",
              "         [72, 72, 72],\n",
              "         [69, 69, 69]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 4,  4,  4],\n",
              "         ...,\n",
              "         [ 3,  3,  3],\n",
              "         [ 3,  3,  3],\n",
              "         [ 3,  3,  3]],\n",
              " \n",
              "        [[ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 5,  5,  5],\n",
              "         ...,\n",
              "         [ 2,  2,  2],\n",
              "         [ 3,  3,  3],\n",
              "         [ 3,  3,  3]],\n",
              " \n",
              "        [[ 0,  0,  0],\n",
              "         [ 0,  0,  0],\n",
              "         [ 6,  6,  6],\n",
              "         ...,\n",
              "         [ 2,  2,  2],\n",
              "         [ 3,  3,  3],\n",
              "         [ 3,  3,  3]]], dtype=uint8), array([[[18, 18, 18],\n",
              "         [18, 18, 18],\n",
              "         [18, 18, 18],\n",
              "         ...,\n",
              "         [ 1,  1,  1],\n",
              "         [ 1,  1,  1],\n",
              "         [ 1,  1,  1]],\n",
              " \n",
              "        [[18, 18, 18],\n",
              "         [18, 18, 18],\n",
              "         [17, 17, 17],\n",
              "         ...,\n",
              "         [ 1,  1,  1],\n",
              "         [ 1,  1,  1],\n",
              "         [ 1,  1,  1]],\n",
              " \n",
              "        [[17, 17, 17],\n",
              "         [17, 17, 17],\n",
              "         [17, 17, 17],\n",
              "         ...,\n",
              "         [ 1,  1,  1],\n",
              "         [ 1,  1,  1],\n",
              "         [ 1,  1,  1]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[66, 66, 66],\n",
              "         [66, 66, 66],\n",
              "         [66, 66, 66],\n",
              "         ...,\n",
              "         [50, 50, 50],\n",
              "         [49, 49, 49],\n",
              "         [47, 47, 47]],\n",
              " \n",
              "        [[65, 65, 65],\n",
              "         [65, 65, 65],\n",
              "         [67, 67, 67],\n",
              "         ...,\n",
              "         [49, 49, 49],\n",
              "         [48, 48, 48],\n",
              "         [48, 48, 48]],\n",
              " \n",
              "        [[67, 67, 67],\n",
              "         [66, 66, 66],\n",
              "         [67, 67, 67],\n",
              "         ...,\n",
              "         [50, 50, 50],\n",
              "         [49, 49, 49],\n",
              "         [49, 49, 49]]], dtype=uint8), array([[[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]]], dtype=uint8), array([[[147, 147, 147],\n",
              "         [150, 150, 150],\n",
              "         [153, 153, 153],\n",
              "         ...,\n",
              "         [ 48,  48,  48],\n",
              "         [ 49,  49,  49],\n",
              "         [ 49,  49,  49]],\n",
              " \n",
              "        [[150, 150, 150],\n",
              "         [152, 152, 152],\n",
              "         [152, 152, 152],\n",
              "         ...,\n",
              "         [ 50,  50,  50],\n",
              "         [ 51,  51,  51],\n",
              "         [ 51,  51,  51]],\n",
              " \n",
              "        [[153, 153, 153],\n",
              "         [152, 152, 152],\n",
              "         [148, 148, 148],\n",
              "         ...,\n",
              "         [ 54,  54,  54],\n",
              "         [ 54,  54,  54],\n",
              "         [ 54,  54,  54]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0]]], dtype=uint8), array([[[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]]], dtype=uint8), array([[[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]]], dtype=uint8), array([[[ 97,  97,  97],\n",
              "         [ 92,  92,  92],\n",
              "         [ 93,  93,  93],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [117, 117, 117]],\n",
              " \n",
              "        [[103, 103, 103],\n",
              "         [ 96,  96,  96],\n",
              "         [ 95,  95,  95],\n",
              "         ...,\n",
              "         [  1,   1,   1],\n",
              "         [  0,   0,   0],\n",
              "         [118, 118, 118]],\n",
              " \n",
              "        [[102, 102, 102],\n",
              "         [ 99,  99,  99],\n",
              "         [ 98,  98,  98],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [117, 117, 117]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[110, 110, 110],\n",
              "         [114, 114, 114],\n",
              "         [120, 120, 120],\n",
              "         ...,\n",
              "         [ 54,  54,  54],\n",
              "         [ 67,  67,  67],\n",
              "         [111, 111, 111]],\n",
              " \n",
              "        [[107, 107, 107],\n",
              "         [113, 113, 113],\n",
              "         [121, 121, 121],\n",
              "         ...,\n",
              "         [ 55,  55,  55],\n",
              "         [ 68,  68,  68],\n",
              "         [111, 111, 111]],\n",
              " \n",
              "        [[109, 109, 109],\n",
              "         [115, 115, 115],\n",
              "         [123, 123, 123],\n",
              "         ...,\n",
              "         [ 56,  56,  56],\n",
              "         [ 68,  68,  68],\n",
              "         [112, 112, 112]]], dtype=uint8), array([[[  9,   9,   9],\n",
              "         [  9,   9,   9],\n",
              "         [  9,   9,   9],\n",
              "         ...,\n",
              "         [ 68,  68,  68],\n",
              "         [173, 173, 173],\n",
              "         [173, 173, 173]],\n",
              " \n",
              "        [[  9,   9,   9],\n",
              "         [  9,   9,   9],\n",
              "         [  9,   9,   9],\n",
              "         ...,\n",
              "         [ 52,  52,  52],\n",
              "         [175, 175, 175],\n",
              "         [176, 176, 176]],\n",
              " \n",
              "        [[  9,   9,   9],\n",
              "         [  9,   9,   9],\n",
              "         [  9,   9,   9],\n",
              "         ...,\n",
              "         [ 29,  29,  29],\n",
              "         [173, 173, 173],\n",
              "         [172, 172, 172]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[166, 166, 166],\n",
              "         [166, 166, 166],\n",
              "         [169, 169, 169],\n",
              "         ...,\n",
              "         [163, 163, 163],\n",
              "         [169, 169, 169],\n",
              "         [169, 169, 169]],\n",
              " \n",
              "        [[167, 167, 167],\n",
              "         [167, 167, 167],\n",
              "         [170, 170, 170],\n",
              "         ...,\n",
              "         [160, 160, 160],\n",
              "         [168, 168, 168],\n",
              "         [169, 169, 169]],\n",
              " \n",
              "        [[168, 168, 168],\n",
              "         [168, 168, 168],\n",
              "         [172, 172, 172],\n",
              "         ...,\n",
              "         [160, 160, 160],\n",
              "         [170, 170, 170],\n",
              "         [170, 170, 170]]], dtype=uint8), array([[[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]],\n",
              " \n",
              "        [[0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0],\n",
              "         [0, 0, 0]]], dtype=uint8), array([[[  0,   0,   0],\n",
              "         [  2,   2,   2],\n",
              "         [  0,   0,   0],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  7,   7,   7],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[  0,   0,   0],\n",
              "         [  8,   8,   8],\n",
              "         [  4,   4,   4],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  6,   6,   6],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        [[  2,   2,   2],\n",
              "         [ 12,  12,  12],\n",
              "         [  7,   7,   7],\n",
              "         ...,\n",
              "         [ 16,  16,  16],\n",
              "         [ 28,  28,  28],\n",
              "         [ 14,  14,  14]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[123, 123, 123],\n",
              "         [120, 120, 120],\n",
              "         [116, 116, 116],\n",
              "         ...,\n",
              "         [187, 187, 187],\n",
              "         [183, 183, 183],\n",
              "         [179, 179, 179]],\n",
              " \n",
              "        [[126, 126, 126],\n",
              "         [123, 123, 123],\n",
              "         [120, 120, 120],\n",
              "         ...,\n",
              "         [184, 184, 184],\n",
              "         [181, 181, 181],\n",
              "         [177, 177, 177]],\n",
              " \n",
              "        [[130, 130, 130],\n",
              "         [129, 129, 129],\n",
              "         [127, 127, 127],\n",
              "         ...,\n",
              "         [177, 177, 177],\n",
              "         [174, 174, 174],\n",
              "         [171, 171, 171]]], dtype=uint8), array([[[100,  51,   3],\n",
              "         [100,  51,   3],\n",
              "         [100,  51,   3],\n",
              "         ...,\n",
              "         [ 99,  50,   6],\n",
              "         [ 96,  47,   1],\n",
              "         [ 99,  53,   0]],\n",
              " \n",
              "        [[ 73,  39,  10],\n",
              "         [ 73,  39,  10],\n",
              "         [ 73,  39,  10],\n",
              "         ...,\n",
              "         [ 70,  34,  10],\n",
              "         [ 70,  36,   7],\n",
              "         [ 77,  44,   5]],\n",
              " \n",
              "        [[ 14,   0,   0],\n",
              "         [ 14,   0,   0],\n",
              "         [ 14,   0,   0],\n",
              "         ...,\n",
              "         [ 20,   0,   1],\n",
              "         [ 22,   4,   0],\n",
              "         [ 31,  17,   0]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[ 73,  86,  88],\n",
              "         [130, 133, 137],\n",
              "         [130, 126, 131],\n",
              "         ...,\n",
              "         [210, 224, 222],\n",
              "         [116, 131, 127],\n",
              "         [ 24,   6,   5]],\n",
              " \n",
              "        [[ 86,  85,  89],\n",
              "         [126, 123, 125],\n",
              "         [139, 135, 134],\n",
              "         ...,\n",
              "         [230, 223, 220],\n",
              "         [140, 125, 122],\n",
              "         [ 39,  17,  11]],\n",
              " \n",
              "        [[ 35,  34,  38],\n",
              "         [ 71,  68,  70],\n",
              "         [ 77,  73,  72],\n",
              "         ...,\n",
              "         [135, 128, 125],\n",
              "         [ 80,  65,  62],\n",
              "         [ 24,   2,   0]]], dtype=uint8), array([[[ 69,  38,   7],\n",
              "         [ 69,  38,   7],\n",
              "         [ 69,  38,   7],\n",
              "         ...,\n",
              "         [ 75,  38,  12],\n",
              "         [ 75,  38,  12],\n",
              "         [ 73,  36,  10]],\n",
              " \n",
              "        [[ 14,   4,   0],\n",
              "         [ 14,   4,   0],\n",
              "         [ 14,   4,   0],\n",
              "         ...,\n",
              "         [ 19,   4,   1],\n",
              "         [ 18,   3,   0],\n",
              "         [ 17,   2,   0]],\n",
              " \n",
              "        [[  0,   0,  10],\n",
              "         [  0,   0,  10],\n",
              "         [  0,   0,  10],\n",
              "         ...,\n",
              "         [  2,   1,  11],\n",
              "         [  1,   0,  10],\n",
              "         [  0,   0,   9]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[ 79,  84,  83],\n",
              "         [128, 133, 132],\n",
              "         [124, 129, 128],\n",
              "         ...,\n",
              "         [210, 214, 209],\n",
              "         [211, 215, 210],\n",
              "         [219, 223, 218]],\n",
              " \n",
              "        [[ 78,  83,  82],\n",
              "         [130, 135, 134],\n",
              "         [129, 134, 133],\n",
              "         ...,\n",
              "         [218, 222, 216],\n",
              "         [218, 222, 216],\n",
              "         [226, 230, 224]],\n",
              " \n",
              "        [[ 40,  37,  46],\n",
              "         [ 71,  67,  73],\n",
              "         [ 78,  73,  75],\n",
              "         ...,\n",
              "         [120, 120, 120],\n",
              "         [124, 119, 120],\n",
              "         [128, 120, 121]]], dtype=uint8), array([[[171, 171, 171],\n",
              "         [154, 154, 154],\n",
              "         [165, 165, 165],\n",
              "         ...,\n",
              "         [ 14,  14,  14],\n",
              "         [ 13,  13,  13],\n",
              "         [ 13,  13,  13]],\n",
              " \n",
              "        [[147, 147, 147],\n",
              "         [110, 110, 110],\n",
              "         [103, 103, 103],\n",
              "         ...,\n",
              "         [  0,   0,   0],\n",
              "         [  0,   0,   0],\n",
              "         [  1,   1,   1]],\n",
              " \n",
              "        [[127, 127, 127],\n",
              "         [108, 108, 108],\n",
              "         [110, 110, 110],\n",
              "         ...,\n",
              "         [  1,   1,   1],\n",
              "         [  1,   1,   1],\n",
              "         [  0,   0,   0]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[146, 146, 146],\n",
              "         [137, 137, 137],\n",
              "         [154, 154, 154],\n",
              "         ...,\n",
              "         [154, 154, 154],\n",
              "         [152, 152, 152],\n",
              "         [149, 149, 149]],\n",
              " \n",
              "        [[150, 150, 150],\n",
              "         [141, 141, 141],\n",
              "         [152, 152, 152],\n",
              "         ...,\n",
              "         [156, 156, 156],\n",
              "         [152, 152, 152],\n",
              "         [152, 152, 152]],\n",
              " \n",
              "        [[151, 151, 151],\n",
              "         [144, 144, 144],\n",
              "         [149, 149, 149],\n",
              "         ...,\n",
              "         [159, 159, 159],\n",
              "         [154, 154, 154],\n",
              "         [147, 147, 147]]], dtype=uint8), array([[[7, 7, 7],\n",
              "         [7, 7, 7],\n",
              "         [7, 7, 7],\n",
              "         ...,\n",
              "         [7, 7, 7],\n",
              "         [7, 7, 7],\n",
              "         [7, 7, 7]],\n",
              " \n",
              "        [[7, 7, 7],\n",
              "         [7, 7, 7],\n",
              "         [7, 7, 7],\n",
              "         ...,\n",
              "         [7, 7, 7],\n",
              "         [7, 7, 7],\n",
              "         [7, 7, 7]],\n",
              " \n",
              "        [[7, 7, 7],\n",
              "         [7, 7, 7],\n",
              "         [7, 7, 7],\n",
              "         ...,\n",
              "         [7, 7, 7],\n",
              "         [7, 7, 7],\n",
              "         [7, 7, 7]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[6, 6, 6],\n",
              "         [6, 6, 6],\n",
              "         [6, 6, 6],\n",
              "         ...,\n",
              "         [7, 7, 7],\n",
              "         [7, 7, 7],\n",
              "         [7, 7, 7]],\n",
              " \n",
              "        [[6, 6, 6],\n",
              "         [6, 6, 6],\n",
              "         [6, 6, 6],\n",
              "         ...,\n",
              "         [7, 7, 7],\n",
              "         [7, 7, 7],\n",
              "         [7, 7, 7]],\n",
              " \n",
              "        [[6, 6, 6],\n",
              "         [6, 6, 6],\n",
              "         [6, 6, 6],\n",
              "         ...,\n",
              "         [7, 7, 7],\n",
              "         [7, 7, 7],\n",
              "         [7, 7, 7]]], dtype=uint8)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGjQ9XAaindg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "9518e1b4-5bac-44fd-aea3-0dac9b5a9976"
      },
      "source": [
        "for i in range(100):\n",
        "  plt.subplot(10,10,1+i)\n",
        "  plt.axis('off')\n",
        "  plt.imshow(covid_images[i])\n",
        "plt.show()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 100 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2VJ3LRlkePj",
        "outputId": "19e6ac79-1e2b-46ab-aa9b-f8760541f495"
      },
      "source": [
        "type(covid_images)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmuXmEKILoKk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bd28705-dfcb-4f98-cf4b-b1c1c08bcd9a"
      },
      "source": [
        "covid_array = np.array(covid_images)\n",
        "type(covid_array)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpTd-H4COZ9-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13f8e60c-e00c-417d-f3f3-7ae323436e55"
      },
      "source": [
        "covid_array.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54IgcR9hP9lN",
        "outputId": "d6b686d9-b503-4815-9662-dfffd4b58943"
      },
      "source": [
        "# create data generator\n",
        "datagen = ImageDataGenerator()\n",
        "\n",
        "datagen = datagen.flow_from_directory(\n",
        "    directory = \"/content/drive/MyDrive/Masters/Semester 2/Data Mining & Visualisation/Assessments/Assessment1/Code/covid19_lungs_dataset\",\n",
        "    target_size = (256 , 256),\n",
        "    color_mode = \"grayscale\",\n",
        "    batch_size = 256,\n",
        "    class_mode = 'binary',\n",
        "    shuffle = True,\n",
        "    seed = 42\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 100 images belonging to 1 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZolNUQIIss7"
      },
      "source": [
        "X_train, y_train = datagen.next()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqL3YWAF7TdH",
        "outputId": "661dc6d6-13c8-4cd0-8305-07b0ad4983c0"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
        "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class GAN():\n",
        "    def __init__(self):\n",
        "        self.img_rows = 256\n",
        "        self.img_cols = 256\n",
        "        self.channels = 1\n",
        "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "        self.latent_dim = 100\n",
        "\n",
        "        optimizer = Adam(0.001, 0.5)\n",
        "\n",
        "        # Build and compile the discriminator\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        self.discriminator.compile(loss='binary_crossentropy',\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "        # Build the generator\n",
        "        self.generator = self.build_generator()\n",
        "\n",
        "        # The generator takes noise as input and generates imgs\n",
        "        z = Input(shape=(self.latent_dim,))\n",
        "        img = self.generator(z)\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # The discriminator takes generated images as input and determines validity\n",
        "        validity = self.discriminator(img)\n",
        "\n",
        "        # The combined model  (stacked generator and discriminator)\n",
        "        # Trains the generator to fool the discriminator\n",
        "        self.combined = Model(z, validity)\n",
        "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "\n",
        "    def build_generator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Dense(256, input_dim=self.latent_dim))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Dense(512))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Dense(1024))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
        "        model.add(Reshape(self.img_shape))\n",
        "\n",
        "        model.summary()\n",
        "\n",
        "        noise = Input(shape=(self.latent_dim,))\n",
        "        img = model(noise)\n",
        "\n",
        "        return Model(noise, img)\n",
        "\n",
        "    def build_discriminator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Flatten(input_shape=self.img_shape))\n",
        "        model.add(Dense(512))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dense(256))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        model.summary()\n",
        "\n",
        "        img = Input(shape=self.img_shape)\n",
        "        validity = model(img)\n",
        "\n",
        "        return Model(img, validity)\n",
        "\n",
        "    def train(self, epochs, batch_size=256, sample_interval=50):\n",
        "\n",
        "        # Load the dataset\n",
        "        X_train, y_train = datagen.next()\n",
        "\n",
        "        # Rescale -1 to 1\n",
        "        X_train = X_train / 127.5 - 1.\n",
        "        X_train = np.expand_dims(X_train, axis=3)\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = np.ones((batch_size, 1))\n",
        "        fake = np.zeros((batch_size, 1))\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "\n",
        "            # Select a random batch of images\n",
        "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "            imgs = X_train[idx]\n",
        "\n",
        "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
        "\n",
        "            # Generate a batch of new images\n",
        "            gen_imgs = self.generator.predict(noise)\n",
        "\n",
        "            # Train the discriminator\n",
        "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
        "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Generator\n",
        "            # ---------------------\n",
        "\n",
        "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
        "\n",
        "            # Train the generator (to have the discriminator label samples as valid)\n",
        "            g_loss = self.combined.train_on_batch(noise, valid)\n",
        "\n",
        "            # Plot the progress\n",
        "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "\n",
        "            # If at save interval => save generated image samples\n",
        "            if epoch % sample_interval == 0:\n",
        "                self.sample_images(epoch)\n",
        "\n",
        "    def sample_images(self, epoch):\n",
        "        r, c = 2, 2\n",
        "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
        "        gen_imgs = self.generator.predict(noise)\n",
        "\n",
        "        # Rescale images 0 - 1\n",
        "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "        fig, axs = plt.subplots(r, c)\n",
        "        cnt = 0\n",
        "        for i in range(r):\n",
        "            for j in range(c):\n",
        "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
        "                axs[i,j].axis('off')\n",
        "                cnt += 1\n",
        "        fig.savefig(\"/content/drive/MyDrive/Masters/Semester 2/Data Mining & Visualisation/Assessments/Assessment1/Code/new_images_final/\" + str(epoch))\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    gan = GAN()\n",
        "    gan.train(epochs=20000, batch_size=256, sample_interval=50)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "15000 [D loss: 0.035074, acc.: 99.22%] [G loss: 5.548608]\n",
            "15001 [D loss: 0.066227, acc.: 98.24%] [G loss: 7.054020]\n",
            "15002 [D loss: 0.103486, acc.: 97.07%] [G loss: 5.765364]\n",
            "15003 [D loss: 0.013077, acc.: 100.00%] [G loss: 5.668079]\n",
            "15004 [D loss: 0.030042, acc.: 99.41%] [G loss: 5.403129]\n",
            "15005 [D loss: 0.064669, acc.: 98.05%] [G loss: 6.242370]\n",
            "15006 [D loss: 0.052726, acc.: 98.05%] [G loss: 5.923774]\n",
            "15007 [D loss: 0.068217, acc.: 98.05%] [G loss: 6.325775]\n",
            "15008 [D loss: 0.029767, acc.: 99.41%] [G loss: 5.540839]\n",
            "15009 [D loss: 0.039559, acc.: 99.02%] [G loss: 5.798642]\n",
            "15010 [D loss: 0.041830, acc.: 98.63%] [G loss: 5.364816]\n",
            "15011 [D loss: 0.042404, acc.: 98.44%] [G loss: 5.554136]\n",
            "15012 [D loss: 0.019907, acc.: 100.00%] [G loss: 6.522004]\n",
            "15013 [D loss: 0.025081, acc.: 100.00%] [G loss: 5.882571]\n",
            "15014 [D loss: 0.021215, acc.: 99.41%] [G loss: 5.294509]\n",
            "15015 [D loss: 0.032864, acc.: 99.80%] [G loss: 6.113298]\n",
            "15016 [D loss: 0.040401, acc.: 99.80%] [G loss: 5.424930]\n",
            "15017 [D loss: 0.102525, acc.: 96.88%] [G loss: 7.524306]\n",
            "15018 [D loss: 0.230262, acc.: 95.12%] [G loss: 4.666052]\n",
            "15019 [D loss: 0.060445, acc.: 97.66%] [G loss: 6.326190]\n",
            "15020 [D loss: 0.039187, acc.: 99.02%] [G loss: 5.353372]\n",
            "15021 [D loss: 0.044283, acc.: 98.63%] [G loss: 5.566146]\n",
            "15022 [D loss: 0.021458, acc.: 99.02%] [G loss: 6.469032]\n",
            "15023 [D loss: 0.014846, acc.: 99.41%] [G loss: 7.070671]\n",
            "15024 [D loss: 0.033067, acc.: 99.02%] [G loss: 6.389228]\n",
            "15025 [D loss: 0.014881, acc.: 99.41%] [G loss: 5.796735]\n",
            "15026 [D loss: 0.029081, acc.: 99.41%] [G loss: 6.045624]\n",
            "15027 [D loss: 0.043046, acc.: 98.83%] [G loss: 6.226000]\n",
            "15028 [D loss: 0.024227, acc.: 99.41%] [G loss: 5.100833]\n",
            "15029 [D loss: 0.119883, acc.: 95.31%] [G loss: 9.367252]\n",
            "15030 [D loss: 0.176577, acc.: 95.51%] [G loss: 5.752399]\n",
            "15031 [D loss: 0.148717, acc.: 95.31%] [G loss: 6.052682]\n",
            "15032 [D loss: 0.043911, acc.: 99.22%] [G loss: 6.532061]\n",
            "15033 [D loss: 0.053006, acc.: 99.22%] [G loss: 4.978261]\n",
            "15034 [D loss: 0.032307, acc.: 100.00%] [G loss: 5.694708]\n",
            "15035 [D loss: 0.076441, acc.: 98.05%] [G loss: 6.119193]\n",
            "15036 [D loss: 0.019843, acc.: 99.41%] [G loss: 6.125029]\n",
            "15037 [D loss: 0.064157, acc.: 98.83%] [G loss: 5.355596]\n",
            "15038 [D loss: 0.021532, acc.: 98.83%] [G loss: 5.870801]\n",
            "15039 [D loss: 0.012982, acc.: 100.00%] [G loss: 5.658089]\n",
            "15040 [D loss: 0.030250, acc.: 100.00%] [G loss: 5.634655]\n",
            "15041 [D loss: 0.071485, acc.: 98.44%] [G loss: 5.572844]\n",
            "15042 [D loss: 0.028177, acc.: 99.22%] [G loss: 5.751962]\n",
            "15043 [D loss: 0.043727, acc.: 99.02%] [G loss: 5.275109]\n",
            "15044 [D loss: 0.044239, acc.: 98.44%] [G loss: 5.775782]\n",
            "15045 [D loss: 0.041611, acc.: 99.02%] [G loss: 5.786145]\n",
            "15046 [D loss: 0.016790, acc.: 99.41%] [G loss: 5.755395]\n",
            "15047 [D loss: 0.067709, acc.: 98.44%] [G loss: 6.739289]\n",
            "15048 [D loss: 0.042137, acc.: 99.02%] [G loss: 5.400290]\n",
            "15049 [D loss: 0.096235, acc.: 97.66%] [G loss: 8.701503]\n",
            "15050 [D loss: 0.141753, acc.: 97.07%] [G loss: 5.050157]\n",
            "15051 [D loss: 0.089171, acc.: 96.48%] [G loss: 7.706118]\n",
            "15052 [D loss: 0.050576, acc.: 99.22%] [G loss: 7.146337]\n",
            "15053 [D loss: 0.069122, acc.: 99.02%] [G loss: 4.834977]\n",
            "15054 [D loss: 0.026590, acc.: 100.00%] [G loss: 7.329511]\n",
            "15055 [D loss: 0.026659, acc.: 98.83%] [G loss: 5.493237]\n",
            "15056 [D loss: 0.027996, acc.: 100.00%] [G loss: 6.037113]\n",
            "15057 [D loss: 0.010920, acc.: 99.61%] [G loss: 6.501792]\n",
            "15058 [D loss: 0.032964, acc.: 99.22%] [G loss: 4.813526]\n",
            "15059 [D loss: 0.015921, acc.: 100.00%] [G loss: 5.437067]\n",
            "15060 [D loss: 0.035502, acc.: 99.02%] [G loss: 6.399396]\n",
            "15061 [D loss: 0.027723, acc.: 99.61%] [G loss: 6.126101]\n",
            "15062 [D loss: 0.006023, acc.: 100.00%] [G loss: 7.457952]\n",
            "15063 [D loss: 0.124893, acc.: 95.70%] [G loss: 7.433921]\n",
            "15064 [D loss: 0.047545, acc.: 99.02%] [G loss: 5.828456]\n",
            "15065 [D loss: 0.040779, acc.: 99.02%] [G loss: 5.701384]\n",
            "15066 [D loss: 0.056175, acc.: 98.63%] [G loss: 6.992783]\n",
            "15067 [D loss: 0.062040, acc.: 98.63%] [G loss: 5.456135]\n",
            "15068 [D loss: 0.012784, acc.: 100.00%] [G loss: 6.299770]\n",
            "15069 [D loss: 0.185084, acc.: 89.45%] [G loss: 14.046255]\n",
            "15070 [D loss: 0.930867, acc.: 86.33%] [G loss: 7.299476]\n",
            "15071 [D loss: 0.229229, acc.: 93.75%] [G loss: 10.573207]\n",
            "15072 [D loss: 0.078655, acc.: 99.02%] [G loss: 10.765880]\n",
            "15073 [D loss: 0.263962, acc.: 96.48%] [G loss: 7.370879]\n",
            "15074 [D loss: 0.052385, acc.: 98.24%] [G loss: 9.283389]\n",
            "15075 [D loss: 0.024345, acc.: 99.41%] [G loss: 9.628588]\n",
            "15076 [D loss: 0.096091, acc.: 98.63%] [G loss: 7.145044]\n",
            "15077 [D loss: 0.052713, acc.: 99.22%] [G loss: 8.847992]\n",
            "15078 [D loss: 0.043454, acc.: 99.22%] [G loss: 9.123903]\n",
            "15079 [D loss: 0.119438, acc.: 97.85%] [G loss: 8.609556]\n",
            "15080 [D loss: 0.010900, acc.: 99.61%] [G loss: 8.202835]\n",
            "15081 [D loss: 0.065978, acc.: 98.83%] [G loss: 6.527712]\n",
            "15082 [D loss: 0.024540, acc.: 99.41%] [G loss: 7.505787]\n",
            "15083 [D loss: 0.044980, acc.: 99.02%] [G loss: 6.979658]\n",
            "15084 [D loss: 0.047590, acc.: 99.02%] [G loss: 6.737490]\n",
            "15085 [D loss: 0.075568, acc.: 98.44%] [G loss: 7.155773]\n",
            "15086 [D loss: 0.014939, acc.: 99.61%] [G loss: 7.272885]\n",
            "15087 [D loss: 0.083304, acc.: 98.83%] [G loss: 6.558649]\n",
            "15088 [D loss: 0.032065, acc.: 99.22%] [G loss: 6.469704]\n",
            "15089 [D loss: 0.026487, acc.: 99.41%] [G loss: 6.731915]\n",
            "15090 [D loss: 0.088137, acc.: 98.24%] [G loss: 8.199469]\n",
            "15091 [D loss: 0.399940, acc.: 83.40%] [G loss: 12.175542]\n",
            "15092 [D loss: 0.274774, acc.: 96.68%] [G loss: 11.583870]\n",
            "15093 [D loss: 0.441030, acc.: 95.12%] [G loss: 6.217963]\n",
            "15094 [D loss: 0.132717, acc.: 95.31%] [G loss: 6.560619]\n",
            "15095 [D loss: 0.012042, acc.: 99.61%] [G loss: 7.220175]\n",
            "15096 [D loss: 0.042568, acc.: 99.02%] [G loss: 5.767620]\n",
            "15097 [D loss: 0.019415, acc.: 100.00%] [G loss: 6.123820]\n",
            "15098 [D loss: 0.020502, acc.: 99.22%] [G loss: 5.773465]\n",
            "15099 [D loss: 0.040197, acc.: 98.83%] [G loss: 5.822671]\n",
            "15100 [D loss: 0.052417, acc.: 98.44%] [G loss: 6.191410]\n",
            "15101 [D loss: 0.016748, acc.: 99.41%] [G loss: 6.653152]\n",
            "15102 [D loss: 0.044559, acc.: 98.83%] [G loss: 5.225451]\n",
            "15103 [D loss: 0.018241, acc.: 99.41%] [G loss: 7.262062]\n",
            "15104 [D loss: 0.022894, acc.: 99.41%] [G loss: 5.981943]\n",
            "15105 [D loss: 0.031516, acc.: 99.02%] [G loss: 6.318362]\n",
            "15106 [D loss: 0.019197, acc.: 99.22%] [G loss: 6.184939]\n",
            "15107 [D loss: 0.015674, acc.: 99.61%] [G loss: 5.914300]\n",
            "15108 [D loss: 0.044618, acc.: 98.63%] [G loss: 6.070899]\n",
            "15109 [D loss: 0.040398, acc.: 98.83%] [G loss: 5.408361]\n",
            "15110 [D loss: 0.026691, acc.: 99.22%] [G loss: 6.441442]\n",
            "15111 [D loss: 0.096304, acc.: 97.66%] [G loss: 7.224784]\n",
            "15112 [D loss: 0.128273, acc.: 97.66%] [G loss: 6.109694]\n",
            "15113 [D loss: 0.041612, acc.: 98.83%] [G loss: 5.894258]\n",
            "15114 [D loss: 0.040945, acc.: 98.83%] [G loss: 5.262944]\n",
            "15115 [D loss: 0.035533, acc.: 99.80%] [G loss: 5.965683]\n",
            "15116 [D loss: 0.037469, acc.: 99.22%] [G loss: 5.288453]\n",
            "15117 [D loss: 0.074075, acc.: 98.63%] [G loss: 6.870666]\n",
            "15118 [D loss: 0.049848, acc.: 98.05%] [G loss: 5.185176]\n",
            "15119 [D loss: 0.072691, acc.: 97.66%] [G loss: 6.829969]\n",
            "15120 [D loss: 0.052165, acc.: 98.63%] [G loss: 5.414674]\n",
            "15121 [D loss: 0.040480, acc.: 99.22%] [G loss: 5.211657]\n",
            "15122 [D loss: 0.059960, acc.: 98.44%] [G loss: 6.174976]\n",
            "15123 [D loss: 0.103785, acc.: 96.68%] [G loss: 6.551165]\n",
            "15124 [D loss: 0.023699, acc.: 99.41%] [G loss: 5.505793]\n",
            "15125 [D loss: 0.094731, acc.: 98.24%] [G loss: 7.070104]\n",
            "15126 [D loss: 0.024237, acc.: 99.22%] [G loss: 6.478792]\n",
            "15127 [D loss: 0.119210, acc.: 98.05%] [G loss: 8.388082]\n",
            "15128 [D loss: 0.120421, acc.: 98.05%] [G loss: 6.086241]\n",
            "15129 [D loss: 0.184275, acc.: 95.70%] [G loss: 7.475363]\n",
            "15130 [D loss: 0.078016, acc.: 98.83%] [G loss: 7.301728]\n",
            "15131 [D loss: 0.112741, acc.: 98.44%] [G loss: 4.392453]\n",
            "15132 [D loss: 0.100783, acc.: 96.68%] [G loss: 7.073339]\n",
            "15133 [D loss: 0.127499, acc.: 98.05%] [G loss: 5.614973]\n",
            "15134 [D loss: 0.049386, acc.: 99.02%] [G loss: 5.217550]\n",
            "15135 [D loss: 0.015910, acc.: 99.61%] [G loss: 5.798147]\n",
            "15136 [D loss: 0.044515, acc.: 99.02%] [G loss: 4.491899]\n",
            "15137 [D loss: 0.022069, acc.: 100.00%] [G loss: 5.417505]\n",
            "15138 [D loss: 0.089885, acc.: 98.44%] [G loss: 5.124407]\n",
            "15139 [D loss: 0.020425, acc.: 99.41%] [G loss: 6.153605]\n",
            "15140 [D loss: 0.069500, acc.: 98.63%] [G loss: 4.484951]\n",
            "15141 [D loss: 0.020332, acc.: 100.00%] [G loss: 6.362430]\n",
            "15142 [D loss: 0.036939, acc.: 98.83%] [G loss: 5.297528]\n",
            "15143 [D loss: 0.031257, acc.: 98.63%] [G loss: 4.226896]\n",
            "15144 [D loss: 0.024301, acc.: 100.00%] [G loss: 6.773548]\n",
            "15145 [D loss: 0.065097, acc.: 98.44%] [G loss: 4.701513]\n",
            "15146 [D loss: 0.014043, acc.: 100.00%] [G loss: 5.376807]\n",
            "15147 [D loss: 0.072271, acc.: 98.63%] [G loss: 7.008932]\n",
            "15148 [D loss: 0.070108, acc.: 98.44%] [G loss: 5.108672]\n",
            "15149 [D loss: 0.047418, acc.: 99.02%] [G loss: 6.508814]\n",
            "15150 [D loss: 0.048942, acc.: 99.02%] [G loss: 5.635103]\n",
            "15151 [D loss: 0.064655, acc.: 98.05%] [G loss: 5.369742]\n",
            "15152 [D loss: 0.020162, acc.: 99.41%] [G loss: 5.249050]\n",
            "15153 [D loss: 0.080023, acc.: 98.83%] [G loss: 6.891874]\n",
            "15154 [D loss: 0.210342, acc.: 95.31%] [G loss: 7.264753]\n",
            "15155 [D loss: 0.086727, acc.: 97.85%] [G loss: 5.521445]\n",
            "15156 [D loss: 0.182851, acc.: 95.12%] [G loss: 7.709799]\n",
            "15157 [D loss: 0.077204, acc.: 98.63%] [G loss: 6.894462]\n",
            "15158 [D loss: 0.056392, acc.: 99.02%] [G loss: 5.262609]\n",
            "15159 [D loss: 0.085878, acc.: 98.05%] [G loss: 6.135387]\n",
            "15160 [D loss: 0.048431, acc.: 99.02%] [G loss: 5.767011]\n",
            "15161 [D loss: 0.093585, acc.: 98.05%] [G loss: 8.245067]\n",
            "15162 [D loss: 0.032469, acc.: 99.22%] [G loss: 6.803275]\n",
            "15163 [D loss: 0.066290, acc.: 98.63%] [G loss: 9.351886]\n",
            "15164 [D loss: 0.021271, acc.: 100.00%] [G loss: 6.610141]\n",
            "15165 [D loss: 0.026141, acc.: 99.41%] [G loss: 5.565776]\n",
            "15166 [D loss: 0.043974, acc.: 98.83%] [G loss: 5.877825]\n",
            "15167 [D loss: 0.035774, acc.: 99.02%] [G loss: 5.028990]\n",
            "15168 [D loss: 0.055134, acc.: 98.83%] [G loss: 6.185369]\n",
            "15169 [D loss: 0.049146, acc.: 99.02%] [G loss: 5.227443]\n",
            "15170 [D loss: 0.022684, acc.: 99.61%] [G loss: 5.455216]\n",
            "15171 [D loss: 0.047874, acc.: 99.02%] [G loss: 5.465265]\n",
            "15172 [D loss: 0.031757, acc.: 99.22%] [G loss: 5.996018]\n",
            "15173 [D loss: 0.059114, acc.: 98.44%] [G loss: 5.874587]\n",
            "15174 [D loss: 0.030260, acc.: 98.83%] [G loss: 5.755732]\n",
            "15175 [D loss: 0.063375, acc.: 98.63%] [G loss: 5.261064]\n",
            "15176 [D loss: 0.040171, acc.: 98.63%] [G loss: 7.134264]\n",
            "15177 [D loss: 0.029332, acc.: 99.41%] [G loss: 5.343034]\n",
            "15178 [D loss: 0.025626, acc.: 99.22%] [G loss: 5.665066]\n",
            "15179 [D loss: 0.042287, acc.: 98.83%] [G loss: 6.807664]\n",
            "15180 [D loss: 0.057387, acc.: 98.44%] [G loss: 5.379470]\n",
            "15181 [D loss: 0.064948, acc.: 99.61%] [G loss: 7.148702]\n",
            "15182 [D loss: 0.103039, acc.: 97.85%] [G loss: 5.477275]\n",
            "15183 [D loss: 0.045243, acc.: 99.02%] [G loss: 5.545629]\n",
            "15184 [D loss: 0.028709, acc.: 99.41%] [G loss: 5.663245]\n",
            "15185 [D loss: 0.103148, acc.: 98.44%] [G loss: 7.819652]\n",
            "15186 [D loss: 0.426178, acc.: 85.16%] [G loss: 14.453474]\n",
            "15187 [D loss: 0.637644, acc.: 90.82%] [G loss: 6.788164]\n",
            "15188 [D loss: 0.139278, acc.: 95.90%] [G loss: 8.269320]\n",
            "15189 [D loss: 0.076381, acc.: 99.02%] [G loss: 7.422441]\n",
            "15190 [D loss: 0.084407, acc.: 98.63%] [G loss: 6.098301]\n",
            "15191 [D loss: 0.037160, acc.: 99.41%] [G loss: 5.925674]\n",
            "15192 [D loss: 0.071164, acc.: 98.63%] [G loss: 5.365839]\n",
            "15193 [D loss: 0.009257, acc.: 99.80%] [G loss: 5.781688]\n",
            "15194 [D loss: 0.132267, acc.: 97.07%] [G loss: 7.100276]\n",
            "15195 [D loss: 0.145172, acc.: 95.31%] [G loss: 6.132127]\n",
            "15196 [D loss: 0.047569, acc.: 99.22%] [G loss: 5.846241]\n",
            "15197 [D loss: 0.094821, acc.: 98.24%] [G loss: 4.843839]\n",
            "15198 [D loss: 0.081682, acc.: 99.80%] [G loss: 7.539394]\n",
            "15199 [D loss: 0.145450, acc.: 97.27%] [G loss: 4.865705]\n",
            "15200 [D loss: 0.062492, acc.: 98.63%] [G loss: 6.727455]\n",
            "15201 [D loss: 0.064242, acc.: 98.83%] [G loss: 6.097455]\n",
            "15202 [D loss: 0.035609, acc.: 99.41%] [G loss: 5.028769]\n",
            "15203 [D loss: 0.067062, acc.: 98.83%] [G loss: 5.672265]\n",
            "15204 [D loss: 0.061807, acc.: 98.44%] [G loss: 5.436083]\n",
            "15205 [D loss: 0.055464, acc.: 98.83%] [G loss: 4.731522]\n",
            "15206 [D loss: 0.018626, acc.: 100.00%] [G loss: 5.124808]\n",
            "15207 [D loss: 0.040921, acc.: 99.02%] [G loss: 5.517962]\n",
            "15208 [D loss: 0.048623, acc.: 98.83%] [G loss: 4.607157]\n",
            "15209 [D loss: 0.026339, acc.: 99.02%] [G loss: 6.262471]\n",
            "15210 [D loss: 0.021389, acc.: 99.41%] [G loss: 5.094322]\n",
            "15211 [D loss: 0.029501, acc.: 98.83%] [G loss: 4.826465]\n",
            "15212 [D loss: 0.025573, acc.: 100.00%] [G loss: 6.567647]\n",
            "15213 [D loss: 0.090497, acc.: 98.63%] [G loss: 7.053461]\n",
            "15214 [D loss: 0.073160, acc.: 97.66%] [G loss: 4.939219]\n",
            "15215 [D loss: 0.089645, acc.: 96.29%] [G loss: 7.331234]\n",
            "15216 [D loss: 0.065995, acc.: 98.44%] [G loss: 6.253261]\n",
            "15217 [D loss: 0.104849, acc.: 98.24%] [G loss: 5.598225]\n",
            "15218 [D loss: 0.020564, acc.: 99.41%] [G loss: 5.617158]\n",
            "15219 [D loss: 0.077380, acc.: 98.83%] [G loss: 6.079319]\n",
            "15220 [D loss: 0.021708, acc.: 99.41%] [G loss: 6.039302]\n",
            "15221 [D loss: 0.048923, acc.: 99.41%] [G loss: 5.422853]\n",
            "15222 [D loss: 0.040291, acc.: 99.02%] [G loss: 6.331159]\n",
            "15223 [D loss: 0.077325, acc.: 98.63%] [G loss: 5.399267]\n",
            "15224 [D loss: 0.028248, acc.: 99.41%] [G loss: 6.324048]\n",
            "15225 [D loss: 0.050368, acc.: 98.63%] [G loss: 5.760045]\n",
            "15226 [D loss: 0.016443, acc.: 100.00%] [G loss: 5.522030]\n",
            "15227 [D loss: 0.029963, acc.: 99.61%] [G loss: 5.794889]\n",
            "15228 [D loss: 0.013692, acc.: 99.61%] [G loss: 6.295526]\n",
            "15229 [D loss: 0.121124, acc.: 97.46%] [G loss: 6.182322]\n",
            "15230 [D loss: 0.080767, acc.: 98.63%] [G loss: 5.687847]\n",
            "15231 [D loss: 0.029766, acc.: 99.61%] [G loss: 5.322846]\n",
            "15232 [D loss: 0.043257, acc.: 99.22%] [G loss: 4.951805]\n",
            "15233 [D loss: 0.034603, acc.: 99.41%] [G loss: 5.585494]\n",
            "15234 [D loss: 0.088058, acc.: 98.63%] [G loss: 5.219544]\n",
            "15235 [D loss: 0.048289, acc.: 98.83%] [G loss: 5.891261]\n",
            "15236 [D loss: 0.026750, acc.: 99.80%] [G loss: 5.855900]\n",
            "15237 [D loss: 0.066009, acc.: 98.83%] [G loss: 5.479563]\n",
            "15238 [D loss: 0.064746, acc.: 98.24%] [G loss: 6.649844]\n",
            "15239 [D loss: 0.316230, acc.: 91.02%] [G loss: 14.098032]\n",
            "15240 [D loss: 0.679604, acc.: 91.60%] [G loss: 5.138584]\n",
            "15241 [D loss: 0.120661, acc.: 96.29%] [G loss: 6.945758]\n",
            "15242 [D loss: 0.115222, acc.: 98.05%] [G loss: 5.186044]\n",
            "15243 [D loss: 0.069805, acc.: 98.83%] [G loss: 6.880669]\n",
            "15244 [D loss: 0.123467, acc.: 97.85%] [G loss: 5.788221]\n",
            "15245 [D loss: 0.155446, acc.: 95.51%] [G loss: 8.931605]\n",
            "15246 [D loss: 0.225509, acc.: 97.46%] [G loss: 6.953015]\n",
            "15247 [D loss: 0.291245, acc.: 95.90%] [G loss: 5.872174]\n",
            "15248 [D loss: 0.045832, acc.: 99.02%] [G loss: 5.791603]\n",
            "15249 [D loss: 0.115238, acc.: 98.24%] [G loss: 5.689241]\n",
            "15250 [D loss: 0.012351, acc.: 99.80%] [G loss: 6.735476]\n",
            "15251 [D loss: 0.176007, acc.: 95.12%] [G loss: 6.201956]\n",
            "15252 [D loss: 0.040386, acc.: 99.22%] [G loss: 6.160199]\n",
            "15253 [D loss: 0.104077, acc.: 97.46%] [G loss: 4.582924]\n",
            "15254 [D loss: 0.078054, acc.: 98.24%] [G loss: 6.079751]\n",
            "15255 [D loss: 0.061750, acc.: 98.63%] [G loss: 5.806685]\n",
            "15256 [D loss: 0.071555, acc.: 98.83%] [G loss: 5.079875]\n",
            "15257 [D loss: 0.037577, acc.: 99.02%] [G loss: 5.042633]\n",
            "15258 [D loss: 0.098678, acc.: 98.24%] [G loss: 8.213103]\n",
            "15259 [D loss: 0.101260, acc.: 97.46%] [G loss: 5.131051]\n",
            "15260 [D loss: 0.048969, acc.: 98.44%] [G loss: 7.807568]\n",
            "15261 [D loss: 0.056637, acc.: 98.63%] [G loss: 4.938893]\n",
            "15262 [D loss: 0.053892, acc.: 98.83%] [G loss: 5.160936]\n",
            "15263 [D loss: 0.025798, acc.: 99.41%] [G loss: 5.144435]\n",
            "15264 [D loss: 0.050844, acc.: 99.22%] [G loss: 5.287884]\n",
            "15265 [D loss: 0.067143, acc.: 98.63%] [G loss: 5.496119]\n",
            "15266 [D loss: 0.063147, acc.: 99.02%] [G loss: 5.584552]\n",
            "15267 [D loss: 0.045579, acc.: 99.02%] [G loss: 4.633515]\n",
            "15268 [D loss: 0.034851, acc.: 99.22%] [G loss: 5.040720]\n",
            "15269 [D loss: 0.042274, acc.: 99.22%] [G loss: 5.176836]\n",
            "15270 [D loss: 0.064491, acc.: 98.63%] [G loss: 4.907218]\n",
            "15271 [D loss: 0.042744, acc.: 99.22%] [G loss: 5.217913]\n",
            "15272 [D loss: 0.030824, acc.: 99.61%] [G loss: 4.873884]\n",
            "15273 [D loss: 0.086273, acc.: 98.05%] [G loss: 6.574071]\n",
            "15274 [D loss: 0.144183, acc.: 97.27%] [G loss: 4.481205]\n",
            "15275 [D loss: 0.033837, acc.: 99.02%] [G loss: 5.243634]\n",
            "15276 [D loss: 0.071289, acc.: 97.85%] [G loss: 4.776241]\n",
            "15277 [D loss: 0.096514, acc.: 98.05%] [G loss: 8.421266]\n",
            "15278 [D loss: 0.149322, acc.: 97.66%] [G loss: 6.360323]\n",
            "15279 [D loss: 0.084250, acc.: 97.85%] [G loss: 4.215924]\n",
            "15280 [D loss: 0.146644, acc.: 96.09%] [G loss: 9.983618]\n",
            "15281 [D loss: 0.205321, acc.: 97.07%] [G loss: 5.082976]\n",
            "15282 [D loss: 0.054612, acc.: 99.22%] [G loss: 4.336000]\n",
            "15283 [D loss: 0.031997, acc.: 98.83%] [G loss: 5.260982]\n",
            "15284 [D loss: 0.111025, acc.: 95.90%] [G loss: 6.316714]\n",
            "15285 [D loss: 0.084619, acc.: 98.24%] [G loss: 5.587109]\n",
            "15286 [D loss: 0.043891, acc.: 99.22%] [G loss: 6.032853]\n",
            "15287 [D loss: 0.072807, acc.: 98.44%] [G loss: 4.828900]\n",
            "15288 [D loss: 0.053724, acc.: 99.02%] [G loss: 5.303371]\n",
            "15289 [D loss: 0.057525, acc.: 98.83%] [G loss: 5.184239]\n",
            "15290 [D loss: 0.102144, acc.: 98.05%] [G loss: 6.388312]\n",
            "15291 [D loss: 0.080046, acc.: 98.24%] [G loss: 4.652100]\n",
            "15292 [D loss: 0.102912, acc.: 97.07%] [G loss: 5.816005]\n",
            "15293 [D loss: 0.063429, acc.: 98.24%] [G loss: 5.447885]\n",
            "15294 [D loss: 0.036350, acc.: 99.41%] [G loss: 5.380208]\n",
            "15295 [D loss: 0.046301, acc.: 99.22%] [G loss: 5.260674]\n",
            "15296 [D loss: 0.092080, acc.: 98.24%] [G loss: 5.841145]\n",
            "15297 [D loss: 0.067098, acc.: 97.85%] [G loss: 4.582250]\n",
            "15298 [D loss: 0.108108, acc.: 97.85%] [G loss: 8.318901]\n",
            "15299 [D loss: 0.129380, acc.: 97.85%] [G loss: 6.102563]\n",
            "15300 [D loss: 0.083964, acc.: 98.24%] [G loss: 4.039582]\n",
            "15301 [D loss: 0.038318, acc.: 99.02%] [G loss: 5.407354]\n",
            "15302 [D loss: 0.114252, acc.: 97.27%] [G loss: 6.016273]\n",
            "15303 [D loss: 0.051404, acc.: 99.22%] [G loss: 5.023374]\n",
            "15304 [D loss: 0.106421, acc.: 97.85%] [G loss: 6.464149]\n",
            "15305 [D loss: 0.017055, acc.: 99.61%] [G loss: 6.685180]\n",
            "15306 [D loss: 0.125711, acc.: 97.85%] [G loss: 4.397869]\n",
            "15307 [D loss: 0.091062, acc.: 97.85%] [G loss: 6.684966]\n",
            "15308 [D loss: 0.101516, acc.: 97.66%] [G loss: 4.917109]\n",
            "15309 [D loss: 0.097400, acc.: 97.07%] [G loss: 8.079065]\n",
            "15310 [D loss: 0.048786, acc.: 98.63%] [G loss: 7.515853]\n",
            "15311 [D loss: 0.115843, acc.: 98.24%] [G loss: 3.861255]\n",
            "15312 [D loss: 0.074286, acc.: 98.24%] [G loss: 8.162268]\n",
            "15313 [D loss: 0.068650, acc.: 98.63%] [G loss: 5.734982]\n",
            "15314 [D loss: 0.032723, acc.: 99.41%] [G loss: 4.673334]\n",
            "15315 [D loss: 0.039219, acc.: 99.02%] [G loss: 4.790073]\n",
            "15316 [D loss: 0.058288, acc.: 98.63%] [G loss: 6.260473]\n",
            "15317 [D loss: 0.045000, acc.: 99.02%] [G loss: 5.532640]\n",
            "15318 [D loss: 0.053961, acc.: 98.63%] [G loss: 5.268648]\n",
            "15319 [D loss: 0.077185, acc.: 98.05%] [G loss: 5.266854]\n",
            "15320 [D loss: 0.098510, acc.: 97.85%] [G loss: 5.735565]\n",
            "15321 [D loss: 0.072146, acc.: 98.05%] [G loss: 4.794385]\n",
            "15322 [D loss: 0.057853, acc.: 98.63%] [G loss: 4.884724]\n",
            "15323 [D loss: 0.081929, acc.: 97.66%] [G loss: 6.609201]\n",
            "15324 [D loss: 0.096704, acc.: 97.85%] [G loss: 4.278496]\n",
            "15325 [D loss: 0.075685, acc.: 98.44%] [G loss: 8.638113]\n",
            "15326 [D loss: 0.289171, acc.: 95.90%] [G loss: 4.387350]\n",
            "15327 [D loss: 0.061026, acc.: 97.46%] [G loss: 6.276210]\n",
            "15328 [D loss: 0.120597, acc.: 96.68%] [G loss: 4.924527]\n",
            "15329 [D loss: 0.023671, acc.: 99.80%] [G loss: 5.544320]\n",
            "15330 [D loss: 0.076805, acc.: 96.88%] [G loss: 4.811599]\n",
            "15331 [D loss: 0.052046, acc.: 99.22%] [G loss: 5.707469]\n",
            "15332 [D loss: 0.041609, acc.: 98.83%] [G loss: 5.129165]\n",
            "15333 [D loss: 0.047818, acc.: 99.22%] [G loss: 4.867842]\n",
            "15334 [D loss: 0.047664, acc.: 99.22%] [G loss: 4.880343]\n",
            "15335 [D loss: 0.070942, acc.: 98.83%] [G loss: 5.113894]\n",
            "15336 [D loss: 0.036784, acc.: 99.41%] [G loss: 4.936358]\n",
            "15337 [D loss: 0.046794, acc.: 99.22%] [G loss: 4.758816]\n",
            "15338 [D loss: 0.048304, acc.: 99.22%] [G loss: 5.119721]\n",
            "15339 [D loss: 0.062939, acc.: 98.83%] [G loss: 4.923368]\n",
            "15340 [D loss: 0.168525, acc.: 95.70%] [G loss: 7.650899]\n",
            "15341 [D loss: 0.151184, acc.: 97.27%] [G loss: 5.581719]\n",
            "15342 [D loss: 0.113098, acc.: 96.29%] [G loss: 5.311112]\n",
            "15343 [D loss: 0.075485, acc.: 98.63%] [G loss: 5.529222]\n",
            "15344 [D loss: 0.025098, acc.: 99.61%] [G loss: 5.665696]\n",
            "15345 [D loss: 0.107799, acc.: 97.85%] [G loss: 6.878137]\n",
            "15346 [D loss: 0.120533, acc.: 98.05%] [G loss: 5.026044]\n",
            "15347 [D loss: 0.050507, acc.: 99.22%] [G loss: 5.537438]\n",
            "15348 [D loss: 0.052033, acc.: 97.85%] [G loss: 5.182005]\n",
            "15349 [D loss: 0.049705, acc.: 98.83%] [G loss: 5.214859]\n",
            "15350 [D loss: 0.053193, acc.: 99.22%] [G loss: 5.780700]\n",
            "15351 [D loss: 0.016163, acc.: 99.41%] [G loss: 6.181325]\n",
            "15352 [D loss: 0.073496, acc.: 98.44%] [G loss: 4.841598]\n",
            "15353 [D loss: 0.036809, acc.: 99.41%] [G loss: 5.914711]\n",
            "15354 [D loss: 0.088803, acc.: 98.05%] [G loss: 4.855732]\n",
            "15355 [D loss: 0.034886, acc.: 99.41%] [G loss: 5.905916]\n",
            "15356 [D loss: 0.093218, acc.: 97.07%] [G loss: 6.152861]\n",
            "15357 [D loss: 0.074366, acc.: 98.05%] [G loss: 8.912012]\n",
            "15358 [D loss: 0.067564, acc.: 98.05%] [G loss: 6.153826]\n",
            "15359 [D loss: 0.017980, acc.: 99.41%] [G loss: 6.006105]\n",
            "15360 [D loss: 0.065086, acc.: 98.83%] [G loss: 5.365241]\n",
            "15361 [D loss: 0.040449, acc.: 98.83%] [G loss: 7.265334]\n",
            "15362 [D loss: 0.093497, acc.: 98.24%] [G loss: 5.519681]\n",
            "15363 [D loss: 0.023488, acc.: 99.22%] [G loss: 5.140982]\n",
            "15364 [D loss: 0.058486, acc.: 99.02%] [G loss: 6.420404]\n",
            "15365 [D loss: 0.079267, acc.: 97.85%] [G loss: 4.426553]\n",
            "15366 [D loss: 0.034042, acc.: 99.02%] [G loss: 6.003772]\n",
            "15367 [D loss: 0.116442, acc.: 96.88%] [G loss: 7.044388]\n",
            "15368 [D loss: 0.036777, acc.: 99.61%] [G loss: 5.373941]\n",
            "15369 [D loss: 0.077633, acc.: 98.63%] [G loss: 5.991734]\n",
            "15370 [D loss: 0.059900, acc.: 98.24%] [G loss: 5.276595]\n",
            "15371 [D loss: 0.025639, acc.: 99.80%] [G loss: 5.732796]\n",
            "15372 [D loss: 0.054592, acc.: 99.02%] [G loss: 4.838767]\n",
            "15373 [D loss: 0.071942, acc.: 98.44%] [G loss: 4.882812]\n",
            "15374 [D loss: 0.039537, acc.: 99.22%] [G loss: 5.667129]\n",
            "15375 [D loss: 0.066637, acc.: 99.02%] [G loss: 6.014423]\n",
            "15376 [D loss: 0.084705, acc.: 98.24%] [G loss: 6.671289]\n",
            "15377 [D loss: 0.110688, acc.: 95.70%] [G loss: 5.711388]\n",
            "15378 [D loss: 0.033416, acc.: 99.02%] [G loss: 5.035062]\n",
            "15379 [D loss: 0.077082, acc.: 99.22%] [G loss: 7.784512]\n",
            "15380 [D loss: 0.219500, acc.: 95.51%] [G loss: 6.941806]\n",
            "15381 [D loss: 0.081632, acc.: 97.66%] [G loss: 5.935353]\n",
            "15382 [D loss: 0.029308, acc.: 99.02%] [G loss: 5.698367]\n",
            "15383 [D loss: 0.100582, acc.: 96.68%] [G loss: 5.196897]\n",
            "15384 [D loss: 0.067908, acc.: 98.24%] [G loss: 6.485083]\n",
            "15385 [D loss: 0.098335, acc.: 98.83%] [G loss: 7.455783]\n",
            "15386 [D loss: 0.159930, acc.: 96.48%] [G loss: 5.858147]\n",
            "15387 [D loss: 0.131056, acc.: 95.90%] [G loss: 8.019148]\n",
            "15388 [D loss: 0.056382, acc.: 98.05%] [G loss: 6.170242]\n",
            "15389 [D loss: 0.069304, acc.: 98.05%] [G loss: 6.225657]\n",
            "15390 [D loss: 0.031466, acc.: 99.41%] [G loss: 5.489412]\n",
            "15391 [D loss: 0.086772, acc.: 98.44%] [G loss: 5.756353]\n",
            "15392 [D loss: 0.096805, acc.: 97.27%] [G loss: 5.162045]\n",
            "15393 [D loss: 0.021155, acc.: 99.61%] [G loss: 5.735656]\n",
            "15394 [D loss: 0.069161, acc.: 98.83%] [G loss: 4.700544]\n",
            "15395 [D loss: 0.042027, acc.: 99.41%] [G loss: 5.490896]\n",
            "15396 [D loss: 0.069197, acc.: 98.05%] [G loss: 4.725815]\n",
            "15397 [D loss: 0.178776, acc.: 95.51%] [G loss: 9.171284]\n",
            "15398 [D loss: 0.185992, acc.: 97.27%] [G loss: 7.587182]\n",
            "15399 [D loss: 0.049761, acc.: 99.41%] [G loss: 6.499007]\n",
            "15400 [D loss: 0.033432, acc.: 99.41%] [G loss: 6.303741]\n",
            "15401 [D loss: 0.076681, acc.: 98.83%] [G loss: 5.174242]\n",
            "15402 [D loss: 0.061522, acc.: 99.02%] [G loss: 5.138614]\n",
            "15403 [D loss: 0.083480, acc.: 96.88%] [G loss: 5.920306]\n",
            "15404 [D loss: 0.028656, acc.: 99.41%] [G loss: 6.930936]\n",
            "15405 [D loss: 0.048210, acc.: 98.83%] [G loss: 6.579405]\n",
            "15406 [D loss: 0.034955, acc.: 98.63%] [G loss: 6.540727]\n",
            "15407 [D loss: 0.059058, acc.: 99.22%] [G loss: 6.102091]\n",
            "15408 [D loss: 0.012198, acc.: 99.80%] [G loss: 6.245579]\n",
            "15409 [D loss: 0.088207, acc.: 97.85%] [G loss: 5.402588]\n",
            "15410 [D loss: 0.048734, acc.: 99.22%] [G loss: 5.524430]\n",
            "15411 [D loss: 0.068146, acc.: 98.24%] [G loss: 5.553880]\n",
            "15412 [D loss: 0.053452, acc.: 99.02%] [G loss: 4.770029]\n",
            "15413 [D loss: 0.068171, acc.: 96.68%] [G loss: 5.677417]\n",
            "15414 [D loss: 0.117119, acc.: 97.46%] [G loss: 5.111936]\n",
            "15415 [D loss: 0.032277, acc.: 99.41%] [G loss: 6.492100]\n",
            "15416 [D loss: 0.065919, acc.: 97.66%] [G loss: 6.503469]\n",
            "15417 [D loss: 0.044237, acc.: 99.22%] [G loss: 7.932268]\n",
            "15418 [D loss: 0.168968, acc.: 94.73%] [G loss: 7.652570]\n",
            "15419 [D loss: 0.094712, acc.: 97.07%] [G loss: 5.502772]\n",
            "15420 [D loss: 0.074292, acc.: 97.66%] [G loss: 6.204341]\n",
            "15421 [D loss: 0.045063, acc.: 98.05%] [G loss: 5.774120]\n",
            "15422 [D loss: 0.065253, acc.: 98.83%] [G loss: 5.005848]\n",
            "15423 [D loss: 0.021753, acc.: 99.61%] [G loss: 5.595355]\n",
            "15424 [D loss: 0.094356, acc.: 98.05%] [G loss: 5.204687]\n",
            "15425 [D loss: 0.058062, acc.: 98.83%] [G loss: 5.115346]\n",
            "15426 [D loss: 0.043026, acc.: 99.22%] [G loss: 5.319652]\n",
            "15427 [D loss: 0.080041, acc.: 98.63%] [G loss: 5.555739]\n",
            "15428 [D loss: 0.040751, acc.: 98.24%] [G loss: 5.274668]\n",
            "15429 [D loss: 0.127828, acc.: 96.68%] [G loss: 5.882503]\n",
            "15430 [D loss: 0.057521, acc.: 97.66%] [G loss: 5.163551]\n",
            "15431 [D loss: 0.172918, acc.: 92.38%] [G loss: 11.279589]\n",
            "15432 [D loss: 0.422515, acc.: 94.92%] [G loss: 8.944758]\n",
            "15433 [D loss: 0.149114, acc.: 96.29%] [G loss: 6.278969]\n",
            "15434 [D loss: 0.097405, acc.: 97.46%] [G loss: 7.580777]\n",
            "15435 [D loss: 0.170563, acc.: 96.68%] [G loss: 6.066208]\n",
            "15436 [D loss: 0.054763, acc.: 99.02%] [G loss: 7.349716]\n",
            "15437 [D loss: 0.143520, acc.: 97.27%] [G loss: 7.200624]\n",
            "15438 [D loss: 0.060938, acc.: 98.44%] [G loss: 6.920931]\n",
            "15439 [D loss: 0.109359, acc.: 97.46%] [G loss: 5.888896]\n",
            "15440 [D loss: 0.085445, acc.: 98.44%] [G loss: 5.950804]\n",
            "15441 [D loss: 0.122480, acc.: 97.85%] [G loss: 6.202005]\n",
            "15442 [D loss: 0.062170, acc.: 98.24%] [G loss: 5.340889]\n",
            "15443 [D loss: 0.043148, acc.: 99.02%] [G loss: 6.066604]\n",
            "15444 [D loss: 0.088896, acc.: 98.05%] [G loss: 7.625305]\n",
            "15445 [D loss: 0.085045, acc.: 98.24%] [G loss: 6.120947]\n",
            "15446 [D loss: 0.082040, acc.: 98.24%] [G loss: 6.571249]\n",
            "15447 [D loss: 0.037107, acc.: 99.22%] [G loss: 5.632734]\n",
            "15448 [D loss: 0.037736, acc.: 99.22%] [G loss: 5.696407]\n",
            "15449 [D loss: 0.019536, acc.: 99.61%] [G loss: 6.746081]\n",
            "15450 [D loss: 0.129329, acc.: 96.68%] [G loss: 6.452208]\n",
            "15451 [D loss: 0.066555, acc.: 98.24%] [G loss: 5.501091]\n",
            "15452 [D loss: 0.081847, acc.: 98.63%] [G loss: 6.599400]\n",
            "15453 [D loss: 0.087315, acc.: 98.05%] [G loss: 4.691000]\n",
            "15454 [D loss: 0.112176, acc.: 96.29%] [G loss: 8.504356]\n",
            "15455 [D loss: 0.148831, acc.: 97.66%] [G loss: 5.552405]\n",
            "15456 [D loss: 0.116509, acc.: 96.88%] [G loss: 6.234362]\n",
            "15457 [D loss: 0.020606, acc.: 99.80%] [G loss: 6.344125]\n",
            "15458 [D loss: 0.061660, acc.: 98.63%] [G loss: 6.470721]\n",
            "15459 [D loss: 0.041131, acc.: 99.02%] [G loss: 5.684398]\n",
            "15460 [D loss: 0.032710, acc.: 99.02%] [G loss: 5.266760]\n",
            "15461 [D loss: 0.051623, acc.: 99.02%] [G loss: 6.153337]\n",
            "15462 [D loss: 0.076326, acc.: 98.05%] [G loss: 6.882250]\n",
            "15463 [D loss: 0.123179, acc.: 94.34%] [G loss: 10.252300]\n",
            "15464 [D loss: 0.100332, acc.: 98.05%] [G loss: 8.828541]\n",
            "15465 [D loss: 0.146437, acc.: 98.24%] [G loss: 6.206389]\n",
            "15466 [D loss: 0.120230, acc.: 96.09%] [G loss: 6.116987]\n",
            "15467 [D loss: 0.035372, acc.: 98.83%] [G loss: 6.380590]\n",
            "15468 [D loss: 0.087161, acc.: 97.85%] [G loss: 6.267720]\n",
            "15469 [D loss: 0.016379, acc.: 99.61%] [G loss: 5.616101]\n",
            "15470 [D loss: 0.048359, acc.: 97.66%] [G loss: 6.359533]\n",
            "15471 [D loss: 0.038252, acc.: 99.41%] [G loss: 6.012092]\n",
            "15472 [D loss: 0.028556, acc.: 99.02%] [G loss: 5.711037]\n",
            "15473 [D loss: 0.156580, acc.: 96.09%] [G loss: 10.173882]\n",
            "15474 [D loss: 0.103029, acc.: 98.63%] [G loss: 8.329188]\n",
            "15475 [D loss: 0.245095, acc.: 96.68%] [G loss: 6.694077]\n",
            "15476 [D loss: 0.071565, acc.: 97.46%] [G loss: 5.652454]\n",
            "15477 [D loss: 0.049534, acc.: 97.46%] [G loss: 5.647738]\n",
            "15478 [D loss: 0.045260, acc.: 99.02%] [G loss: 6.278392]\n",
            "15479 [D loss: 0.167297, acc.: 95.51%] [G loss: 6.975936]\n",
            "15480 [D loss: 0.132425, acc.: 97.07%] [G loss: 4.429319]\n",
            "15481 [D loss: 0.048393, acc.: 99.41%] [G loss: 6.252808]\n",
            "15482 [D loss: 0.021684, acc.: 98.83%] [G loss: 7.526082]\n",
            "15483 [D loss: 0.120807, acc.: 97.07%] [G loss: 5.077888]\n",
            "15484 [D loss: 0.053489, acc.: 99.02%] [G loss: 5.035847]\n",
            "15485 [D loss: 0.034539, acc.: 98.83%] [G loss: 4.978333]\n",
            "15486 [D loss: 0.050365, acc.: 99.22%] [G loss: 5.022456]\n",
            "15487 [D loss: 0.064535, acc.: 97.85%] [G loss: 5.601832]\n",
            "15488 [D loss: 0.105084, acc.: 96.09%] [G loss: 5.309420]\n",
            "15489 [D loss: 0.149311, acc.: 96.09%] [G loss: 9.007640]\n",
            "15490 [D loss: 0.106404, acc.: 98.24%] [G loss: 8.192152]\n",
            "15491 [D loss: 0.164343, acc.: 98.05%] [G loss: 5.652467]\n",
            "15492 [D loss: 0.135927, acc.: 96.09%] [G loss: 6.251125]\n",
            "15493 [D loss: 0.110800, acc.: 97.46%] [G loss: 4.580109]\n",
            "15494 [D loss: 0.079952, acc.: 97.85%] [G loss: 6.843386]\n",
            "15495 [D loss: 0.062752, acc.: 98.24%] [G loss: 5.205887]\n",
            "15496 [D loss: 0.051711, acc.: 99.41%] [G loss: 5.301116]\n",
            "15497 [D loss: 0.032568, acc.: 98.05%] [G loss: 5.852248]\n",
            "15498 [D loss: 0.077263, acc.: 97.85%] [G loss: 6.229645]\n",
            "15499 [D loss: 0.065794, acc.: 98.44%] [G loss: 5.253566]\n",
            "15500 [D loss: 0.063974, acc.: 99.02%] [G loss: 5.131388]\n",
            "15501 [D loss: 0.060508, acc.: 98.83%] [G loss: 6.007452]\n",
            "15502 [D loss: 0.129202, acc.: 94.34%] [G loss: 6.393814]\n",
            "15503 [D loss: 0.101499, acc.: 98.44%] [G loss: 5.443582]\n",
            "15504 [D loss: 0.127621, acc.: 97.27%] [G loss: 6.110394]\n",
            "15505 [D loss: 0.050821, acc.: 99.22%] [G loss: 5.640903]\n",
            "15506 [D loss: 0.147946, acc.: 95.51%] [G loss: 7.432914]\n",
            "15507 [D loss: 0.241005, acc.: 93.95%] [G loss: 5.996791]\n",
            "15508 [D loss: 0.113135, acc.: 96.48%] [G loss: 4.496169]\n",
            "15509 [D loss: 0.037665, acc.: 98.83%] [G loss: 5.060236]\n",
            "15510 [D loss: 0.071743, acc.: 97.46%] [G loss: 7.941500]\n",
            "15511 [D loss: 0.052794, acc.: 98.63%] [G loss: 4.962106]\n",
            "15512 [D loss: 0.050266, acc.: 99.41%] [G loss: 5.997978]\n",
            "15513 [D loss: 0.049624, acc.: 99.02%] [G loss: 5.664310]\n",
            "15514 [D loss: 0.068797, acc.: 98.44%] [G loss: 4.916087]\n",
            "15515 [D loss: 0.035977, acc.: 99.41%] [G loss: 5.143254]\n",
            "15516 [D loss: 0.028106, acc.: 99.61%] [G loss: 5.698408]\n",
            "15517 [D loss: 0.040233, acc.: 99.41%] [G loss: 4.985575]\n",
            "15518 [D loss: 0.039993, acc.: 99.41%] [G loss: 6.309843]\n",
            "15519 [D loss: 0.033518, acc.: 99.22%] [G loss: 5.701476]\n",
            "15520 [D loss: 0.022519, acc.: 99.61%] [G loss: 5.513906]\n",
            "15521 [D loss: 0.089101, acc.: 98.05%] [G loss: 5.633500]\n",
            "15522 [D loss: 0.200599, acc.: 94.14%] [G loss: 8.952522]\n",
            "15523 [D loss: 0.130246, acc.: 98.63%] [G loss: 10.777056]\n",
            "15524 [D loss: 0.159694, acc.: 97.66%] [G loss: 5.742899]\n",
            "15525 [D loss: 0.206127, acc.: 95.51%] [G loss: 3.960836]\n",
            "15526 [D loss: 0.028172, acc.: 99.41%] [G loss: 5.288171]\n",
            "15527 [D loss: 0.089429, acc.: 96.88%] [G loss: 4.643095]\n",
            "15528 [D loss: 0.086850, acc.: 98.05%] [G loss: 4.616719]\n",
            "15529 [D loss: 0.037860, acc.: 98.24%] [G loss: 5.299575]\n",
            "15530 [D loss: 0.140224, acc.: 95.31%] [G loss: 5.676611]\n",
            "15531 [D loss: 0.079780, acc.: 97.27%] [G loss: 4.699709]\n",
            "15532 [D loss: 0.056533, acc.: 98.83%] [G loss: 5.294930]\n",
            "15533 [D loss: 0.103590, acc.: 96.88%] [G loss: 7.715970]\n",
            "15534 [D loss: 0.112218, acc.: 97.66%] [G loss: 5.628155]\n",
            "15535 [D loss: 0.157930, acc.: 96.68%] [G loss: 6.502267]\n",
            "15536 [D loss: 0.068591, acc.: 98.24%] [G loss: 5.624006]\n",
            "15537 [D loss: 0.089846, acc.: 97.66%] [G loss: 5.876520]\n",
            "15538 [D loss: 0.070017, acc.: 98.24%] [G loss: 5.557466]\n",
            "15539 [D loss: 0.057176, acc.: 98.83%] [G loss: 5.655755]\n",
            "15540 [D loss: 0.043858, acc.: 98.63%] [G loss: 5.940943]\n",
            "15541 [D loss: 0.083657, acc.: 97.07%] [G loss: 5.652647]\n",
            "15542 [D loss: 0.043817, acc.: 98.05%] [G loss: 6.696187]\n",
            "15543 [D loss: 0.038286, acc.: 99.02%] [G loss: 5.498541]\n",
            "15544 [D loss: 0.032245, acc.: 98.83%] [G loss: 5.564881]\n",
            "15545 [D loss: 0.085241, acc.: 97.27%] [G loss: 7.381329]\n",
            "15546 [D loss: 0.061715, acc.: 98.44%] [G loss: 7.002750]\n",
            "15547 [D loss: 0.050008, acc.: 97.85%] [G loss: 5.232613]\n",
            "15548 [D loss: 0.091197, acc.: 97.07%] [G loss: 7.653957]\n",
            "15549 [D loss: 0.075877, acc.: 98.83%] [G loss: 6.395627]\n",
            "15550 [D loss: 0.173890, acc.: 95.70%] [G loss: 8.150488]\n",
            "15551 [D loss: 0.053600, acc.: 98.24%] [G loss: 8.317842]\n",
            "15552 [D loss: 0.209541, acc.: 93.55%] [G loss: 9.476990]\n",
            "15553 [D loss: 0.146417, acc.: 97.66%] [G loss: 7.123306]\n",
            "15554 [D loss: 0.096374, acc.: 96.68%] [G loss: 5.412011]\n",
            "15555 [D loss: 0.052167, acc.: 100.00%] [G loss: 6.927713]\n",
            "15556 [D loss: 0.115728, acc.: 97.66%] [G loss: 5.348455]\n",
            "15557 [D loss: 0.119462, acc.: 97.27%] [G loss: 8.056477]\n",
            "15558 [D loss: 0.125462, acc.: 97.85%] [G loss: 6.798812]\n",
            "15559 [D loss: 0.161443, acc.: 94.73%] [G loss: 6.090391]\n",
            "15560 [D loss: 0.045403, acc.: 98.83%] [G loss: 6.146490]\n",
            "15561 [D loss: 0.107228, acc.: 96.68%] [G loss: 6.069345]\n",
            "15562 [D loss: 0.112545, acc.: 96.88%] [G loss: 6.394074]\n",
            "15563 [D loss: 0.127967, acc.: 97.66%] [G loss: 5.765663]\n",
            "15564 [D loss: 0.019303, acc.: 99.02%] [G loss: 5.226600]\n",
            "15565 [D loss: 0.076675, acc.: 98.44%] [G loss: 6.079066]\n",
            "15566 [D loss: 0.049386, acc.: 98.44%] [G loss: 4.962512]\n",
            "15567 [D loss: 0.067357, acc.: 98.44%] [G loss: 5.397991]\n",
            "15568 [D loss: 0.052761, acc.: 98.24%] [G loss: 5.354717]\n",
            "15569 [D loss: 0.121352, acc.: 96.48%] [G loss: 8.106718]\n",
            "15570 [D loss: 0.173957, acc.: 96.88%] [G loss: 5.175640]\n",
            "15571 [D loss: 0.099700, acc.: 96.29%] [G loss: 5.628829]\n",
            "15572 [D loss: 0.182336, acc.: 94.53%] [G loss: 6.984579]\n",
            "15573 [D loss: 0.125285, acc.: 97.85%] [G loss: 6.336962]\n",
            "15574 [D loss: 0.132071, acc.: 97.46%] [G loss: 4.565529]\n",
            "15575 [D loss: 0.071523, acc.: 97.46%] [G loss: 5.724410]\n",
            "15576 [D loss: 0.052014, acc.: 98.44%] [G loss: 5.541433]\n",
            "15577 [D loss: 0.098174, acc.: 96.09%] [G loss: 5.884766]\n",
            "15578 [D loss: 0.146717, acc.: 96.68%] [G loss: 5.543364]\n",
            "15579 [D loss: 0.073125, acc.: 97.27%] [G loss: 5.807084]\n",
            "15580 [D loss: 0.072584, acc.: 98.44%] [G loss: 7.777279]\n",
            "15581 [D loss: 0.046948, acc.: 99.02%] [G loss: 7.128357]\n",
            "15582 [D loss: 0.041903, acc.: 98.83%] [G loss: 4.985626]\n",
            "15583 [D loss: 0.054173, acc.: 98.83%] [G loss: 5.855304]\n",
            "15584 [D loss: 0.027047, acc.: 99.41%] [G loss: 5.227263]\n",
            "15585 [D loss: 0.071780, acc.: 98.44%] [G loss: 5.035285]\n",
            "15586 [D loss: 0.053327, acc.: 97.85%] [G loss: 5.159191]\n",
            "15587 [D loss: 0.087518, acc.: 97.46%] [G loss: 8.653257]\n",
            "15588 [D loss: 0.221849, acc.: 96.09%] [G loss: 7.488059]\n",
            "15589 [D loss: 0.179306, acc.: 95.90%] [G loss: 5.058251]\n",
            "15590 [D loss: 0.030126, acc.: 99.02%] [G loss: 5.767438]\n",
            "15591 [D loss: 0.120882, acc.: 96.88%] [G loss: 5.249639]\n",
            "15592 [D loss: 0.067493, acc.: 97.66%] [G loss: 5.283113]\n",
            "15593 [D loss: 0.109769, acc.: 96.68%] [G loss: 5.804225]\n",
            "15594 [D loss: 0.155195, acc.: 95.51%] [G loss: 6.081242]\n",
            "15595 [D loss: 0.103548, acc.: 97.07%] [G loss: 5.344740]\n",
            "15596 [D loss: 0.065798, acc.: 97.46%] [G loss: 5.326939]\n",
            "15597 [D loss: 0.050623, acc.: 98.63%] [G loss: 4.975242]\n",
            "15598 [D loss: 0.042811, acc.: 99.02%] [G loss: 5.456096]\n",
            "15599 [D loss: 0.109176, acc.: 95.90%] [G loss: 6.678658]\n",
            "15600 [D loss: 0.057660, acc.: 98.63%] [G loss: 5.881770]\n",
            "15601 [D loss: 0.089502, acc.: 97.27%] [G loss: 5.616151]\n",
            "15602 [D loss: 0.041703, acc.: 98.63%] [G loss: 6.509672]\n",
            "15603 [D loss: 0.078481, acc.: 97.66%] [G loss: 5.004188]\n",
            "15604 [D loss: 0.116070, acc.: 96.88%] [G loss: 5.396554]\n",
            "15605 [D loss: 0.045252, acc.: 99.22%] [G loss: 5.455163]\n",
            "15606 [D loss: 0.075012, acc.: 97.85%] [G loss: 6.089804]\n",
            "15607 [D loss: 0.087291, acc.: 98.05%] [G loss: 4.966733]\n",
            "15608 [D loss: 0.049986, acc.: 99.41%] [G loss: 6.207403]\n",
            "15609 [D loss: 0.287856, acc.: 92.77%] [G loss: 8.203613]\n",
            "15610 [D loss: 0.140007, acc.: 97.46%] [G loss: 6.855799]\n",
            "15611 [D loss: 0.119497, acc.: 96.29%] [G loss: 4.757072]\n",
            "15612 [D loss: 0.021591, acc.: 99.61%] [G loss: 5.697101]\n",
            "15613 [D loss: 0.096193, acc.: 96.68%] [G loss: 4.747742]\n",
            "15614 [D loss: 0.047487, acc.: 98.63%] [G loss: 5.223515]\n",
            "15615 [D loss: 0.125787, acc.: 96.48%] [G loss: 6.909359]\n",
            "15616 [D loss: 0.119366, acc.: 97.66%] [G loss: 5.067983]\n",
            "15617 [D loss: 0.070769, acc.: 98.44%] [G loss: 5.869195]\n",
            "15618 [D loss: 0.076758, acc.: 97.46%] [G loss: 4.909391]\n",
            "15619 [D loss: 0.041247, acc.: 99.41%] [G loss: 4.882522]\n",
            "15620 [D loss: 0.117548, acc.: 96.88%] [G loss: 6.203224]\n",
            "15621 [D loss: 0.181289, acc.: 96.48%] [G loss: 8.553617]\n",
            "15622 [D loss: 0.200001, acc.: 96.09%] [G loss: 4.450182]\n",
            "15623 [D loss: 0.085406, acc.: 98.05%] [G loss: 4.962962]\n",
            "15624 [D loss: 0.045365, acc.: 98.83%] [G loss: 5.272876]\n",
            "15625 [D loss: 0.107366, acc.: 98.05%] [G loss: 6.966201]\n",
            "15626 [D loss: 0.103969, acc.: 97.85%] [G loss: 6.979336]\n",
            "15627 [D loss: 0.213109, acc.: 93.95%] [G loss: 5.918791]\n",
            "15628 [D loss: 0.046954, acc.: 98.83%] [G loss: 5.719318]\n",
            "15629 [D loss: 0.144249, acc.: 93.95%] [G loss: 6.456781]\n",
            "15630 [D loss: 0.116073, acc.: 97.27%] [G loss: 4.579948]\n",
            "15631 [D loss: 0.069902, acc.: 98.05%] [G loss: 5.757043]\n",
            "15632 [D loss: 0.189258, acc.: 95.70%] [G loss: 7.282897]\n",
            "15633 [D loss: 0.068769, acc.: 98.24%] [G loss: 5.797303]\n",
            "15634 [D loss: 0.203293, acc.: 94.53%] [G loss: 6.114970]\n",
            "15635 [D loss: 0.099296, acc.: 97.85%] [G loss: 6.207825]\n",
            "15636 [D loss: 0.110964, acc.: 97.85%] [G loss: 4.918007]\n",
            "15637 [D loss: 0.104783, acc.: 96.48%] [G loss: 6.511292]\n",
            "15638 [D loss: 0.109948, acc.: 96.88%] [G loss: 4.437061]\n",
            "15639 [D loss: 0.114754, acc.: 96.68%] [G loss: 6.005961]\n",
            "15640 [D loss: 0.120825, acc.: 97.07%] [G loss: 4.414265]\n",
            "15641 [D loss: 0.062995, acc.: 97.85%] [G loss: 6.823101]\n",
            "15642 [D loss: 0.176287, acc.: 94.73%] [G loss: 6.974468]\n",
            "15643 [D loss: 0.077428, acc.: 98.05%] [G loss: 5.041384]\n",
            "15644 [D loss: 0.076153, acc.: 97.27%] [G loss: 5.016370]\n",
            "15645 [D loss: 0.028833, acc.: 99.22%] [G loss: 5.090441]\n",
            "15646 [D loss: 0.109367, acc.: 96.68%] [G loss: 4.694323]\n",
            "15647 [D loss: 0.069403, acc.: 98.24%] [G loss: 4.623977]\n",
            "15648 [D loss: 0.129870, acc.: 94.73%] [G loss: 6.006751]\n",
            "15649 [D loss: 0.057318, acc.: 98.24%] [G loss: 6.016698]\n",
            "15650 [D loss: 0.137364, acc.: 96.88%] [G loss: 5.895716]\n",
            "15651 [D loss: 0.066360, acc.: 97.85%] [G loss: 4.628110]\n",
            "15652 [D loss: 0.075519, acc.: 97.46%] [G loss: 7.139238]\n",
            "15653 [D loss: 0.091141, acc.: 98.05%] [G loss: 4.741599]\n",
            "15654 [D loss: 0.074010, acc.: 98.05%] [G loss: 5.865799]\n",
            "15655 [D loss: 0.091961, acc.: 98.44%] [G loss: 5.499441]\n",
            "15656 [D loss: 0.034132, acc.: 99.02%] [G loss: 6.154049]\n",
            "15657 [D loss: 0.091887, acc.: 97.27%] [G loss: 4.954230]\n",
            "15658 [D loss: 0.077136, acc.: 98.44%] [G loss: 5.034053]\n",
            "15659 [D loss: 0.082164, acc.: 96.48%] [G loss: 5.686747]\n",
            "15660 [D loss: 0.095390, acc.: 96.68%] [G loss: 5.553185]\n",
            "15661 [D loss: 0.015963, acc.: 99.61%] [G loss: 6.392992]\n",
            "15662 [D loss: 0.092481, acc.: 98.24%] [G loss: 5.250766]\n",
            "15663 [D loss: 0.143258, acc.: 95.51%] [G loss: 5.801996]\n",
            "15664 [D loss: 0.061507, acc.: 98.05%] [G loss: 4.929029]\n",
            "15665 [D loss: 0.076793, acc.: 96.88%] [G loss: 4.683784]\n",
            "15666 [D loss: 0.063938, acc.: 99.02%] [G loss: 4.882878]\n",
            "15667 [D loss: 0.095536, acc.: 95.12%] [G loss: 5.520438]\n",
            "15668 [D loss: 0.138582, acc.: 94.34%] [G loss: 5.321125]\n",
            "15669 [D loss: 0.055098, acc.: 98.05%] [G loss: 4.925044]\n",
            "15670 [D loss: 0.065142, acc.: 96.88%] [G loss: 5.255641]\n",
            "15671 [D loss: 0.054806, acc.: 97.85%] [G loss: 5.378256]\n",
            "15672 [D loss: 0.057165, acc.: 98.83%] [G loss: 4.853926]\n",
            "15673 [D loss: 0.116106, acc.: 96.48%] [G loss: 5.521274]\n",
            "15674 [D loss: 0.059558, acc.: 98.63%] [G loss: 4.675262]\n",
            "15675 [D loss: 0.170771, acc.: 94.73%] [G loss: 6.481688]\n",
            "15676 [D loss: 0.158948, acc.: 97.27%] [G loss: 4.370787]\n",
            "15677 [D loss: 0.052077, acc.: 98.83%] [G loss: 5.196115]\n",
            "15678 [D loss: 0.039104, acc.: 98.63%] [G loss: 5.311291]\n",
            "15679 [D loss: 0.078566, acc.: 98.44%] [G loss: 4.805607]\n",
            "15680 [D loss: 0.069890, acc.: 97.85%] [G loss: 6.317017]\n",
            "15681 [D loss: 0.206744, acc.: 93.36%] [G loss: 7.462585]\n",
            "15682 [D loss: 0.124131, acc.: 96.88%] [G loss: 7.391972]\n",
            "15683 [D loss: 0.217577, acc.: 94.53%] [G loss: 5.990587]\n",
            "15684 [D loss: 0.065563, acc.: 97.66%] [G loss: 6.095705]\n",
            "15685 [D loss: 0.113053, acc.: 97.85%] [G loss: 4.848176]\n",
            "15686 [D loss: 0.126125, acc.: 95.31%] [G loss: 5.870022]\n",
            "15687 [D loss: 0.087760, acc.: 97.85%] [G loss: 4.952469]\n",
            "15688 [D loss: 0.102751, acc.: 96.29%] [G loss: 7.095981]\n",
            "15689 [D loss: 0.067217, acc.: 97.85%] [G loss: 5.230669]\n",
            "15690 [D loss: 0.051491, acc.: 98.24%] [G loss: 6.139625]\n",
            "15691 [D loss: 0.049650, acc.: 99.02%] [G loss: 5.401741]\n",
            "15692 [D loss: 0.164885, acc.: 95.70%] [G loss: 6.831697]\n",
            "15693 [D loss: 0.063831, acc.: 98.83%] [G loss: 7.217036]\n",
            "15694 [D loss: 0.057252, acc.: 98.83%] [G loss: 5.968251]\n",
            "15695 [D loss: 0.081109, acc.: 97.27%] [G loss: 5.431164]\n",
            "15696 [D loss: 0.043328, acc.: 99.41%] [G loss: 5.493833]\n",
            "15697 [D loss: 0.133708, acc.: 94.53%] [G loss: 5.470065]\n",
            "15698 [D loss: 0.041133, acc.: 98.63%] [G loss: 6.027097]\n",
            "15699 [D loss: 0.073819, acc.: 98.44%] [G loss: 5.650547]\n",
            "15700 [D loss: 0.109214, acc.: 95.70%] [G loss: 5.273482]\n",
            "15701 [D loss: 0.069074, acc.: 97.85%] [G loss: 5.081295]\n",
            "15702 [D loss: 0.045713, acc.: 98.44%] [G loss: 4.942931]\n",
            "15703 [D loss: 0.087338, acc.: 97.66%] [G loss: 4.971601]\n",
            "15704 [D loss: 0.037790, acc.: 98.44%] [G loss: 5.114637]\n",
            "15705 [D loss: 0.060599, acc.: 98.83%] [G loss: 4.955245]\n",
            "15706 [D loss: 0.048796, acc.: 99.02%] [G loss: 5.577172]\n",
            "15707 [D loss: 0.077718, acc.: 97.66%] [G loss: 5.187947]\n",
            "15708 [D loss: 0.079027, acc.: 98.05%] [G loss: 5.615009]\n",
            "15709 [D loss: 0.102870, acc.: 95.70%] [G loss: 6.647373]\n",
            "15710 [D loss: 0.097909, acc.: 97.85%] [G loss: 5.608648]\n",
            "15711 [D loss: 0.096428, acc.: 96.68%] [G loss: 6.247537]\n",
            "15712 [D loss: 0.092272, acc.: 97.46%] [G loss: 5.176270]\n",
            "15713 [D loss: 0.078874, acc.: 98.05%] [G loss: 5.656094]\n",
            "15714 [D loss: 0.062669, acc.: 99.02%] [G loss: 5.681712]\n",
            "15715 [D loss: 0.034198, acc.: 98.83%] [G loss: 5.810510]\n",
            "15716 [D loss: 0.052813, acc.: 98.63%] [G loss: 5.998036]\n",
            "15717 [D loss: 0.039275, acc.: 99.02%] [G loss: 5.787836]\n",
            "15718 [D loss: 0.036854, acc.: 99.41%] [G loss: 5.436096]\n",
            "15719 [D loss: 0.066734, acc.: 97.85%] [G loss: 5.261224]\n",
            "15720 [D loss: 0.057959, acc.: 98.05%] [G loss: 5.166677]\n",
            "15721 [D loss: 0.084991, acc.: 98.05%] [G loss: 5.278702]\n",
            "15722 [D loss: 0.073066, acc.: 96.68%] [G loss: 5.896248]\n",
            "15723 [D loss: 0.128627, acc.: 95.31%] [G loss: 5.380699]\n",
            "15724 [D loss: 0.117576, acc.: 95.51%] [G loss: 6.205397]\n",
            "15725 [D loss: 0.049181, acc.: 98.63%] [G loss: 6.059713]\n",
            "15726 [D loss: 0.038318, acc.: 99.02%] [G loss: 5.189756]\n",
            "15727 [D loss: 0.051349, acc.: 98.05%] [G loss: 5.008819]\n",
            "15728 [D loss: 0.124333, acc.: 97.27%] [G loss: 5.551225]\n",
            "15729 [D loss: 0.104385, acc.: 97.27%] [G loss: 5.187243]\n",
            "15730 [D loss: 0.029874, acc.: 99.61%] [G loss: 5.372954]\n",
            "15731 [D loss: 0.030232, acc.: 99.02%] [G loss: 5.618975]\n",
            "15732 [D loss: 0.049739, acc.: 98.63%] [G loss: 5.299668]\n",
            "15733 [D loss: 0.053770, acc.: 98.63%] [G loss: 6.888958]\n",
            "15734 [D loss: 0.101355, acc.: 97.85%] [G loss: 4.831511]\n",
            "15735 [D loss: 0.046599, acc.: 99.22%] [G loss: 5.002080]\n",
            "15736 [D loss: 0.088838, acc.: 97.46%] [G loss: 7.091351]\n",
            "15737 [D loss: 0.088043, acc.: 97.07%] [G loss: 5.261869]\n",
            "15738 [D loss: 0.058428, acc.: 97.46%] [G loss: 6.855699]\n",
            "15739 [D loss: 0.079975, acc.: 97.85%] [G loss: 5.243867]\n",
            "15740 [D loss: 0.025634, acc.: 99.61%] [G loss: 6.025367]\n",
            "15741 [D loss: 0.081974, acc.: 98.44%] [G loss: 4.408382]\n",
            "15742 [D loss: 0.029876, acc.: 99.02%] [G loss: 5.921003]\n",
            "15743 [D loss: 0.093954, acc.: 97.85%] [G loss: 7.097585]\n",
            "15744 [D loss: 0.149865, acc.: 96.88%] [G loss: 4.618207]\n",
            "15745 [D loss: 0.101905, acc.: 96.88%] [G loss: 7.072873]\n",
            "15746 [D loss: 0.099154, acc.: 95.90%] [G loss: 5.458351]\n",
            "15747 [D loss: 0.066505, acc.: 98.05%] [G loss: 5.356149]\n",
            "15748 [D loss: 0.043205, acc.: 98.83%] [G loss: 5.149773]\n",
            "15749 [D loss: 0.089604, acc.: 97.46%] [G loss: 6.361172]\n",
            "15750 [D loss: 0.075990, acc.: 98.24%] [G loss: 5.227449]\n",
            "15751 [D loss: 0.095771, acc.: 97.66%] [G loss: 5.931682]\n",
            "15752 [D loss: 0.060914, acc.: 98.24%] [G loss: 5.861858]\n",
            "15753 [D loss: 0.143243, acc.: 94.92%] [G loss: 6.201159]\n",
            "15754 [D loss: 0.111985, acc.: 97.66%] [G loss: 4.734064]\n",
            "15755 [D loss: 0.074537, acc.: 96.88%] [G loss: 6.286002]\n",
            "15756 [D loss: 0.071782, acc.: 98.63%] [G loss: 5.837284]\n",
            "15757 [D loss: 0.199056, acc.: 96.09%] [G loss: 7.070926]\n",
            "15758 [D loss: 0.050256, acc.: 98.63%] [G loss: 7.517025]\n",
            "15759 [D loss: 0.172495, acc.: 94.34%] [G loss: 10.750964]\n",
            "15760 [D loss: 0.414802, acc.: 90.82%] [G loss: 7.896649]\n",
            "15761 [D loss: 0.106467, acc.: 98.05%] [G loss: 7.338946]\n",
            "15762 [D loss: 0.067562, acc.: 98.24%] [G loss: 8.600429]\n",
            "15763 [D loss: 0.120221, acc.: 96.68%] [G loss: 7.605419]\n",
            "15764 [D loss: 0.096689, acc.: 96.29%] [G loss: 7.098341]\n",
            "15765 [D loss: 0.081988, acc.: 97.66%] [G loss: 7.049355]\n",
            "15766 [D loss: 0.080603, acc.: 97.66%] [G loss: 7.221698]\n",
            "15767 [D loss: 0.041043, acc.: 98.24%] [G loss: 6.543611]\n",
            "15768 [D loss: 0.072588, acc.: 98.44%] [G loss: 7.199125]\n",
            "15769 [D loss: 0.060539, acc.: 98.24%] [G loss: 6.043643]\n",
            "15770 [D loss: 0.071777, acc.: 98.24%] [G loss: 6.645672]\n",
            "15771 [D loss: 0.081440, acc.: 98.05%] [G loss: 6.165621]\n",
            "15772 [D loss: 0.110891, acc.: 95.70%] [G loss: 6.749907]\n",
            "15773 [D loss: 0.101462, acc.: 97.27%] [G loss: 5.993236]\n",
            "15774 [D loss: 0.198534, acc.: 92.19%] [G loss: 9.705387]\n",
            "15775 [D loss: 0.319211, acc.: 95.70%] [G loss: 6.544534]\n",
            "15776 [D loss: 0.088089, acc.: 98.24%] [G loss: 5.266637]\n",
            "15777 [D loss: 0.139884, acc.: 97.27%] [G loss: 6.700219]\n",
            "15778 [D loss: 0.130782, acc.: 96.68%] [G loss: 5.585294]\n",
            "15779 [D loss: 0.059916, acc.: 98.44%] [G loss: 5.856602]\n",
            "15780 [D loss: 0.078884, acc.: 98.05%] [G loss: 5.542017]\n",
            "15781 [D loss: 0.057770, acc.: 98.24%] [G loss: 5.877511]\n",
            "15782 [D loss: 0.208803, acc.: 93.95%] [G loss: 7.089140]\n",
            "15783 [D loss: 0.081458, acc.: 98.44%] [G loss: 5.751484]\n",
            "15784 [D loss: 0.083382, acc.: 98.44%] [G loss: 5.614678]\n",
            "15785 [D loss: 0.075476, acc.: 97.46%] [G loss: 6.380622]\n",
            "15786 [D loss: 0.070903, acc.: 97.85%] [G loss: 5.741485]\n",
            "15787 [D loss: 0.110856, acc.: 95.90%] [G loss: 7.266752]\n",
            "15788 [D loss: 0.102253, acc.: 97.27%] [G loss: 5.966574]\n",
            "15789 [D loss: 0.055966, acc.: 98.83%] [G loss: 5.616333]\n",
            "15790 [D loss: 0.040020, acc.: 98.63%] [G loss: 5.792901]\n",
            "15791 [D loss: 0.088428, acc.: 96.48%] [G loss: 5.789807]\n",
            "15792 [D loss: 0.093379, acc.: 96.48%] [G loss: 5.894302]\n",
            "15793 [D loss: 0.049812, acc.: 98.83%] [G loss: 5.744550]\n",
            "15794 [D loss: 0.054594, acc.: 98.05%] [G loss: 4.722857]\n",
            "15795 [D loss: 0.081796, acc.: 97.46%] [G loss: 5.699553]\n",
            "15796 [D loss: 0.103630, acc.: 97.46%] [G loss: 5.881423]\n",
            "15797 [D loss: 0.053461, acc.: 98.63%] [G loss: 5.321212]\n",
            "15798 [D loss: 0.138742, acc.: 96.09%] [G loss: 6.151131]\n",
            "15799 [D loss: 0.066260, acc.: 98.63%] [G loss: 5.601838]\n",
            "15800 [D loss: 0.074289, acc.: 98.24%] [G loss: 5.555894]\n",
            "15801 [D loss: 0.019014, acc.: 99.22%] [G loss: 6.237359]\n",
            "15802 [D loss: 0.100734, acc.: 96.29%] [G loss: 5.790252]\n",
            "15803 [D loss: 0.061722, acc.: 98.24%] [G loss: 6.147990]\n",
            "15804 [D loss: 0.122516, acc.: 95.51%] [G loss: 5.522761]\n",
            "15805 [D loss: 0.066925, acc.: 98.44%] [G loss: 5.628287]\n",
            "15806 [D loss: 0.071223, acc.: 98.44%] [G loss: 6.284825]\n",
            "15807 [D loss: 0.110270, acc.: 96.29%] [G loss: 5.182438]\n",
            "15808 [D loss: 0.097492, acc.: 95.90%] [G loss: 6.730270]\n",
            "15809 [D loss: 0.086618, acc.: 98.24%] [G loss: 4.836942]\n",
            "15810 [D loss: 0.115267, acc.: 96.88%] [G loss: 7.454859]\n",
            "15811 [D loss: 0.148415, acc.: 97.07%] [G loss: 7.666489]\n",
            "15812 [D loss: 0.098933, acc.: 98.24%] [G loss: 7.024261]\n",
            "15813 [D loss: 0.049000, acc.: 98.24%] [G loss: 6.826685]\n",
            "15814 [D loss: 0.067271, acc.: 98.44%] [G loss: 6.891875]\n",
            "15815 [D loss: 0.079827, acc.: 97.07%] [G loss: 5.291352]\n",
            "15816 [D loss: 0.055315, acc.: 99.02%] [G loss: 6.342855]\n",
            "15817 [D loss: 0.085594, acc.: 97.85%] [G loss: 4.914002]\n",
            "15818 [D loss: 0.085503, acc.: 97.66%] [G loss: 5.357031]\n",
            "15819 [D loss: 0.075696, acc.: 98.05%] [G loss: 5.074459]\n",
            "15820 [D loss: 0.083640, acc.: 97.27%] [G loss: 5.638678]\n",
            "15821 [D loss: 0.061788, acc.: 98.24%] [G loss: 5.271546]\n",
            "15822 [D loss: 0.166887, acc.: 93.55%] [G loss: 6.444161]\n",
            "15823 [D loss: 0.104465, acc.: 97.85%] [G loss: 5.299119]\n",
            "15824 [D loss: 0.072301, acc.: 98.44%] [G loss: 4.844254]\n",
            "15825 [D loss: 0.064854, acc.: 98.63%] [G loss: 5.322724]\n",
            "15826 [D loss: 0.064302, acc.: 98.05%] [G loss: 4.887854]\n",
            "15827 [D loss: 0.155371, acc.: 94.53%] [G loss: 6.623659]\n",
            "15828 [D loss: 0.069456, acc.: 98.63%] [G loss: 7.859138]\n",
            "15829 [D loss: 0.101070, acc.: 98.24%] [G loss: 7.035401]\n",
            "15830 [D loss: 0.166450, acc.: 96.88%] [G loss: 4.398808]\n",
            "15831 [D loss: 0.077023, acc.: 98.05%] [G loss: 5.205703]\n",
            "15832 [D loss: 0.211531, acc.: 94.92%] [G loss: 9.059735]\n",
            "15833 [D loss: 0.170372, acc.: 96.88%] [G loss: 6.366691]\n",
            "15834 [D loss: 0.115960, acc.: 95.51%] [G loss: 6.064456]\n",
            "15835 [D loss: 0.055194, acc.: 98.44%] [G loss: 5.494583]\n",
            "15836 [D loss: 0.084040, acc.: 97.27%] [G loss: 5.127399]\n",
            "15837 [D loss: 0.097915, acc.: 98.05%] [G loss: 4.779670]\n",
            "15838 [D loss: 0.084892, acc.: 98.05%] [G loss: 4.537018]\n",
            "15839 [D loss: 0.086042, acc.: 98.05%] [G loss: 4.437722]\n",
            "15840 [D loss: 0.121006, acc.: 96.29%] [G loss: 5.923306]\n",
            "15841 [D loss: 0.115782, acc.: 96.88%] [G loss: 4.677681]\n",
            "15842 [D loss: 0.066345, acc.: 97.85%] [G loss: 5.941660]\n",
            "15843 [D loss: 0.102302, acc.: 97.85%] [G loss: 5.260837]\n",
            "15844 [D loss: 0.028154, acc.: 99.22%] [G loss: 5.491032]\n",
            "15845 [D loss: 0.135754, acc.: 96.88%] [G loss: 5.405060]\n",
            "15846 [D loss: 0.131573, acc.: 96.88%] [G loss: 4.832455]\n",
            "15847 [D loss: 0.086462, acc.: 97.66%] [G loss: 4.412019]\n",
            "15848 [D loss: 0.034068, acc.: 98.83%] [G loss: 5.066379]\n",
            "15849 [D loss: 0.118293, acc.: 94.34%] [G loss: 5.254649]\n",
            "15850 [D loss: 0.075299, acc.: 98.05%] [G loss: 4.957414]\n",
            "15851 [D loss: 0.135612, acc.: 94.34%] [G loss: 6.027059]\n",
            "15852 [D loss: 0.093135, acc.: 97.85%] [G loss: 4.723422]\n",
            "15853 [D loss: 0.072748, acc.: 98.05%] [G loss: 4.520020]\n",
            "15854 [D loss: 0.054418, acc.: 98.63%] [G loss: 5.104893]\n",
            "15855 [D loss: 0.112192, acc.: 95.70%] [G loss: 7.518785]\n",
            "15856 [D loss: 0.082466, acc.: 96.68%] [G loss: 7.189070]\n",
            "15857 [D loss: 0.103421, acc.: 95.12%] [G loss: 5.437368]\n",
            "15858 [D loss: 0.089019, acc.: 97.85%] [G loss: 4.819019]\n",
            "15859 [D loss: 0.095207, acc.: 97.66%] [G loss: 4.667557]\n",
            "15860 [D loss: 0.115236, acc.: 97.66%] [G loss: 5.932329]\n",
            "15861 [D loss: 0.138429, acc.: 95.70%] [G loss: 4.117558]\n",
            "15862 [D loss: 0.040962, acc.: 99.22%] [G loss: 6.331164]\n",
            "15863 [D loss: 0.083818, acc.: 97.85%] [G loss: 4.516305]\n",
            "15864 [D loss: 0.070513, acc.: 98.05%] [G loss: 5.843730]\n",
            "15865 [D loss: 0.047830, acc.: 98.44%] [G loss: 4.793465]\n",
            "15866 [D loss: 0.053456, acc.: 99.02%] [G loss: 4.870806]\n",
            "15867 [D loss: 0.095196, acc.: 97.85%] [G loss: 6.862540]\n",
            "15868 [D loss: 0.092241, acc.: 98.05%] [G loss: 6.227121]\n",
            "15869 [D loss: 0.075710, acc.: 98.44%] [G loss: 4.616209]\n",
            "15870 [D loss: 0.079921, acc.: 98.05%] [G loss: 4.776355]\n",
            "15871 [D loss: 0.065161, acc.: 98.05%] [G loss: 4.552198]\n",
            "15872 [D loss: 0.088699, acc.: 97.46%] [G loss: 5.541393]\n",
            "15873 [D loss: 0.079432, acc.: 98.05%] [G loss: 4.469106]\n",
            "15874 [D loss: 0.049941, acc.: 98.83%] [G loss: 4.903488]\n",
            "15875 [D loss: 0.092083, acc.: 97.46%] [G loss: 4.980691]\n",
            "15876 [D loss: 0.064564, acc.: 98.24%] [G loss: 5.445448]\n",
            "15877 [D loss: 0.087586, acc.: 97.85%] [G loss: 6.099359]\n",
            "15878 [D loss: 0.054670, acc.: 99.02%] [G loss: 5.601944]\n",
            "15879 [D loss: 0.109327, acc.: 97.66%] [G loss: 4.388546]\n",
            "15880 [D loss: 0.066318, acc.: 98.05%] [G loss: 4.933363]\n",
            "15881 [D loss: 0.076865, acc.: 98.24%] [G loss: 5.514019]\n",
            "15882 [D loss: 0.072505, acc.: 97.85%] [G loss: 5.171919]\n",
            "15883 [D loss: 0.044669, acc.: 98.24%] [G loss: 5.073980]\n",
            "15884 [D loss: 0.136905, acc.: 97.27%] [G loss: 5.464170]\n",
            "15885 [D loss: 0.083509, acc.: 98.05%] [G loss: 5.203499]\n",
            "15886 [D loss: 0.152601, acc.: 96.29%] [G loss: 6.245535]\n",
            "15887 [D loss: 0.140582, acc.: 96.48%] [G loss: 5.636800]\n",
            "15888 [D loss: 0.092822, acc.: 98.44%] [G loss: 5.156428]\n",
            "15889 [D loss: 0.063119, acc.: 98.44%] [G loss: 5.112520]\n",
            "15890 [D loss: 0.087436, acc.: 98.05%] [G loss: 5.368026]\n",
            "15891 [D loss: 0.081686, acc.: 98.05%] [G loss: 5.307584]\n",
            "15892 [D loss: 0.085436, acc.: 98.24%] [G loss: 5.189668]\n",
            "15893 [D loss: 0.152591, acc.: 95.12%] [G loss: 5.364457]\n",
            "15894 [D loss: 0.062784, acc.: 98.44%] [G loss: 5.238591]\n",
            "15895 [D loss: 0.028568, acc.: 99.41%] [G loss: 5.107750]\n",
            "15896 [D loss: 0.092196, acc.: 97.27%] [G loss: 8.551840]\n",
            "15897 [D loss: 0.087615, acc.: 97.46%] [G loss: 7.946261]\n",
            "15898 [D loss: 0.087420, acc.: 97.46%] [G loss: 5.114428]\n",
            "15899 [D loss: 0.045456, acc.: 98.63%] [G loss: 5.459780]\n",
            "15900 [D loss: 0.074145, acc.: 98.24%] [G loss: 4.713549]\n",
            "15901 [D loss: 0.084930, acc.: 96.48%] [G loss: 5.847459]\n",
            "15902 [D loss: 0.054098, acc.: 98.24%] [G loss: 5.876973]\n",
            "15903 [D loss: 0.126289, acc.: 95.51%] [G loss: 6.298255]\n",
            "15904 [D loss: 0.057200, acc.: 98.05%] [G loss: 5.576951]\n",
            "15905 [D loss: 0.112127, acc.: 96.48%] [G loss: 6.931978]\n",
            "15906 [D loss: 0.112714, acc.: 97.85%] [G loss: 6.889943]\n",
            "15907 [D loss: 0.051401, acc.: 99.02%] [G loss: 5.085180]\n",
            "15908 [D loss: 0.038735, acc.: 98.63%] [G loss: 4.618637]\n",
            "15909 [D loss: 0.061180, acc.: 99.02%] [G loss: 4.937576]\n",
            "15910 [D loss: 0.101269, acc.: 97.85%] [G loss: 7.088140]\n",
            "15911 [D loss: 0.117882, acc.: 97.07%] [G loss: 5.109003]\n",
            "15912 [D loss: 0.254592, acc.: 91.21%] [G loss: 13.961250]\n",
            "15913 [D loss: 0.604727, acc.: 90.43%] [G loss: 10.214076]\n",
            "15914 [D loss: 0.289520, acc.: 94.92%] [G loss: 11.839621]\n",
            "15915 [D loss: 0.231762, acc.: 93.16%] [G loss: 8.763305]\n",
            "15916 [D loss: 0.181775, acc.: 96.48%] [G loss: 7.109038]\n",
            "15917 [D loss: 0.153777, acc.: 96.48%] [G loss: 7.750732]\n",
            "15918 [D loss: 0.031465, acc.: 99.41%] [G loss: 7.105760]\n",
            "15919 [D loss: 0.102539, acc.: 97.85%] [G loss: 5.860058]\n",
            "15920 [D loss: 0.248269, acc.: 90.62%] [G loss: 9.314288]\n",
            "15921 [D loss: 0.142417, acc.: 97.46%] [G loss: 10.433436]\n",
            "15922 [D loss: 0.244064, acc.: 97.27%] [G loss: 7.237574]\n",
            "15923 [D loss: 0.293199, acc.: 94.14%] [G loss: 10.805418]\n",
            "15924 [D loss: 0.213237, acc.: 96.68%] [G loss: 8.589881]\n",
            "15925 [D loss: 0.105379, acc.: 98.05%] [G loss: 6.187572]\n",
            "15926 [D loss: 0.082385, acc.: 96.88%] [G loss: 6.849636]\n",
            "15927 [D loss: 0.096571, acc.: 96.29%] [G loss: 8.598322]\n",
            "15928 [D loss: 0.101293, acc.: 98.44%] [G loss: 6.452286]\n",
            "15929 [D loss: 0.124621, acc.: 97.46%] [G loss: 6.808092]\n",
            "15930 [D loss: 0.063083, acc.: 97.46%] [G loss: 6.394732]\n",
            "15931 [D loss: 0.172335, acc.: 93.55%] [G loss: 7.442277]\n",
            "15932 [D loss: 0.064295, acc.: 98.63%] [G loss: 7.748149]\n",
            "15933 [D loss: 0.177262, acc.: 97.27%] [G loss: 5.349996]\n",
            "15934 [D loss: 0.087939, acc.: 98.05%] [G loss: 5.541994]\n",
            "15935 [D loss: 0.079522, acc.: 96.88%] [G loss: 5.773118]\n",
            "15936 [D loss: 0.021989, acc.: 99.41%] [G loss: 6.071371]\n",
            "15937 [D loss: 0.165123, acc.: 97.46%] [G loss: 5.485931]\n",
            "15938 [D loss: 0.050078, acc.: 97.27%] [G loss: 6.236026]\n",
            "15939 [D loss: 0.081604, acc.: 98.44%] [G loss: 5.637996]\n",
            "15940 [D loss: 0.025977, acc.: 99.22%] [G loss: 5.657508]\n",
            "15941 [D loss: 0.095342, acc.: 97.46%] [G loss: 5.153571]\n",
            "15942 [D loss: 0.063095, acc.: 98.63%] [G loss: 5.216868]\n",
            "15943 [D loss: 0.083039, acc.: 97.85%] [G loss: 5.724167]\n",
            "15944 [D loss: 0.081448, acc.: 98.05%] [G loss: 5.293283]\n",
            "15945 [D loss: 0.070596, acc.: 98.24%] [G loss: 5.045698]\n",
            "15946 [D loss: 0.072883, acc.: 98.24%] [G loss: 5.112452]\n",
            "15947 [D loss: 0.102166, acc.: 97.66%] [G loss: 5.461268]\n",
            "15948 [D loss: 0.099169, acc.: 97.27%] [G loss: 5.382247]\n",
            "15949 [D loss: 0.133351, acc.: 94.53%] [G loss: 6.530493]\n",
            "15950 [D loss: 0.134811, acc.: 97.27%] [G loss: 4.832565]\n",
            "15951 [D loss: 0.117954, acc.: 98.05%] [G loss: 6.115503]\n",
            "15952 [D loss: 0.088808, acc.: 98.05%] [G loss: 6.406122]\n",
            "15953 [D loss: 0.182862, acc.: 94.73%] [G loss: 5.678401]\n",
            "15954 [D loss: 0.051106, acc.: 98.63%] [G loss: 5.654730]\n",
            "15955 [D loss: 0.157575, acc.: 96.68%] [G loss: 6.074257]\n",
            "15956 [D loss: 0.112531, acc.: 96.09%] [G loss: 5.970964]\n",
            "15957 [D loss: 0.080195, acc.: 97.66%] [G loss: 6.524727]\n",
            "15958 [D loss: 0.092633, acc.: 98.44%] [G loss: 5.134459]\n",
            "15959 [D loss: 0.125977, acc.: 96.48%] [G loss: 5.998718]\n",
            "15960 [D loss: 0.079924, acc.: 97.27%] [G loss: 5.752572]\n",
            "15961 [D loss: 0.157090, acc.: 93.95%] [G loss: 10.096328]\n",
            "15962 [D loss: 0.279224, acc.: 97.46%] [G loss: 7.516635]\n",
            "15963 [D loss: 0.117050, acc.: 98.44%] [G loss: 6.247228]\n",
            "15964 [D loss: 0.095859, acc.: 97.27%] [G loss: 5.159961]\n",
            "15965 [D loss: 0.063954, acc.: 98.83%] [G loss: 6.261281]\n",
            "15966 [D loss: 0.192849, acc.: 96.09%] [G loss: 5.473732]\n",
            "15967 [D loss: 0.079790, acc.: 97.66%] [G loss: 5.295531]\n",
            "15968 [D loss: 0.161333, acc.: 95.70%] [G loss: 4.720132]\n",
            "15969 [D loss: 0.053842, acc.: 98.44%] [G loss: 5.020697]\n",
            "15970 [D loss: 0.083948, acc.: 97.66%] [G loss: 4.705681]\n",
            "15971 [D loss: 0.054188, acc.: 98.24%] [G loss: 4.985747]\n",
            "15972 [D loss: 0.069911, acc.: 98.24%] [G loss: 5.094287]\n",
            "15973 [D loss: 0.085913, acc.: 98.05%] [G loss: 4.996863]\n",
            "15974 [D loss: 0.137123, acc.: 96.29%] [G loss: 6.259281]\n",
            "15975 [D loss: 0.136573, acc.: 97.07%] [G loss: 5.187191]\n",
            "15976 [D loss: 0.085246, acc.: 98.05%] [G loss: 6.045333]\n",
            "15977 [D loss: 0.032416, acc.: 99.80%] [G loss: 5.637176]\n",
            "15978 [D loss: 0.108592, acc.: 96.88%] [G loss: 6.469829]\n",
            "15979 [D loss: 0.105448, acc.: 97.07%] [G loss: 4.799571]\n",
            "15980 [D loss: 0.138374, acc.: 93.75%] [G loss: 6.966547]\n",
            "15981 [D loss: 0.150527, acc.: 96.48%] [G loss: 4.578562]\n",
            "15982 [D loss: 0.140702, acc.: 97.27%] [G loss: 7.212988]\n",
            "15983 [D loss: 0.092014, acc.: 98.24%] [G loss: 7.205772]\n",
            "15984 [D loss: 0.092688, acc.: 98.44%] [G loss: 5.482171]\n",
            "15985 [D loss: 0.125054, acc.: 95.12%] [G loss: 6.596251]\n",
            "15986 [D loss: 0.094326, acc.: 97.66%] [G loss: 5.798491]\n",
            "15987 [D loss: 0.160471, acc.: 94.34%] [G loss: 5.507990]\n",
            "15988 [D loss: 0.026721, acc.: 98.83%] [G loss: 6.010308]\n",
            "15989 [D loss: 0.109862, acc.: 97.66%] [G loss: 5.603907]\n",
            "15990 [D loss: 0.125409, acc.: 96.09%] [G loss: 4.509043]\n",
            "15991 [D loss: 0.085765, acc.: 97.27%] [G loss: 5.971982]\n",
            "15992 [D loss: 0.085388, acc.: 98.05%] [G loss: 5.101014]\n",
            "15993 [D loss: 0.088540, acc.: 97.46%] [G loss: 5.975508]\n",
            "15994 [D loss: 0.063004, acc.: 98.44%] [G loss: 6.816115]\n",
            "15995 [D loss: 0.150672, acc.: 97.07%] [G loss: 6.783144]\n",
            "15996 [D loss: 0.125007, acc.: 95.70%] [G loss: 5.410922]\n",
            "15997 [D loss: 0.119393, acc.: 96.88%] [G loss: 6.648804]\n",
            "15998 [D loss: 0.116115, acc.: 97.07%] [G loss: 6.100126]\n",
            "15999 [D loss: 0.124995, acc.: 97.46%] [G loss: 5.356193]\n",
            "16000 [D loss: 0.078851, acc.: 98.24%] [G loss: 4.751970]\n",
            "16001 [D loss: 0.065895, acc.: 98.05%] [G loss: 5.268836]\n",
            "16002 [D loss: 0.156991, acc.: 95.31%] [G loss: 6.064569]\n",
            "16003 [D loss: 0.070585, acc.: 98.05%] [G loss: 5.240277]\n",
            "16004 [D loss: 0.066155, acc.: 98.63%] [G loss: 4.831889]\n",
            "16005 [D loss: 0.123632, acc.: 97.27%] [G loss: 5.074575]\n",
            "16006 [D loss: 0.075240, acc.: 97.85%] [G loss: 4.875197]\n",
            "16007 [D loss: 0.096563, acc.: 97.66%] [G loss: 5.493414]\n",
            "16008 [D loss: 0.084099, acc.: 97.66%] [G loss: 5.014656]\n",
            "16009 [D loss: 0.062857, acc.: 97.66%] [G loss: 5.789547]\n",
            "16010 [D loss: 0.115555, acc.: 97.07%] [G loss: 5.314593]\n",
            "16011 [D loss: 0.047766, acc.: 98.63%] [G loss: 5.483413]\n",
            "16012 [D loss: 0.083991, acc.: 97.85%] [G loss: 4.495452]\n",
            "16013 [D loss: 0.054359, acc.: 98.63%] [G loss: 4.718242]\n",
            "16014 [D loss: 0.072383, acc.: 98.24%] [G loss: 5.225817]\n",
            "16015 [D loss: 0.085482, acc.: 97.46%] [G loss: 5.368449]\n",
            "16016 [D loss: 0.151000, acc.: 96.48%] [G loss: 5.872344]\n",
            "16017 [D loss: 0.101900, acc.: 97.46%] [G loss: 5.080643]\n",
            "16018 [D loss: 0.087697, acc.: 97.27%] [G loss: 5.798906]\n",
            "16019 [D loss: 0.089532, acc.: 97.66%] [G loss: 5.318213]\n",
            "16020 [D loss: 0.156591, acc.: 92.97%] [G loss: 8.310664]\n",
            "16021 [D loss: 0.248670, acc.: 94.92%] [G loss: 4.949994]\n",
            "16022 [D loss: 0.070355, acc.: 98.05%] [G loss: 5.293820]\n",
            "16023 [D loss: 0.039623, acc.: 99.02%] [G loss: 4.891559]\n",
            "16024 [D loss: 0.106216, acc.: 96.88%] [G loss: 5.322551]\n",
            "16025 [D loss: 0.109921, acc.: 97.46%] [G loss: 5.156957]\n",
            "16026 [D loss: 0.065557, acc.: 99.41%] [G loss: 6.222082]\n",
            "16027 [D loss: 0.121219, acc.: 97.85%] [G loss: 5.887258]\n",
            "16028 [D loss: 0.093275, acc.: 97.46%] [G loss: 5.539862]\n",
            "16029 [D loss: 0.110591, acc.: 95.51%] [G loss: 5.693786]\n",
            "16030 [D loss: 0.095411, acc.: 97.27%] [G loss: 4.930723]\n",
            "16031 [D loss: 0.064986, acc.: 98.63%] [G loss: 4.477515]\n",
            "16032 [D loss: 0.051371, acc.: 98.05%] [G loss: 5.518529]\n",
            "16033 [D loss: 0.055915, acc.: 98.63%] [G loss: 5.988624]\n",
            "16034 [D loss: 0.101535, acc.: 96.88%] [G loss: 4.787269]\n",
            "16035 [D loss: 0.036629, acc.: 99.22%] [G loss: 5.231482]\n",
            "16036 [D loss: 0.143536, acc.: 96.48%] [G loss: 5.515893]\n",
            "16037 [D loss: 0.042407, acc.: 98.44%] [G loss: 5.455753]\n",
            "16038 [D loss: 0.065327, acc.: 98.83%] [G loss: 5.407652]\n",
            "16039 [D loss: 0.038886, acc.: 98.83%] [G loss: 5.436731]\n",
            "16040 [D loss: 0.090912, acc.: 97.66%] [G loss: 4.983385]\n",
            "16041 [D loss: 0.079565, acc.: 97.66%] [G loss: 4.663391]\n",
            "16042 [D loss: 0.097718, acc.: 97.66%] [G loss: 6.119715]\n",
            "16043 [D loss: 0.031725, acc.: 98.83%] [G loss: 5.705879]\n",
            "16044 [D loss: 0.127226, acc.: 96.48%] [G loss: 5.904794]\n",
            "16045 [D loss: 0.115110, acc.: 97.27%] [G loss: 6.285113]\n",
            "16046 [D loss: 0.104862, acc.: 97.66%] [G loss: 4.737572]\n",
            "16047 [D loss: 0.095379, acc.: 96.29%] [G loss: 6.628886]\n",
            "16048 [D loss: 0.276927, acc.: 91.21%] [G loss: 9.583614]\n",
            "16049 [D loss: 0.225499, acc.: 96.88%] [G loss: 7.332509]\n",
            "16050 [D loss: 0.200372, acc.: 95.70%] [G loss: 4.267238]\n",
            "16051 [D loss: 0.104498, acc.: 97.27%] [G loss: 6.417612]\n",
            "16052 [D loss: 0.071277, acc.: 97.66%] [G loss: 5.451300]\n",
            "16053 [D loss: 0.078801, acc.: 98.44%] [G loss: 5.122953]\n",
            "16054 [D loss: 0.088008, acc.: 97.46%] [G loss: 4.996429]\n",
            "16055 [D loss: 0.066956, acc.: 98.05%] [G loss: 5.567100]\n",
            "16056 [D loss: 0.137187, acc.: 96.68%] [G loss: 5.476116]\n",
            "16057 [D loss: 0.049932, acc.: 98.63%] [G loss: 6.095412]\n",
            "16058 [D loss: 0.056066, acc.: 98.63%] [G loss: 5.416054]\n",
            "16059 [D loss: 0.123464, acc.: 96.09%] [G loss: 5.005830]\n",
            "16060 [D loss: 0.091551, acc.: 97.07%] [G loss: 5.141850]\n",
            "16061 [D loss: 0.069636, acc.: 98.24%] [G loss: 5.214191]\n",
            "16062 [D loss: 0.089821, acc.: 97.85%] [G loss: 4.955153]\n",
            "16063 [D loss: 0.110293, acc.: 97.07%] [G loss: 5.365950]\n",
            "16064 [D loss: 0.086690, acc.: 97.27%] [G loss: 5.447946]\n",
            "16065 [D loss: 0.104649, acc.: 96.68%] [G loss: 5.506370]\n",
            "16066 [D loss: 0.100119, acc.: 97.27%] [G loss: 5.946023]\n",
            "16067 [D loss: 0.137795, acc.: 95.51%] [G loss: 5.213921]\n",
            "16068 [D loss: 0.068380, acc.: 97.66%] [G loss: 5.253492]\n",
            "16069 [D loss: 0.244154, acc.: 89.26%] [G loss: 9.389634]\n",
            "16070 [D loss: 0.148316, acc.: 97.66%] [G loss: 6.514741]\n",
            "16071 [D loss: 0.242288, acc.: 96.48%] [G loss: 5.886331]\n",
            "16072 [D loss: 0.103574, acc.: 96.29%] [G loss: 5.321050]\n",
            "16073 [D loss: 0.075728, acc.: 97.85%] [G loss: 7.773143]\n",
            "16074 [D loss: 0.082363, acc.: 97.66%] [G loss: 5.080482]\n",
            "16075 [D loss: 0.041183, acc.: 98.83%] [G loss: 6.001478]\n",
            "16076 [D loss: 0.074209, acc.: 97.85%] [G loss: 5.212760]\n",
            "16077 [D loss: 0.026138, acc.: 99.41%] [G loss: 5.811402]\n",
            "16078 [D loss: 0.135800, acc.: 96.29%] [G loss: 5.981519]\n",
            "16079 [D loss: 0.048194, acc.: 98.24%] [G loss: 5.601426]\n",
            "16080 [D loss: 0.113683, acc.: 96.88%] [G loss: 5.042317]\n",
            "16081 [D loss: 0.066804, acc.: 98.05%] [G loss: 5.557489]\n",
            "16082 [D loss: 0.036789, acc.: 98.83%] [G loss: 5.446056]\n",
            "16083 [D loss: 0.085849, acc.: 97.66%] [G loss: 5.232842]\n",
            "16084 [D loss: 0.091540, acc.: 97.85%] [G loss: 5.363848]\n",
            "16085 [D loss: 0.047657, acc.: 98.44%] [G loss: 5.103153]\n",
            "16086 [D loss: 0.085077, acc.: 97.85%] [G loss: 4.744991]\n",
            "16087 [D loss: 0.139419, acc.: 95.51%] [G loss: 5.607778]\n",
            "16088 [D loss: 0.119819, acc.: 96.29%] [G loss: 5.151369]\n",
            "16089 [D loss: 0.099937, acc.: 97.46%] [G loss: 5.335018]\n",
            "16090 [D loss: 0.084687, acc.: 96.68%] [G loss: 5.720436]\n",
            "16091 [D loss: 0.085581, acc.: 97.46%] [G loss: 5.714635]\n",
            "16092 [D loss: 0.042638, acc.: 99.02%] [G loss: 5.425307]\n",
            "16093 [D loss: 0.114751, acc.: 97.27%] [G loss: 5.554183]\n",
            "16094 [D loss: 0.062818, acc.: 98.83%] [G loss: 6.524740]\n",
            "16095 [D loss: 0.063821, acc.: 97.66%] [G loss: 5.888962]\n",
            "16096 [D loss: 0.039040, acc.: 98.63%] [G loss: 6.550075]\n",
            "16097 [D loss: 0.071868, acc.: 98.63%] [G loss: 5.766545]\n",
            "16098 [D loss: 0.085318, acc.: 97.46%] [G loss: 5.511853]\n",
            "16099 [D loss: 0.066274, acc.: 98.44%] [G loss: 5.003480]\n",
            "16100 [D loss: 0.070366, acc.: 97.27%] [G loss: 4.852272]\n",
            "16101 [D loss: 0.096759, acc.: 97.66%] [G loss: 5.681482]\n",
            "16102 [D loss: 0.138723, acc.: 95.70%] [G loss: 4.634479]\n",
            "16103 [D loss: 0.070343, acc.: 98.05%] [G loss: 5.367399]\n",
            "16104 [D loss: 0.100632, acc.: 98.05%] [G loss: 5.090033]\n",
            "16105 [D loss: 0.122933, acc.: 96.68%] [G loss: 5.029799]\n",
            "16106 [D loss: 0.087044, acc.: 98.05%] [G loss: 5.162352]\n",
            "16107 [D loss: 0.104678, acc.: 97.66%] [G loss: 5.057717]\n",
            "16108 [D loss: 0.091472, acc.: 97.66%] [G loss: 6.432633]\n",
            "16109 [D loss: 0.163449, acc.: 94.34%] [G loss: 6.220797]\n",
            "16110 [D loss: 0.107609, acc.: 97.46%] [G loss: 4.950912]\n",
            "16111 [D loss: 0.108406, acc.: 97.07%] [G loss: 5.121099]\n",
            "16112 [D loss: 0.080638, acc.: 97.66%] [G loss: 4.760982]\n",
            "16113 [D loss: 0.082890, acc.: 98.05%] [G loss: 5.709147]\n",
            "16114 [D loss: 0.051840, acc.: 98.05%] [G loss: 4.408902]\n",
            "16115 [D loss: 0.091384, acc.: 97.66%] [G loss: 5.350070]\n",
            "16116 [D loss: 0.081340, acc.: 97.85%] [G loss: 5.506394]\n",
            "16117 [D loss: 0.068550, acc.: 98.44%] [G loss: 5.825333]\n",
            "16118 [D loss: 0.125016, acc.: 96.48%] [G loss: 5.725371]\n",
            "16119 [D loss: 0.082223, acc.: 97.27%] [G loss: 5.089759]\n",
            "16120 [D loss: 0.026715, acc.: 99.61%] [G loss: 6.476132]\n",
            "16121 [D loss: 0.163420, acc.: 94.34%] [G loss: 7.366579]\n",
            "16122 [D loss: 0.132844, acc.: 98.24%] [G loss: 7.611079]\n",
            "16123 [D loss: 0.145746, acc.: 97.66%] [G loss: 5.231625]\n",
            "16124 [D loss: 0.190496, acc.: 96.29%] [G loss: 5.183877]\n",
            "16125 [D loss: 0.099376, acc.: 97.66%] [G loss: 4.946138]\n",
            "16126 [D loss: 0.132317, acc.: 96.29%] [G loss: 5.380160]\n",
            "16127 [D loss: 0.118330, acc.: 97.46%] [G loss: 5.442701]\n",
            "16128 [D loss: 0.096701, acc.: 97.07%] [G loss: 5.709059]\n",
            "16129 [D loss: 0.058987, acc.: 97.27%] [G loss: 5.752932]\n",
            "16130 [D loss: 0.111405, acc.: 97.85%] [G loss: 5.819293]\n",
            "16131 [D loss: 0.174939, acc.: 95.90%] [G loss: 6.738982]\n",
            "16132 [D loss: 0.113507, acc.: 96.29%] [G loss: 5.394682]\n",
            "16133 [D loss: 0.107380, acc.: 97.46%] [G loss: 5.334876]\n",
            "16134 [D loss: 0.061374, acc.: 97.85%] [G loss: 5.568170]\n",
            "16135 [D loss: 0.173072, acc.: 95.12%] [G loss: 6.055074]\n",
            "16136 [D loss: 0.078644, acc.: 97.66%] [G loss: 6.504285]\n",
            "16137 [D loss: 0.116234, acc.: 97.66%] [G loss: 6.880394]\n",
            "16138 [D loss: 0.085759, acc.: 96.88%] [G loss: 5.652012]\n",
            "16139 [D loss: 0.091550, acc.: 97.07%] [G loss: 4.995536]\n",
            "16140 [D loss: 0.097532, acc.: 97.46%] [G loss: 5.026832]\n",
            "16141 [D loss: 0.078322, acc.: 97.85%] [G loss: 5.050122]\n",
            "16142 [D loss: 0.077639, acc.: 97.85%] [G loss: 4.796630]\n",
            "16143 [D loss: 0.073547, acc.: 98.05%] [G loss: 4.684866]\n",
            "16144 [D loss: 0.062670, acc.: 98.83%] [G loss: 4.907278]\n",
            "16145 [D loss: 0.062151, acc.: 98.44%] [G loss: 6.192286]\n",
            "16146 [D loss: 0.094195, acc.: 97.85%] [G loss: 5.215530]\n",
            "16147 [D loss: 0.079700, acc.: 97.27%] [G loss: 6.019519]\n",
            "16148 [D loss: 0.113131, acc.: 96.88%] [G loss: 5.326847]\n",
            "16149 [D loss: 0.051763, acc.: 98.05%] [G loss: 5.508896]\n",
            "16150 [D loss: 0.090584, acc.: 97.66%] [G loss: 5.531056]\n",
            "16151 [D loss: 0.087010, acc.: 97.46%] [G loss: 5.520767]\n",
            "16152 [D loss: 0.043341, acc.: 99.02%] [G loss: 7.320264]\n",
            "16153 [D loss: 0.124518, acc.: 96.68%] [G loss: 5.603208]\n",
            "16154 [D loss: 0.077838, acc.: 97.66%] [G loss: 5.170090]\n",
            "16155 [D loss: 0.107798, acc.: 96.68%] [G loss: 7.772615]\n",
            "16156 [D loss: 0.277250, acc.: 95.90%] [G loss: 4.616847]\n",
            "16157 [D loss: 0.046444, acc.: 98.24%] [G loss: 5.259336]\n",
            "16158 [D loss: 0.034748, acc.: 99.22%] [G loss: 6.855300]\n",
            "16159 [D loss: 0.151421, acc.: 96.48%] [G loss: 4.818604]\n",
            "16160 [D loss: 0.100167, acc.: 96.88%] [G loss: 5.794990]\n",
            "16161 [D loss: 0.054348, acc.: 97.46%] [G loss: 5.667403]\n",
            "16162 [D loss: 0.160514, acc.: 95.31%] [G loss: 9.255166]\n",
            "16163 [D loss: 0.125143, acc.: 98.05%] [G loss: 7.543055]\n",
            "16164 [D loss: 0.242727, acc.: 95.31%] [G loss: 6.941551]\n",
            "16165 [D loss: 0.122329, acc.: 97.66%] [G loss: 4.246269]\n",
            "16166 [D loss: 0.112070, acc.: 96.68%] [G loss: 6.983204]\n",
            "16167 [D loss: 0.190873, acc.: 97.07%] [G loss: 4.655249]\n",
            "16168 [D loss: 0.198227, acc.: 94.53%] [G loss: 5.449417]\n",
            "16169 [D loss: 0.126760, acc.: 96.48%] [G loss: 6.111396]\n",
            "16170 [D loss: 0.083386, acc.: 96.88%] [G loss: 6.531053]\n",
            "16171 [D loss: 0.131994, acc.: 95.31%] [G loss: 7.106287]\n",
            "16172 [D loss: 0.098949, acc.: 97.85%] [G loss: 5.204223]\n",
            "16173 [D loss: 0.131039, acc.: 97.66%] [G loss: 5.589819]\n",
            "16174 [D loss: 0.082654, acc.: 97.27%] [G loss: 5.066445]\n",
            "16175 [D loss: 0.132689, acc.: 96.09%] [G loss: 5.310001]\n",
            "16176 [D loss: 0.074248, acc.: 97.46%] [G loss: 5.225378]\n",
            "16177 [D loss: 0.094959, acc.: 97.85%] [G loss: 5.024129]\n",
            "16178 [D loss: 0.097668, acc.: 97.46%] [G loss: 5.403519]\n",
            "16179 [D loss: 0.075857, acc.: 98.44%] [G loss: 5.273890]\n",
            "16180 [D loss: 0.093737, acc.: 96.88%] [G loss: 4.995622]\n",
            "16181 [D loss: 0.061134, acc.: 98.05%] [G loss: 5.619553]\n",
            "16182 [D loss: 0.166303, acc.: 92.97%] [G loss: 6.521365]\n",
            "16183 [D loss: 0.063921, acc.: 98.24%] [G loss: 5.748021]\n",
            "16184 [D loss: 0.122228, acc.: 97.85%] [G loss: 4.980041]\n",
            "16185 [D loss: 0.203939, acc.: 96.48%] [G loss: 8.437212]\n",
            "16186 [D loss: 0.224675, acc.: 96.68%] [G loss: 7.573422]\n",
            "16187 [D loss: 0.102372, acc.: 98.44%] [G loss: 6.234142]\n",
            "16188 [D loss: 0.277152, acc.: 91.99%] [G loss: 5.488601]\n",
            "16189 [D loss: 0.075121, acc.: 98.24%] [G loss: 5.081411]\n",
            "16190 [D loss: 0.079644, acc.: 98.24%] [G loss: 4.520930]\n",
            "16191 [D loss: 0.100084, acc.: 97.66%] [G loss: 4.298306]\n",
            "16192 [D loss: 0.057714, acc.: 98.63%] [G loss: 4.674010]\n",
            "16193 [D loss: 0.061991, acc.: 98.44%] [G loss: 4.457198]\n",
            "16194 [D loss: 0.086226, acc.: 97.85%] [G loss: 4.456923]\n",
            "16195 [D loss: 0.096525, acc.: 97.66%] [G loss: 4.639248]\n",
            "16196 [D loss: 0.110002, acc.: 97.07%] [G loss: 4.845189]\n",
            "16197 [D loss: 0.063290, acc.: 98.44%] [G loss: 4.590261]\n",
            "16198 [D loss: 0.073337, acc.: 98.05%] [G loss: 4.187410]\n",
            "16199 [D loss: 0.113151, acc.: 96.88%] [G loss: 4.836357]\n",
            "16200 [D loss: 0.132172, acc.: 96.48%] [G loss: 4.060999]\n",
            "16201 [D loss: 0.097825, acc.: 97.46%] [G loss: 4.955109]\n",
            "16202 [D loss: 0.132652, acc.: 96.09%] [G loss: 4.351631]\n",
            "16203 [D loss: 0.051733, acc.: 98.63%] [G loss: 4.762038]\n",
            "16204 [D loss: 0.071011, acc.: 98.24%] [G loss: 4.658010]\n",
            "16205 [D loss: 0.100262, acc.: 96.48%] [G loss: 5.170718]\n",
            "16206 [D loss: 0.083652, acc.: 98.05%] [G loss: 4.553167]\n",
            "16207 [D loss: 0.181039, acc.: 95.90%] [G loss: 4.851303]\n",
            "16208 [D loss: 0.043801, acc.: 98.83%] [G loss: 5.653988]\n",
            "16209 [D loss: 0.140957, acc.: 97.07%] [G loss: 4.450131]\n",
            "16210 [D loss: 0.094371, acc.: 97.85%] [G loss: 4.754279]\n",
            "16211 [D loss: 0.068065, acc.: 98.24%] [G loss: 4.718235]\n",
            "16212 [D loss: 0.087725, acc.: 97.46%] [G loss: 4.577579]\n",
            "16213 [D loss: 0.093665, acc.: 97.07%] [G loss: 5.252579]\n",
            "16214 [D loss: 0.064448, acc.: 98.44%] [G loss: 5.592795]\n",
            "16215 [D loss: 0.047127, acc.: 97.46%] [G loss: 5.133133]\n",
            "16216 [D loss: 0.154817, acc.: 96.09%] [G loss: 7.049754]\n",
            "16217 [D loss: 0.074872, acc.: 98.05%] [G loss: 5.926653]\n",
            "16218 [D loss: 0.106073, acc.: 97.27%] [G loss: 7.221765]\n",
            "16219 [D loss: 0.346585, acc.: 90.82%] [G loss: 9.372961]\n",
            "16220 [D loss: 0.375070, acc.: 93.55%] [G loss: 8.881389]\n",
            "16221 [D loss: 0.163950, acc.: 96.88%] [G loss: 8.433341]\n",
            "16222 [D loss: 0.598376, acc.: 71.68%] [G loss: 22.032352]\n",
            "16223 [D loss: 1.031324, acc.: 94.92%] [G loss: 16.567972]\n",
            "16224 [D loss: 0.501202, acc.: 95.90%] [G loss: 17.390625]\n",
            "16225 [D loss: 1.228834, acc.: 84.18%] [G loss: 21.920736]\n",
            "16226 [D loss: 0.876726, acc.: 95.31%] [G loss: 10.818791]\n",
            "16227 [D loss: 0.663475, acc.: 94.34%] [G loss: 10.868633]\n",
            "16228 [D loss: 0.421716, acc.: 96.09%] [G loss: 6.517570]\n",
            "16229 [D loss: 0.189906, acc.: 96.88%] [G loss: 13.681485]\n",
            "16230 [D loss: 0.326530, acc.: 97.85%] [G loss: 12.662447]\n",
            "16231 [D loss: 0.290081, acc.: 98.05%] [G loss: 7.039516]\n",
            "16232 [D loss: 0.502269, acc.: 86.13%] [G loss: 15.134657]\n",
            "16233 [D loss: 0.643069, acc.: 96.29%] [G loss: 14.370762]\n",
            "16234 [D loss: 0.484489, acc.: 96.29%] [G loss: 9.047044]\n",
            "16235 [D loss: 0.248566, acc.: 97.66%] [G loss: 5.954053]\n",
            "16236 [D loss: 0.237320, acc.: 90.82%] [G loss: 15.994193]\n",
            "16237 [D loss: 0.260862, acc.: 97.66%] [G loss: 12.687220]\n",
            "16238 [D loss: 0.542910, acc.: 95.12%] [G loss: 4.176936]\n",
            "16239 [D loss: 1.176614, acc.: 52.93%] [G loss: 20.838667]\n",
            "16240 [D loss: 1.298207, acc.: 89.84%] [G loss: 14.437758]\n",
            "16241 [D loss: 2.574861, acc.: 51.17%] [G loss: 33.976784]\n",
            "16242 [D loss: 48.711079, acc.: 41.21%] [G loss: 93.443367]\n",
            "16243 [D loss: 70.189198, acc.: 10.94%] [G loss: 23.631611]\n",
            "16244 [D loss: 9.902340, acc.: 45.31%] [G loss: 58.611664]\n",
            "16245 [D loss: 26.426411, acc.: 33.20%] [G loss: 11.491663]\n",
            "16246 [D loss: 2.169985, acc.: 77.34%] [G loss: 29.869244]\n",
            "16247 [D loss: 1.639385, acc.: 86.91%] [G loss: 29.584467]\n",
            "16248 [D loss: 0.067058, acc.: 97.07%] [G loss: 19.263670]\n",
            "16249 [D loss: 1.670071, acc.: 69.14%] [G loss: 69.841705]\n",
            "16250 [D loss: 21.474799, acc.: 23.83%] [G loss: 44.225334]\n",
            "16251 [D loss: 16.829464, acc.: 65.82%] [G loss: 117.009361]\n",
            "16252 [D loss: 13.426489, acc.: 42.97%] [G loss: 103.815674]\n",
            "16253 [D loss: 13.710262, acc.: 75.78%] [G loss: 20.614285]\n",
            "16254 [D loss: 14.072599, acc.: 46.29%] [G loss: 88.010498]\n",
            "16255 [D loss: 8.348534, acc.: 79.69%] [G loss: 11.016766]\n",
            "16256 [D loss: 10.780961, acc.: 50.39%] [G loss: 92.920898]\n",
            "16257 [D loss: 11.948412, acc.: 68.75%] [G loss: 20.999680]\n",
            "16258 [D loss: 3.258963, acc.: 64.84%] [G loss: 79.022148]\n",
            "16259 [D loss: 1.780235, acc.: 91.80%] [G loss: 37.815674]\n",
            "16260 [D loss: 1.480317, acc.: 88.87%] [G loss: 33.215504]\n",
            "16261 [D loss: 0.362455, acc.: 94.34%] [G loss: 24.231867]\n",
            "16262 [D loss: 1.824365, acc.: 77.15%] [G loss: 32.249863]\n",
            "16263 [D loss: 1.367071, acc.: 92.19%] [G loss: 28.089745]\n",
            "16264 [D loss: 2.007401, acc.: 67.38%] [G loss: 40.426834]\n",
            "16265 [D loss: 3.485577, acc.: 85.74%] [G loss: 20.397886]\n",
            "16266 [D loss: 1.008346, acc.: 88.48%] [G loss: 15.603969]\n",
            "16267 [D loss: 1.096045, acc.: 86.52%] [G loss: 30.124304]\n",
            "16268 [D loss: 2.474097, acc.: 70.70%] [G loss: 34.770481]\n",
            "16269 [D loss: 2.290221, acc.: 88.09%] [G loss: 13.428429]\n",
            "16270 [D loss: 1.025993, acc.: 81.25%] [G loss: 21.148302]\n",
            "16271 [D loss: 0.889674, acc.: 87.70%] [G loss: 9.490194]\n",
            "16272 [D loss: 0.887897, acc.: 75.00%] [G loss: 29.242130]\n",
            "16273 [D loss: 2.484740, acc.: 83.79%] [G loss: 15.797492]\n",
            "16274 [D loss: 1.137604, acc.: 83.40%] [G loss: 16.876236]\n",
            "16275 [D loss: 1.379607, acc.: 82.42%] [G loss: 12.222982]\n",
            "16276 [D loss: 1.006096, acc.: 89.84%] [G loss: 16.699261]\n",
            "16277 [D loss: 0.657358, acc.: 90.62%] [G loss: 16.153976]\n",
            "16278 [D loss: 0.333053, acc.: 90.43%] [G loss: 10.549267]\n",
            "16279 [D loss: 0.335380, acc.: 93.55%] [G loss: 17.355907]\n",
            "16280 [D loss: 0.246785, acc.: 95.31%] [G loss: 13.143693]\n",
            "16281 [D loss: 0.711391, acc.: 85.55%] [G loss: 16.403662]\n",
            "16282 [D loss: 0.170693, acc.: 96.88%] [G loss: 15.476340]\n",
            "16283 [D loss: 0.295305, acc.: 92.19%] [G loss: 10.700701]\n",
            "16284 [D loss: 0.080901, acc.: 97.46%] [G loss: 18.249758]\n",
            "16285 [D loss: 0.083940, acc.: 97.66%] [G loss: 12.393990]\n",
            "16286 [D loss: 0.055123, acc.: 98.83%] [G loss: 10.806007]\n",
            "16287 [D loss: 0.133860, acc.: 96.88%] [G loss: 9.576843]\n",
            "16288 [D loss: 0.528671, acc.: 89.45%] [G loss: 16.101112]\n",
            "16289 [D loss: 1.970076, acc.: 61.91%] [G loss: 28.944853]\n",
            "16290 [D loss: 2.900375, acc.: 82.23%] [G loss: 17.443819]\n",
            "16291 [D loss: 0.441792, acc.: 90.82%] [G loss: 19.440765]\n",
            "16292 [D loss: 0.279883, acc.: 95.31%] [G loss: 18.348057]\n",
            "16293 [D loss: 0.319125, acc.: 94.73%] [G loss: 10.886719]\n",
            "16294 [D loss: 0.185401, acc.: 94.53%] [G loss: 13.751896]\n",
            "16295 [D loss: 0.063811, acc.: 98.24%] [G loss: 13.488361]\n",
            "16296 [D loss: 0.120666, acc.: 96.68%] [G loss: 9.629213]\n",
            "16297 [D loss: 0.098803, acc.: 96.88%] [G loss: 9.025197]\n",
            "16298 [D loss: 0.074703, acc.: 98.24%] [G loss: 8.722103]\n",
            "16299 [D loss: 0.093459, acc.: 97.07%] [G loss: 10.464838]\n",
            "16300 [D loss: 0.141593, acc.: 95.31%] [G loss: 9.437203]\n",
            "16301 [D loss: 0.079518, acc.: 98.05%] [G loss: 8.324637]\n",
            "16302 [D loss: 0.116147, acc.: 95.90%] [G loss: 9.337811]\n",
            "16303 [D loss: 1.100476, acc.: 66.60%] [G loss: 22.198040]\n",
            "16304 [D loss: 2.089601, acc.: 85.74%] [G loss: 11.016205]\n",
            "16305 [D loss: 1.183781, acc.: 81.05%] [G loss: 16.464117]\n",
            "16306 [D loss: 0.062393, acc.: 97.46%] [G loss: 17.312019]\n",
            "16307 [D loss: 0.260555, acc.: 95.31%] [G loss: 12.880002]\n",
            "16308 [D loss: 0.238063, acc.: 91.41%] [G loss: 13.696358]\n",
            "16309 [D loss: 0.101837, acc.: 97.46%] [G loss: 14.981220]\n",
            "16310 [D loss: 0.136101, acc.: 94.92%] [G loss: 7.573678]\n",
            "16311 [D loss: 0.026917, acc.: 99.61%] [G loss: 12.340353]\n",
            "16312 [D loss: 0.027182, acc.: 98.83%] [G loss: 8.349836]\n",
            "16313 [D loss: 0.016319, acc.: 99.61%] [G loss: 8.889139]\n",
            "16314 [D loss: 0.022412, acc.: 99.80%] [G loss: 7.936216]\n",
            "16315 [D loss: 0.031145, acc.: 99.22%] [G loss: 8.150506]\n",
            "16316 [D loss: 0.038430, acc.: 99.22%] [G loss: 8.095286]\n",
            "16317 [D loss: 0.101648, acc.: 96.29%] [G loss: 9.009748]\n",
            "16318 [D loss: 1.225454, acc.: 59.18%] [G loss: 27.779865]\n",
            "16319 [D loss: 2.577490, acc.: 84.77%] [G loss: 16.965490]\n",
            "16320 [D loss: 1.090007, acc.: 88.48%] [G loss: 9.848212]\n",
            "16321 [D loss: 0.099701, acc.: 97.46%] [G loss: 10.697740]\n",
            "16322 [D loss: 0.204007, acc.: 94.73%] [G loss: 14.557691]\n",
            "16323 [D loss: 0.042402, acc.: 98.24%] [G loss: 11.659836]\n",
            "16324 [D loss: 0.049380, acc.: 98.63%] [G loss: 13.009804]\n",
            "16325 [D loss: 0.040897, acc.: 99.61%] [G loss: 8.817999]\n",
            "16326 [D loss: 0.057389, acc.: 98.44%] [G loss: 8.061178]\n",
            "16327 [D loss: 0.022175, acc.: 100.00%] [G loss: 8.217266]\n",
            "16328 [D loss: 0.034687, acc.: 100.00%] [G loss: 7.041318]\n",
            "16329 [D loss: 0.022039, acc.: 100.00%] [G loss: 8.618788]\n",
            "16330 [D loss: 0.030997, acc.: 99.22%] [G loss: 6.889742]\n",
            "16331 [D loss: 0.039590, acc.: 99.22%] [G loss: 7.715512]\n",
            "16332 [D loss: 0.050064, acc.: 99.22%] [G loss: 7.477568]\n",
            "16333 [D loss: 0.047342, acc.: 99.61%] [G loss: 6.949243]\n",
            "16334 [D loss: 0.021919, acc.: 100.00%] [G loss: 6.582335]\n",
            "16335 [D loss: 0.019336, acc.: 100.00%] [G loss: 6.254805]\n",
            "16336 [D loss: 0.021967, acc.: 99.61%] [G loss: 6.610197]\n",
            "16337 [D loss: 0.020618, acc.: 100.00%] [G loss: 6.926970]\n",
            "16338 [D loss: 0.050318, acc.: 98.63%] [G loss: 5.885292]\n",
            "16339 [D loss: 0.036276, acc.: 99.22%] [G loss: 7.499540]\n",
            "16340 [D loss: 0.114959, acc.: 96.09%] [G loss: 6.269681]\n",
            "16341 [D loss: 0.060852, acc.: 97.27%] [G loss: 6.418718]\n",
            "16342 [D loss: 0.026883, acc.: 100.00%] [G loss: 6.726266]\n",
            "16343 [D loss: 0.039752, acc.: 99.61%] [G loss: 6.533697]\n",
            "16344 [D loss: 0.020969, acc.: 100.00%] [G loss: 6.105812]\n",
            "16345 [D loss: 0.043128, acc.: 98.83%] [G loss: 7.022182]\n",
            "16346 [D loss: 0.254483, acc.: 92.58%] [G loss: 8.639355]\n",
            "16347 [D loss: 0.344699, acc.: 87.70%] [G loss: 10.134786]\n",
            "16348 [D loss: 0.306024, acc.: 93.75%] [G loss: 6.137400]\n",
            "16349 [D loss: 0.251759, acc.: 92.38%] [G loss: 9.782237]\n",
            "16350 [D loss: 0.094648, acc.: 97.46%] [G loss: 7.398921]\n",
            "16351 [D loss: 0.069263, acc.: 97.85%] [G loss: 6.650373]\n",
            "16352 [D loss: 0.016010, acc.: 99.80%] [G loss: 7.349152]\n",
            "16353 [D loss: 0.037487, acc.: 99.22%] [G loss: 6.842322]\n",
            "16354 [D loss: 0.063535, acc.: 98.05%] [G loss: 10.656719]\n",
            "16355 [D loss: 0.036913, acc.: 99.22%] [G loss: 7.633387]\n",
            "16356 [D loss: 0.055246, acc.: 99.02%] [G loss: 7.041103]\n",
            "16357 [D loss: 0.047927, acc.: 98.83%] [G loss: 5.465351]\n",
            "16358 [D loss: 0.033531, acc.: 99.61%] [G loss: 5.669322]\n",
            "16359 [D loss: 0.093932, acc.: 97.07%] [G loss: 8.314115]\n",
            "16360 [D loss: 0.146897, acc.: 97.07%] [G loss: 6.701583]\n",
            "16361 [D loss: 0.128997, acc.: 96.29%] [G loss: 8.220751]\n",
            "16362 [D loss: 1.400610, acc.: 85.16%] [G loss: 11.731940]\n",
            "16363 [D loss: 0.742310, acc.: 86.33%] [G loss: 11.944702]\n",
            "16364 [D loss: 0.486657, acc.: 87.50%] [G loss: 11.953112]\n",
            "16365 [D loss: 0.194104, acc.: 95.51%] [G loss: 10.990999]\n",
            "16366 [D loss: 0.155192, acc.: 94.14%] [G loss: 5.923477]\n",
            "16367 [D loss: 0.091056, acc.: 98.05%] [G loss: 8.960444]\n",
            "16368 [D loss: 0.051750, acc.: 96.88%] [G loss: 7.063231]\n",
            "16369 [D loss: 0.022965, acc.: 99.02%] [G loss: 7.238796]\n",
            "16370 [D loss: 0.073014, acc.: 99.22%] [G loss: 6.869561]\n",
            "16371 [D loss: 0.014331, acc.: 99.80%] [G loss: 6.683670]\n",
            "16372 [D loss: 0.072730, acc.: 98.44%] [G loss: 7.984980]\n",
            "16373 [D loss: 0.271318, acc.: 89.06%] [G loss: 10.220511]\n",
            "16374 [D loss: 0.199997, acc.: 95.90%] [G loss: 8.696655]\n",
            "16375 [D loss: 0.060336, acc.: 98.63%] [G loss: 6.417490]\n",
            "16376 [D loss: 0.062546, acc.: 98.24%] [G loss: 7.349120]\n",
            "16377 [D loss: 0.045290, acc.: 98.44%] [G loss: 7.175556]\n",
            "16378 [D loss: 0.064196, acc.: 99.02%] [G loss: 5.929617]\n",
            "16379 [D loss: 0.082387, acc.: 96.88%] [G loss: 7.615561]\n",
            "16380 [D loss: 0.059341, acc.: 98.05%] [G loss: 5.728938]\n",
            "16381 [D loss: 0.033674, acc.: 99.80%] [G loss: 6.842512]\n",
            "16382 [D loss: 0.023806, acc.: 100.00%] [G loss: 6.340217]\n",
            "16383 [D loss: 0.022998, acc.: 99.80%] [G loss: 5.791435]\n",
            "16384 [D loss: 0.031568, acc.: 99.41%] [G loss: 6.753645]\n",
            "16385 [D loss: 0.037495, acc.: 98.05%] [G loss: 6.381454]\n",
            "16386 [D loss: 0.025855, acc.: 100.00%] [G loss: 6.172602]\n",
            "16387 [D loss: 0.113230, acc.: 94.53%] [G loss: 8.566125]\n",
            "16388 [D loss: 0.126970, acc.: 98.05%] [G loss: 5.415489]\n",
            "16389 [D loss: 0.122895, acc.: 95.90%] [G loss: 8.743267]\n",
            "16390 [D loss: 0.223446, acc.: 92.77%] [G loss: 6.596925]\n",
            "16391 [D loss: 0.020020, acc.: 100.00%] [G loss: 5.688756]\n",
            "16392 [D loss: 0.019818, acc.: 100.00%] [G loss: 6.888177]\n",
            "16393 [D loss: 0.023789, acc.: 99.80%] [G loss: 5.964860]\n",
            "16394 [D loss: 0.012718, acc.: 100.00%] [G loss: 6.132332]\n",
            "16395 [D loss: 0.019878, acc.: 100.00%] [G loss: 6.601046]\n",
            "16396 [D loss: 0.023939, acc.: 99.02%] [G loss: 6.879662]\n",
            "16397 [D loss: 0.125983, acc.: 96.48%] [G loss: 7.422665]\n",
            "16398 [D loss: 0.196752, acc.: 90.43%] [G loss: 9.333481]\n",
            "16399 [D loss: 0.168345, acc.: 97.27%] [G loss: 7.054955]\n",
            "16400 [D loss: 0.104468, acc.: 96.88%] [G loss: 8.516304]\n",
            "16401 [D loss: 0.026208, acc.: 100.00%] [G loss: 8.319291]\n",
            "16402 [D loss: 0.018320, acc.: 99.80%] [G loss: 6.786608]\n",
            "16403 [D loss: 0.164340, acc.: 93.55%] [G loss: 10.455355]\n",
            "16404 [D loss: 0.245976, acc.: 93.36%] [G loss: 6.036669]\n",
            "16405 [D loss: 0.029486, acc.: 99.80%] [G loss: 6.138156]\n",
            "16406 [D loss: 0.010239, acc.: 99.41%] [G loss: 7.198738]\n",
            "16407 [D loss: 0.060371, acc.: 97.85%] [G loss: 5.458984]\n",
            "16408 [D loss: 0.026338, acc.: 99.61%] [G loss: 7.405586]\n",
            "16409 [D loss: 0.076072, acc.: 96.68%] [G loss: 6.575294]\n",
            "16410 [D loss: 0.113005, acc.: 97.07%] [G loss: 6.946876]\n",
            "16411 [D loss: 0.035097, acc.: 98.63%] [G loss: 5.935077]\n",
            "16412 [D loss: 0.084668, acc.: 96.09%] [G loss: 7.814354]\n",
            "16413 [D loss: 0.149442, acc.: 96.29%] [G loss: 9.302322]\n",
            "16414 [D loss: 0.058665, acc.: 97.66%] [G loss: 6.576080]\n",
            "16415 [D loss: 0.082479, acc.: 97.27%] [G loss: 6.435869]\n",
            "16416 [D loss: 0.063531, acc.: 97.85%] [G loss: 5.836806]\n",
            "16417 [D loss: 0.017803, acc.: 100.00%] [G loss: 6.316611]\n",
            "16418 [D loss: 0.043141, acc.: 98.63%] [G loss: 6.390785]\n",
            "16419 [D loss: 0.029281, acc.: 98.83%] [G loss: 5.178449]\n",
            "16420 [D loss: 0.025665, acc.: 99.41%] [G loss: 6.223989]\n",
            "16421 [D loss: 0.053873, acc.: 97.85%] [G loss: 6.922758]\n",
            "16422 [D loss: 0.054981, acc.: 98.24%] [G loss: 6.347831]\n",
            "16423 [D loss: 0.024025, acc.: 99.41%] [G loss: 6.322108]\n",
            "16424 [D loss: 0.021240, acc.: 99.61%] [G loss: 6.712815]\n",
            "16425 [D loss: 0.039760, acc.: 98.24%] [G loss: 7.921320]\n",
            "16426 [D loss: 0.052530, acc.: 99.22%] [G loss: 9.489766]\n",
            "16427 [D loss: 0.285894, acc.: 92.58%] [G loss: 5.459097]\n",
            "16428 [D loss: 0.107946, acc.: 97.07%] [G loss: 9.877376]\n",
            "16429 [D loss: 0.193320, acc.: 95.31%] [G loss: 4.961080]\n",
            "16430 [D loss: 0.275040, acc.: 87.50%] [G loss: 15.785359]\n",
            "16431 [D loss: 0.969417, acc.: 91.99%] [G loss: 6.985474]\n",
            "16432 [D loss: 0.289608, acc.: 91.80%] [G loss: 10.817868]\n",
            "16433 [D loss: 0.173978, acc.: 96.68%] [G loss: 10.851577]\n",
            "16434 [D loss: 0.094986, acc.: 97.85%] [G loss: 5.242028]\n",
            "16435 [D loss: 0.032132, acc.: 100.00%] [G loss: 6.938251]\n",
            "16436 [D loss: 0.034006, acc.: 98.44%] [G loss: 6.073818]\n",
            "16437 [D loss: 0.023858, acc.: 100.00%] [G loss: 5.545549]\n",
            "16438 [D loss: 0.046128, acc.: 98.05%] [G loss: 5.628788]\n",
            "16439 [D loss: 0.012003, acc.: 100.00%] [G loss: 5.800320]\n",
            "16440 [D loss: 0.097314, acc.: 97.46%] [G loss: 7.959331]\n",
            "16441 [D loss: 0.077600, acc.: 98.05%] [G loss: 6.251099]\n",
            "16442 [D loss: 0.040059, acc.: 98.44%] [G loss: 6.337936]\n",
            "16443 [D loss: 0.017147, acc.: 99.22%] [G loss: 5.756616]\n",
            "16444 [D loss: 0.021458, acc.: 100.00%] [G loss: 5.324456]\n",
            "16445 [D loss: 0.018727, acc.: 99.22%] [G loss: 5.876807]\n",
            "16446 [D loss: 0.018657, acc.: 100.00%] [G loss: 5.531476]\n",
            "16447 [D loss: 0.055425, acc.: 98.63%] [G loss: 6.116374]\n",
            "16448 [D loss: 0.045351, acc.: 98.44%] [G loss: 5.420780]\n",
            "16449 [D loss: 0.035719, acc.: 99.02%] [G loss: 5.932319]\n",
            "16450 [D loss: 0.057235, acc.: 98.44%] [G loss: 6.115753]\n",
            "16451 [D loss: 0.104968, acc.: 97.27%] [G loss: 6.447348]\n",
            "16452 [D loss: 0.057712, acc.: 98.63%] [G loss: 7.780254]\n",
            "16453 [D loss: 0.048792, acc.: 98.63%] [G loss: 9.337602]\n",
            "16454 [D loss: 0.024778, acc.: 98.44%] [G loss: 6.238369]\n",
            "16455 [D loss: 0.017339, acc.: 100.00%] [G loss: 6.080597]\n",
            "16456 [D loss: 0.025855, acc.: 100.00%] [G loss: 7.505151]\n",
            "16457 [D loss: 0.044000, acc.: 98.83%] [G loss: 5.432126]\n",
            "16458 [D loss: 0.035208, acc.: 99.41%] [G loss: 6.090606]\n",
            "16459 [D loss: 0.029625, acc.: 99.02%] [G loss: 6.735378]\n",
            "16460 [D loss: 0.055704, acc.: 98.63%] [G loss: 6.273874]\n",
            "16461 [D loss: 0.060792, acc.: 97.85%] [G loss: 6.143891]\n",
            "16462 [D loss: 0.050220, acc.: 98.24%] [G loss: 7.481675]\n",
            "16463 [D loss: 0.313535, acc.: 89.84%] [G loss: 12.702298]\n",
            "16464 [D loss: 0.300840, acc.: 96.29%] [G loss: 9.461682]\n",
            "16465 [D loss: 0.561848, acc.: 90.23%] [G loss: 10.538250]\n",
            "16466 [D loss: 0.178672, acc.: 97.07%] [G loss: 7.163837]\n",
            "16467 [D loss: 0.072890, acc.: 98.83%] [G loss: 5.788641]\n",
            "16468 [D loss: 0.052503, acc.: 99.41%] [G loss: 7.861827]\n",
            "16469 [D loss: 0.052038, acc.: 98.83%] [G loss: 6.744925]\n",
            "16470 [D loss: 0.040740, acc.: 99.02%] [G loss: 5.769220]\n",
            "16471 [D loss: 0.056498, acc.: 98.63%] [G loss: 7.954381]\n",
            "16472 [D loss: 0.090407, acc.: 97.66%] [G loss: 6.451151]\n",
            "16473 [D loss: 0.279126, acc.: 84.96%] [G loss: 14.738989]\n",
            "16474 [D loss: 0.761359, acc.: 93.16%] [G loss: 10.788055]\n",
            "16475 [D loss: 0.383912, acc.: 94.34%] [G loss: 4.877159]\n",
            "16476 [D loss: 0.036777, acc.: 99.41%] [G loss: 7.873610]\n",
            "16477 [D loss: 0.217928, acc.: 96.48%] [G loss: 6.002633]\n",
            "16478 [D loss: 0.045492, acc.: 98.05%] [G loss: 7.183622]\n",
            "16479 [D loss: 0.069563, acc.: 97.27%] [G loss: 5.291903]\n",
            "16480 [D loss: 0.072144, acc.: 98.24%] [G loss: 8.048739]\n",
            "16481 [D loss: 0.128337, acc.: 97.85%] [G loss: 5.782679]\n",
            "16482 [D loss: 0.071057, acc.: 97.07%] [G loss: 7.472268]\n",
            "16483 [D loss: 0.030848, acc.: 99.22%] [G loss: 8.666258]\n",
            "16484 [D loss: 0.140064, acc.: 95.31%] [G loss: 5.062653]\n",
            "16485 [D loss: 0.048059, acc.: 99.22%] [G loss: 6.221363]\n",
            "16486 [D loss: 0.031104, acc.: 99.02%] [G loss: 5.244893]\n",
            "16487 [D loss: 0.140493, acc.: 95.90%] [G loss: 9.510015]\n",
            "16488 [D loss: 0.229882, acc.: 95.90%] [G loss: 6.883019]\n",
            "16489 [D loss: 0.276940, acc.: 92.97%] [G loss: 9.374431]\n",
            "16490 [D loss: 0.211283, acc.: 96.48%] [G loss: 5.310878]\n",
            "16491 [D loss: 0.064893, acc.: 98.83%] [G loss: 6.037136]\n",
            "16492 [D loss: 0.031393, acc.: 99.22%] [G loss: 5.601513]\n",
            "16493 [D loss: 0.053779, acc.: 98.83%] [G loss: 5.243915]\n",
            "16494 [D loss: 0.130164, acc.: 96.68%] [G loss: 8.578055]\n",
            "16495 [D loss: 0.110479, acc.: 97.07%] [G loss: 6.378182]\n",
            "16496 [D loss: 0.134877, acc.: 96.48%] [G loss: 6.060263]\n",
            "16497 [D loss: 0.066596, acc.: 97.66%] [G loss: 5.346056]\n",
            "16498 [D loss: 0.041193, acc.: 98.83%] [G loss: 6.151651]\n",
            "16499 [D loss: 0.011903, acc.: 99.41%] [G loss: 6.614861]\n",
            "16500 [D loss: 0.095221, acc.: 97.85%] [G loss: 8.561411]\n",
            "16501 [D loss: 0.070526, acc.: 98.05%] [G loss: 6.377282]\n",
            "16502 [D loss: 0.098428, acc.: 96.48%] [G loss: 6.159056]\n",
            "16503 [D loss: 0.068696, acc.: 98.24%] [G loss: 4.879951]\n",
            "16504 [D loss: 0.036679, acc.: 98.63%] [G loss: 5.822630]\n",
            "16505 [D loss: 0.164166, acc.: 95.70%] [G loss: 8.821513]\n",
            "16506 [D loss: 0.164502, acc.: 96.29%] [G loss: 5.948606]\n",
            "16507 [D loss: 0.102975, acc.: 96.09%] [G loss: 6.985572]\n",
            "16508 [D loss: 0.042378, acc.: 98.83%] [G loss: 6.341372]\n",
            "16509 [D loss: 0.220298, acc.: 91.60%] [G loss: 12.011359]\n",
            "16510 [D loss: 0.261216, acc.: 94.14%] [G loss: 4.847344]\n",
            "16511 [D loss: 0.228857, acc.: 92.97%] [G loss: 10.416433]\n",
            "16512 [D loss: 0.234068, acc.: 97.27%] [G loss: 9.585483]\n",
            "16513 [D loss: 0.216443, acc.: 95.70%] [G loss: 9.349859]\n",
            "16514 [D loss: 0.179983, acc.: 97.46%] [G loss: 6.338023]\n",
            "16515 [D loss: 0.109506, acc.: 97.27%] [G loss: 5.795680]\n",
            "16516 [D loss: 0.110586, acc.: 97.07%] [G loss: 6.325200]\n",
            "16517 [D loss: 0.067620, acc.: 98.24%] [G loss: 5.566747]\n",
            "16518 [D loss: 0.032959, acc.: 99.41%] [G loss: 5.418138]\n",
            "16519 [D loss: 0.030889, acc.: 99.02%] [G loss: 6.038939]\n",
            "16520 [D loss: 0.062140, acc.: 98.44%] [G loss: 7.058279]\n",
            "16521 [D loss: 0.069139, acc.: 98.44%] [G loss: 5.048656]\n",
            "16522 [D loss: 0.079896, acc.: 98.44%] [G loss: 7.629037]\n",
            "16523 [D loss: 0.050768, acc.: 99.02%] [G loss: 10.173031]\n",
            "16524 [D loss: 0.096291, acc.: 98.44%] [G loss: 4.901817]\n",
            "16525 [D loss: 0.103627, acc.: 96.88%] [G loss: 8.112757]\n",
            "16526 [D loss: 0.099181, acc.: 98.63%] [G loss: 9.301820]\n",
            "16527 [D loss: 0.277339, acc.: 96.48%] [G loss: 3.591306]\n",
            "16528 [D loss: 0.136730, acc.: 97.85%] [G loss: 13.536385]\n",
            "16529 [D loss: 0.235628, acc.: 96.68%] [G loss: 9.461535]\n",
            "16530 [D loss: 0.183921, acc.: 95.70%] [G loss: 7.044069]\n",
            "16531 [D loss: 0.467485, acc.: 88.67%] [G loss: 9.998690]\n",
            "16532 [D loss: 0.274015, acc.: 97.07%] [G loss: 10.599876]\n",
            "16533 [D loss: 0.328854, acc.: 94.92%] [G loss: 4.435919]\n",
            "16534 [D loss: 0.147953, acc.: 96.09%] [G loss: 7.888214]\n",
            "16535 [D loss: 0.046247, acc.: 98.44%] [G loss: 6.827340]\n",
            "16536 [D loss: 0.114410, acc.: 97.46%] [G loss: 4.417416]\n",
            "16537 [D loss: 0.061263, acc.: 99.22%] [G loss: 5.999227]\n",
            "16538 [D loss: 0.086404, acc.: 97.66%] [G loss: 5.056627]\n",
            "16539 [D loss: 0.034821, acc.: 100.00%] [G loss: 5.299100]\n",
            "16540 [D loss: 0.023801, acc.: 99.02%] [G loss: 5.122997]\n",
            "16541 [D loss: 0.006858, acc.: 100.00%] [G loss: 6.032491]\n",
            "16542 [D loss: 0.113217, acc.: 96.88%] [G loss: 6.776801]\n",
            "16543 [D loss: 0.044570, acc.: 98.83%] [G loss: 6.800526]\n",
            "16544 [D loss: 0.048255, acc.: 99.02%] [G loss: 5.808888]\n",
            "16545 [D loss: 0.130922, acc.: 95.12%] [G loss: 10.454683]\n",
            "16546 [D loss: 0.152775, acc.: 96.48%] [G loss: 6.187768]\n",
            "16547 [D loss: 0.127758, acc.: 96.29%] [G loss: 6.135821]\n",
            "16548 [D loss: 0.117210, acc.: 98.05%] [G loss: 5.843566]\n",
            "16549 [D loss: 0.105095, acc.: 98.24%] [G loss: 4.697450]\n",
            "16550 [D loss: 0.032653, acc.: 98.63%] [G loss: 6.357451]\n",
            "16551 [D loss: 0.027409, acc.: 99.80%] [G loss: 5.211638]\n",
            "16552 [D loss: 0.055367, acc.: 98.83%] [G loss: 6.407073]\n",
            "16553 [D loss: 0.027050, acc.: 99.02%] [G loss: 5.704092]\n",
            "16554 [D loss: 0.029578, acc.: 99.41%] [G loss: 5.762327]\n",
            "16555 [D loss: 0.019268, acc.: 99.22%] [G loss: 5.991947]\n",
            "16556 [D loss: 0.230680, acc.: 89.06%] [G loss: 10.904428]\n",
            "16557 [D loss: 0.281219, acc.: 95.70%] [G loss: 10.831617]\n",
            "16558 [D loss: 0.442068, acc.: 96.29%] [G loss: 5.904038]\n",
            "16559 [D loss: 0.230149, acc.: 93.75%] [G loss: 7.131026]\n",
            "16560 [D loss: 0.138023, acc.: 97.85%] [G loss: 7.802308]\n",
            "16561 [D loss: 0.069487, acc.: 98.83%] [G loss: 9.492503]\n",
            "16562 [D loss: 0.035861, acc.: 99.22%] [G loss: 6.130617]\n",
            "16563 [D loss: 0.043093, acc.: 98.24%] [G loss: 10.541557]\n",
            "16564 [D loss: 0.021829, acc.: 98.83%] [G loss: 13.032962]\n",
            "16565 [D loss: 0.027138, acc.: 98.63%] [G loss: 7.635433]\n",
            "16566 [D loss: 0.012199, acc.: 100.00%] [G loss: 7.437767]\n",
            "16567 [D loss: 0.022960, acc.: 100.00%] [G loss: 7.218181]\n",
            "16568 [D loss: 0.035507, acc.: 98.63%] [G loss: 5.306908]\n",
            "16569 [D loss: 0.029718, acc.: 98.83%] [G loss: 4.941050]\n",
            "16570 [D loss: 0.019584, acc.: 100.00%] [G loss: 5.449657]\n",
            "16571 [D loss: 0.054087, acc.: 98.24%] [G loss: 5.469675]\n",
            "16572 [D loss: 0.185518, acc.: 94.92%] [G loss: 8.601644]\n",
            "16573 [D loss: 0.183314, acc.: 97.07%] [G loss: 5.741686]\n",
            "16574 [D loss: 0.059938, acc.: 98.24%] [G loss: 5.069612]\n",
            "16575 [D loss: 0.044735, acc.: 99.61%] [G loss: 6.659462]\n",
            "16576 [D loss: 0.097463, acc.: 97.27%] [G loss: 5.254460]\n",
            "16577 [D loss: 0.076388, acc.: 97.46%] [G loss: 5.627247]\n",
            "16578 [D loss: 0.073100, acc.: 98.05%] [G loss: 5.411171]\n",
            "16579 [D loss: 0.043196, acc.: 98.63%] [G loss: 5.926895]\n",
            "16580 [D loss: 0.066660, acc.: 97.85%] [G loss: 4.725700]\n",
            "16581 [D loss: 0.058480, acc.: 96.68%] [G loss: 4.848982]\n",
            "16582 [D loss: 0.049730, acc.: 97.46%] [G loss: 5.291699]\n",
            "16583 [D loss: 0.020202, acc.: 99.22%] [G loss: 7.236729]\n",
            "16584 [D loss: 0.065459, acc.: 97.85%] [G loss: 7.391690]\n",
            "16585 [D loss: 0.067807, acc.: 98.05%] [G loss: 5.810810]\n",
            "16586 [D loss: 0.033104, acc.: 99.22%] [G loss: 5.395949]\n",
            "16587 [D loss: 0.042174, acc.: 98.24%] [G loss: 5.456162]\n",
            "16588 [D loss: 0.039569, acc.: 100.00%] [G loss: 5.551106]\n",
            "16589 [D loss: 0.039398, acc.: 98.44%] [G loss: 5.250255]\n",
            "16590 [D loss: 0.070572, acc.: 99.22%] [G loss: 7.437757]\n",
            "16591 [D loss: 0.143934, acc.: 97.27%] [G loss: 7.346554]\n",
            "16592 [D loss: 0.076938, acc.: 97.66%] [G loss: 6.341131]\n",
            "16593 [D loss: 0.036610, acc.: 98.44%] [G loss: 5.824502]\n",
            "16594 [D loss: 0.077762, acc.: 98.44%] [G loss: 7.793309]\n",
            "16595 [D loss: 0.068460, acc.: 98.83%] [G loss: 6.718508]\n",
            "16596 [D loss: 0.104371, acc.: 98.44%] [G loss: 4.809083]\n",
            "16597 [D loss: 0.039639, acc.: 99.61%] [G loss: 6.806550]\n",
            "16598 [D loss: 0.073566, acc.: 96.68%] [G loss: 5.763218]\n",
            "16599 [D loss: 0.103680, acc.: 97.66%] [G loss: 15.128284]\n",
            "16600 [D loss: 0.658077, acc.: 90.23%] [G loss: 8.264937]\n",
            "16601 [D loss: 0.054297, acc.: 98.24%] [G loss: 5.788105]\n",
            "16602 [D loss: 0.138496, acc.: 96.88%] [G loss: 7.022685]\n",
            "16603 [D loss: 0.184661, acc.: 94.92%] [G loss: 6.132932]\n",
            "16604 [D loss: 0.094376, acc.: 98.24%] [G loss: 8.153500]\n",
            "16605 [D loss: 0.082807, acc.: 98.05%] [G loss: 9.738127]\n",
            "16606 [D loss: 0.244569, acc.: 97.46%] [G loss: 4.438693]\n",
            "16607 [D loss: 0.189592, acc.: 95.12%] [G loss: 9.043335]\n",
            "16608 [D loss: 0.146610, acc.: 98.83%] [G loss: 10.813203]\n",
            "16609 [D loss: 0.270983, acc.: 97.46%] [G loss: 6.466834]\n",
            "16610 [D loss: 0.129571, acc.: 97.66%] [G loss: 5.611643]\n",
            "16611 [D loss: 0.067188, acc.: 98.24%] [G loss: 4.701927]\n",
            "16612 [D loss: 0.055386, acc.: 97.85%] [G loss: 5.044964]\n",
            "16613 [D loss: 0.035939, acc.: 99.22%] [G loss: 5.024617]\n",
            "16614 [D loss: 0.094196, acc.: 98.05%] [G loss: 5.272484]\n",
            "16615 [D loss: 0.075473, acc.: 98.24%] [G loss: 4.767763]\n",
            "16616 [D loss: 0.040012, acc.: 98.63%] [G loss: 4.756543]\n",
            "16617 [D loss: 0.056307, acc.: 98.05%] [G loss: 5.361969]\n",
            "16618 [D loss: 0.054543, acc.: 98.24%] [G loss: 5.761132]\n",
            "16619 [D loss: 0.054908, acc.: 98.44%] [G loss: 5.644124]\n",
            "16620 [D loss: 0.096099, acc.: 97.27%] [G loss: 5.111468]\n",
            "16621 [D loss: 0.066747, acc.: 98.05%] [G loss: 4.853675]\n",
            "16622 [D loss: 0.055559, acc.: 97.66%] [G loss: 4.798535]\n",
            "16623 [D loss: 0.047437, acc.: 99.61%] [G loss: 8.064599]\n",
            "16624 [D loss: 0.142274, acc.: 97.07%] [G loss: 6.088766]\n",
            "16625 [D loss: 0.044829, acc.: 98.44%] [G loss: 7.585651]\n",
            "16626 [D loss: 0.035716, acc.: 99.02%] [G loss: 5.142157]\n",
            "16627 [D loss: 0.094618, acc.: 97.85%] [G loss: 8.261504]\n",
            "16628 [D loss: 0.060185, acc.: 98.24%] [G loss: 6.430004]\n",
            "16629 [D loss: 0.108798, acc.: 97.07%] [G loss: 5.796378]\n",
            "16630 [D loss: 0.060113, acc.: 98.44%] [G loss: 4.960320]\n",
            "16631 [D loss: 0.166416, acc.: 93.75%] [G loss: 9.180617]\n",
            "16632 [D loss: 0.178724, acc.: 97.07%] [G loss: 7.549479]\n",
            "16633 [D loss: 0.188585, acc.: 96.09%] [G loss: 5.904064]\n",
            "16634 [D loss: 0.211676, acc.: 92.77%] [G loss: 7.974506]\n",
            "16635 [D loss: 0.099200, acc.: 97.66%] [G loss: 7.249824]\n",
            "16636 [D loss: 0.138017, acc.: 97.27%] [G loss: 5.310802]\n",
            "16637 [D loss: 0.068997, acc.: 98.44%] [G loss: 7.254291]\n",
            "16638 [D loss: 0.075899, acc.: 97.85%] [G loss: 6.706031]\n",
            "16639 [D loss: 0.028272, acc.: 99.02%] [G loss: 7.145550]\n",
            "16640 [D loss: 0.062837, acc.: 98.63%] [G loss: 5.424206]\n",
            "16641 [D loss: 0.043233, acc.: 99.02%] [G loss: 5.533363]\n",
            "16642 [D loss: 0.040122, acc.: 98.63%] [G loss: 5.399245]\n",
            "16643 [D loss: 0.054308, acc.: 98.83%] [G loss: 5.447253]\n",
            "16644 [D loss: 0.055695, acc.: 98.44%] [G loss: 5.705163]\n",
            "16645 [D loss: 0.150098, acc.: 95.51%] [G loss: 6.381936]\n",
            "16646 [D loss: 0.052182, acc.: 97.27%] [G loss: 5.374903]\n",
            "16647 [D loss: 0.107202, acc.: 95.70%] [G loss: 6.467776]\n",
            "16648 [D loss: 0.117159, acc.: 96.29%] [G loss: 5.600025]\n",
            "16649 [D loss: 0.065382, acc.: 99.02%] [G loss: 6.199458]\n",
            "16650 [D loss: 0.121207, acc.: 95.70%] [G loss: 6.495754]\n",
            "16651 [D loss: 0.185949, acc.: 96.88%] [G loss: 4.707582]\n",
            "16652 [D loss: 0.076353, acc.: 99.61%] [G loss: 8.706175]\n",
            "16653 [D loss: 0.185505, acc.: 95.70%] [G loss: 5.284522]\n",
            "16654 [D loss: 0.041706, acc.: 98.24%] [G loss: 5.793472]\n",
            "16655 [D loss: 0.142482, acc.: 95.90%] [G loss: 8.645048]\n",
            "16656 [D loss: 0.254681, acc.: 96.29%] [G loss: 5.203063]\n",
            "16657 [D loss: 0.123824, acc.: 97.07%] [G loss: 7.773844]\n",
            "16658 [D loss: 0.088136, acc.: 97.85%] [G loss: 6.736082]\n",
            "16659 [D loss: 0.054661, acc.: 99.22%] [G loss: 5.596036]\n",
            "16660 [D loss: 0.072056, acc.: 98.24%] [G loss: 5.984927]\n",
            "16661 [D loss: 0.104396, acc.: 97.27%] [G loss: 5.444293]\n",
            "16662 [D loss: 0.033479, acc.: 99.02%] [G loss: 5.295911]\n",
            "16663 [D loss: 0.146120, acc.: 95.90%] [G loss: 7.828062]\n",
            "16664 [D loss: 0.122635, acc.: 95.90%] [G loss: 5.789995]\n",
            "16665 [D loss: 0.034668, acc.: 99.22%] [G loss: 5.055054]\n",
            "16666 [D loss: 0.278144, acc.: 90.43%] [G loss: 9.949253]\n",
            "16667 [D loss: 0.164388, acc.: 97.85%] [G loss: 12.181668]\n",
            "16668 [D loss: 0.376544, acc.: 95.90%] [G loss: 7.082413]\n",
            "16669 [D loss: 0.531004, acc.: 87.50%] [G loss: 8.521034]\n",
            "16670 [D loss: 0.159377, acc.: 97.46%] [G loss: 8.800759]\n",
            "16671 [D loss: 0.182776, acc.: 96.88%] [G loss: 5.238737]\n",
            "16672 [D loss: 0.081719, acc.: 97.27%] [G loss: 6.120055]\n",
            "16673 [D loss: 0.035154, acc.: 98.83%] [G loss: 5.875833]\n",
            "16674 [D loss: 0.086495, acc.: 97.66%] [G loss: 5.819990]\n",
            "16675 [D loss: 0.071972, acc.: 97.85%] [G loss: 5.202693]\n",
            "16676 [D loss: 0.061259, acc.: 96.68%] [G loss: 6.074434]\n",
            "16677 [D loss: 0.038558, acc.: 99.02%] [G loss: 5.517173]\n",
            "16678 [D loss: 0.054049, acc.: 97.85%] [G loss: 6.992515]\n",
            "16679 [D loss: 0.095899, acc.: 96.88%] [G loss: 6.892641]\n",
            "16680 [D loss: 0.081222, acc.: 98.24%] [G loss: 7.187881]\n",
            "16681 [D loss: 0.103396, acc.: 98.24%] [G loss: 4.607584]\n",
            "16682 [D loss: 0.056405, acc.: 98.83%] [G loss: 5.516685]\n",
            "16683 [D loss: 0.032128, acc.: 99.02%] [G loss: 6.164122]\n",
            "16684 [D loss: 0.032110, acc.: 99.22%] [G loss: 5.302555]\n",
            "16685 [D loss: 0.047032, acc.: 99.02%] [G loss: 5.098185]\n",
            "16686 [D loss: 0.078809, acc.: 97.66%] [G loss: 6.271873]\n",
            "16687 [D loss: 0.119227, acc.: 97.27%] [G loss: 5.530232]\n",
            "16688 [D loss: 0.076384, acc.: 98.44%] [G loss: 5.609344]\n",
            "16689 [D loss: 0.032740, acc.: 98.83%] [G loss: 5.646317]\n",
            "16690 [D loss: 0.088955, acc.: 98.44%] [G loss: 5.907881]\n",
            "16691 [D loss: 0.085333, acc.: 98.05%] [G loss: 5.665707]\n",
            "16692 [D loss: 0.055334, acc.: 98.63%] [G loss: 5.594629]\n",
            "16693 [D loss: 0.072308, acc.: 97.85%] [G loss: 5.988321]\n",
            "16694 [D loss: 0.049770, acc.: 98.83%] [G loss: 5.252836]\n",
            "16695 [D loss: 0.127497, acc.: 97.66%] [G loss: 7.049114]\n",
            "16696 [D loss: 0.089457, acc.: 97.27%] [G loss: 6.701442]\n",
            "16697 [D loss: 0.099781, acc.: 96.88%] [G loss: 5.559490]\n",
            "16698 [D loss: 0.068710, acc.: 98.83%] [G loss: 5.119464]\n",
            "16699 [D loss: 0.028581, acc.: 99.22%] [G loss: 5.279486]\n",
            "16700 [D loss: 0.082626, acc.: 98.24%] [G loss: 6.191169]\n",
            "16701 [D loss: 0.053376, acc.: 98.63%] [G loss: 8.692760]\n",
            "16702 [D loss: 0.073886, acc.: 98.63%] [G loss: 5.198340]\n",
            "16703 [D loss: 0.037387, acc.: 99.22%] [G loss: 7.071520]\n",
            "16704 [D loss: 0.053638, acc.: 98.83%] [G loss: 6.639791]\n",
            "16705 [D loss: 0.099920, acc.: 97.85%] [G loss: 6.086122]\n",
            "16706 [D loss: 0.028419, acc.: 99.61%] [G loss: 6.026757]\n",
            "16707 [D loss: 0.284655, acc.: 87.89%] [G loss: 14.584540]\n",
            "16708 [D loss: 0.645523, acc.: 95.70%] [G loss: 9.566131]\n",
            "16709 [D loss: 0.219685, acc.: 97.27%] [G loss: 9.028296]\n",
            "16710 [D loss: 0.112731, acc.: 97.66%] [G loss: 6.426410]\n",
            "16711 [D loss: 0.115657, acc.: 95.70%] [G loss: 6.858415]\n",
            "16712 [D loss: 0.120548, acc.: 98.05%] [G loss: 6.821998]\n",
            "16713 [D loss: 0.037970, acc.: 99.22%] [G loss: 5.070645]\n",
            "16714 [D loss: 0.108775, acc.: 96.88%] [G loss: 5.547112]\n",
            "16715 [D loss: 0.058307, acc.: 98.05%] [G loss: 5.147084]\n",
            "16716 [D loss: 0.077155, acc.: 98.63%] [G loss: 5.707120]\n",
            "16717 [D loss: 0.023430, acc.: 99.22%] [G loss: 6.535890]\n",
            "16718 [D loss: 0.104815, acc.: 97.66%] [G loss: 4.601933]\n",
            "16719 [D loss: 0.097362, acc.: 97.46%] [G loss: 5.959480]\n",
            "16720 [D loss: 0.099568, acc.: 96.88%] [G loss: 4.946156]\n",
            "16721 [D loss: 0.085041, acc.: 97.66%] [G loss: 5.375260]\n",
            "16722 [D loss: 0.090286, acc.: 97.46%] [G loss: 4.848421]\n",
            "16723 [D loss: 0.098374, acc.: 97.66%] [G loss: 5.687831]\n",
            "16724 [D loss: 0.092964, acc.: 97.85%] [G loss: 4.133228]\n",
            "16725 [D loss: 0.071614, acc.: 98.44%] [G loss: 7.833047]\n",
            "16726 [D loss: 0.118630, acc.: 96.68%] [G loss: 5.821233]\n",
            "16727 [D loss: 0.046047, acc.: 98.05%] [G loss: 4.982704]\n",
            "16728 [D loss: 0.082436, acc.: 97.66%] [G loss: 6.264218]\n",
            "16729 [D loss: 0.074445, acc.: 98.05%] [G loss: 7.458858]\n",
            "16730 [D loss: 0.048075, acc.: 98.63%] [G loss: 5.418978]\n",
            "16731 [D loss: 0.027899, acc.: 99.61%] [G loss: 5.760834]\n",
            "16732 [D loss: 0.035286, acc.: 99.22%] [G loss: 5.608343]\n",
            "16733 [D loss: 0.101067, acc.: 97.46%] [G loss: 7.428767]\n",
            "16734 [D loss: 0.148302, acc.: 96.48%] [G loss: 6.052984]\n",
            "16735 [D loss: 0.169435, acc.: 96.88%] [G loss: 8.442653]\n",
            "16736 [D loss: 0.170845, acc.: 95.31%] [G loss: 6.777681]\n",
            "16737 [D loss: 0.055981, acc.: 99.02%] [G loss: 5.868593]\n",
            "16738 [D loss: 0.099974, acc.: 96.88%] [G loss: 6.044304]\n",
            "16739 [D loss: 0.076233, acc.: 97.85%] [G loss: 6.774131]\n",
            "16740 [D loss: 0.124894, acc.: 97.46%] [G loss: 6.885645]\n",
            "16741 [D loss: 0.050922, acc.: 99.02%] [G loss: 6.949355]\n",
            "16742 [D loss: 0.223061, acc.: 91.99%] [G loss: 5.896335]\n",
            "16743 [D loss: 0.073873, acc.: 98.05%] [G loss: 6.081412]\n",
            "16744 [D loss: 0.087730, acc.: 97.66%] [G loss: 6.088322]\n",
            "16745 [D loss: 0.071433, acc.: 97.66%] [G loss: 5.639503]\n",
            "16746 [D loss: 0.040311, acc.: 98.44%] [G loss: 5.379185]\n",
            "16747 [D loss: 0.053495, acc.: 98.05%] [G loss: 4.327721]\n",
            "16748 [D loss: 0.076294, acc.: 98.63%] [G loss: 5.906188]\n",
            "16749 [D loss: 0.080256, acc.: 98.24%] [G loss: 5.093411]\n",
            "16750 [D loss: 0.107593, acc.: 98.05%] [G loss: 6.016448]\n",
            "16751 [D loss: 0.029495, acc.: 98.44%] [G loss: 5.839134]\n",
            "16752 [D loss: 0.088758, acc.: 97.07%] [G loss: 5.727880]\n",
            "16753 [D loss: 0.046678, acc.: 99.02%] [G loss: 5.269867]\n",
            "16754 [D loss: 0.086579, acc.: 97.85%] [G loss: 6.109830]\n",
            "16755 [D loss: 0.051105, acc.: 99.02%] [G loss: 5.744408]\n",
            "16756 [D loss: 0.081397, acc.: 97.27%] [G loss: 6.527926]\n",
            "16757 [D loss: 0.059778, acc.: 98.63%] [G loss: 5.771559]\n",
            "16758 [D loss: 0.074574, acc.: 98.05%] [G loss: 6.449615]\n",
            "16759 [D loss: 0.128739, acc.: 96.48%] [G loss: 4.720950]\n",
            "16760 [D loss: 0.038910, acc.: 98.83%] [G loss: 4.942497]\n",
            "16761 [D loss: 0.169335, acc.: 96.29%] [G loss: 9.911881]\n",
            "16762 [D loss: 0.071622, acc.: 98.63%] [G loss: 8.337778]\n",
            "16763 [D loss: 0.248017, acc.: 93.75%] [G loss: 4.176851]\n",
            "16764 [D loss: 0.071707, acc.: 97.07%] [G loss: 6.073473]\n",
            "16765 [D loss: 0.080829, acc.: 98.44%] [G loss: 5.762239]\n",
            "16766 [D loss: 0.061070, acc.: 98.63%] [G loss: 5.054256]\n",
            "16767 [D loss: 0.093491, acc.: 97.46%] [G loss: 5.244819]\n",
            "16768 [D loss: 0.097337, acc.: 98.05%] [G loss: 4.799006]\n",
            "16769 [D loss: 0.109574, acc.: 97.27%] [G loss: 5.926373]\n",
            "16770 [D loss: 0.062432, acc.: 98.44%] [G loss: 4.959956]\n",
            "16771 [D loss: 0.094485, acc.: 97.46%] [G loss: 6.369833]\n",
            "16772 [D loss: 0.071335, acc.: 98.24%] [G loss: 5.055358]\n",
            "16773 [D loss: 0.088112, acc.: 96.88%] [G loss: 5.883085]\n",
            "16774 [D loss: 0.081871, acc.: 98.63%] [G loss: 7.366063]\n",
            "16775 [D loss: 0.143433, acc.: 97.66%] [G loss: 5.837748]\n",
            "16776 [D loss: 0.076481, acc.: 96.09%] [G loss: 6.324244]\n",
            "16777 [D loss: 0.109047, acc.: 96.88%] [G loss: 6.747895]\n",
            "16778 [D loss: 0.118272, acc.: 97.07%] [G loss: 6.718859]\n",
            "16779 [D loss: 0.036881, acc.: 98.63%] [G loss: 5.592737]\n",
            "16780 [D loss: 0.060694, acc.: 99.22%] [G loss: 4.593233]\n",
            "16781 [D loss: 0.059375, acc.: 98.63%] [G loss: 4.690904]\n",
            "16782 [D loss: 0.058075, acc.: 98.63%] [G loss: 5.618583]\n",
            "16783 [D loss: 0.069481, acc.: 98.44%] [G loss: 5.194623]\n",
            "16784 [D loss: 0.075260, acc.: 97.85%] [G loss: 5.132404]\n",
            "16785 [D loss: 0.062640, acc.: 98.24%] [G loss: 6.549901]\n",
            "16786 [D loss: 0.068205, acc.: 98.63%] [G loss: 5.925559]\n",
            "16787 [D loss: 0.114347, acc.: 96.88%] [G loss: 5.116447]\n",
            "16788 [D loss: 0.104104, acc.: 96.48%] [G loss: 10.717195]\n",
            "16789 [D loss: 0.241912, acc.: 96.09%] [G loss: 5.097948]\n",
            "16790 [D loss: 0.032784, acc.: 99.02%] [G loss: 4.503196]\n",
            "16791 [D loss: 0.066731, acc.: 98.05%] [G loss: 7.297894]\n",
            "16792 [D loss: 0.180537, acc.: 95.70%] [G loss: 5.012141]\n",
            "16793 [D loss: 0.023086, acc.: 99.61%] [G loss: 6.614105]\n",
            "16794 [D loss: 0.061531, acc.: 98.24%] [G loss: 4.541188]\n",
            "16795 [D loss: 0.111960, acc.: 95.90%] [G loss: 5.278354]\n",
            "16796 [D loss: 0.043417, acc.: 99.02%] [G loss: 6.254117]\n",
            "16797 [D loss: 0.067620, acc.: 98.83%] [G loss: 4.710947]\n",
            "16798 [D loss: 0.107450, acc.: 97.66%] [G loss: 6.707335]\n",
            "16799 [D loss: 0.044684, acc.: 99.02%] [G loss: 6.293890]\n",
            "16800 [D loss: 0.180141, acc.: 96.48%] [G loss: 5.744489]\n",
            "16801 [D loss: 0.067584, acc.: 97.46%] [G loss: 4.497348]\n",
            "16802 [D loss: 0.153864, acc.: 96.88%] [G loss: 7.410900]\n",
            "16803 [D loss: 0.134281, acc.: 96.88%] [G loss: 6.628713]\n",
            "16804 [D loss: 0.123291, acc.: 97.66%] [G loss: 8.010471]\n",
            "16805 [D loss: 0.048175, acc.: 98.24%] [G loss: 5.175321]\n",
            "16806 [D loss: 0.071900, acc.: 98.05%] [G loss: 4.796724]\n",
            "16807 [D loss: 0.044756, acc.: 98.63%] [G loss: 5.795861]\n",
            "16808 [D loss: 0.134521, acc.: 96.88%] [G loss: 6.075236]\n",
            "16809 [D loss: 0.098298, acc.: 97.85%] [G loss: 5.756539]\n",
            "16810 [D loss: 0.033318, acc.: 99.41%] [G loss: 4.683923]\n",
            "16811 [D loss: 0.078125, acc.: 97.85%] [G loss: 4.775505]\n",
            "16812 [D loss: 0.131221, acc.: 96.88%] [G loss: 5.592853]\n",
            "16813 [D loss: 0.074914, acc.: 97.66%] [G loss: 4.772091]\n",
            "16814 [D loss: 0.064735, acc.: 98.24%] [G loss: 4.963603]\n",
            "16815 [D loss: 0.029029, acc.: 99.22%] [G loss: 5.161446]\n",
            "16816 [D loss: 0.082752, acc.: 98.24%] [G loss: 4.457304]\n",
            "16817 [D loss: 0.118610, acc.: 96.09%] [G loss: 7.351949]\n",
            "16818 [D loss: 0.096459, acc.: 97.66%] [G loss: 5.822771]\n",
            "16819 [D loss: 0.066951, acc.: 98.05%] [G loss: 4.570813]\n",
            "16820 [D loss: 0.047616, acc.: 99.02%] [G loss: 4.645617]\n",
            "16821 [D loss: 0.031645, acc.: 99.61%] [G loss: 5.289015]\n",
            "16822 [D loss: 0.047052, acc.: 98.83%] [G loss: 5.064656]\n",
            "16823 [D loss: 0.032834, acc.: 99.41%] [G loss: 6.095280]\n",
            "16824 [D loss: 0.061810, acc.: 98.63%] [G loss: 5.184818]\n",
            "16825 [D loss: 0.062790, acc.: 98.05%] [G loss: 5.190594]\n",
            "16826 [D loss: 0.085421, acc.: 97.66%] [G loss: 5.685417]\n",
            "16827 [D loss: 0.093074, acc.: 97.85%] [G loss: 5.053847]\n",
            "16828 [D loss: 0.094120, acc.: 97.66%] [G loss: 6.652342]\n",
            "16829 [D loss: 0.249397, acc.: 94.92%] [G loss: 7.534736]\n",
            "16830 [D loss: 0.119684, acc.: 96.29%] [G loss: 5.891452]\n",
            "16831 [D loss: 0.101353, acc.: 96.88%] [G loss: 5.151825]\n",
            "16832 [D loss: 0.075210, acc.: 97.07%] [G loss: 5.212173]\n",
            "16833 [D loss: 0.038937, acc.: 99.02%] [G loss: 4.914934]\n",
            "16834 [D loss: 0.137331, acc.: 96.29%] [G loss: 5.296250]\n",
            "16835 [D loss: 0.065266, acc.: 97.85%] [G loss: 4.748006]\n",
            "16836 [D loss: 0.084730, acc.: 97.85%] [G loss: 4.608635]\n",
            "16837 [D loss: 0.039129, acc.: 98.83%] [G loss: 5.273277]\n",
            "16838 [D loss: 0.120073, acc.: 97.07%] [G loss: 5.330961]\n",
            "16839 [D loss: 0.046199, acc.: 98.24%] [G loss: 5.624154]\n",
            "16840 [D loss: 0.133240, acc.: 94.14%] [G loss: 6.062498]\n",
            "16841 [D loss: 0.079993, acc.: 98.24%] [G loss: 5.366759]\n",
            "16842 [D loss: 0.122254, acc.: 96.88%] [G loss: 5.401993]\n",
            "16843 [D loss: 0.050262, acc.: 99.22%] [G loss: 5.751417]\n",
            "16844 [D loss: 0.130609, acc.: 94.53%] [G loss: 5.039433]\n",
            "16845 [D loss: 0.032337, acc.: 99.02%] [G loss: 4.147645]\n",
            "16846 [D loss: 0.079086, acc.: 98.05%] [G loss: 6.907405]\n",
            "16847 [D loss: 0.098246, acc.: 98.24%] [G loss: 8.404984]\n",
            "16848 [D loss: 0.127838, acc.: 95.70%] [G loss: 5.849232]\n",
            "16849 [D loss: 0.087840, acc.: 97.27%] [G loss: 5.028605]\n",
            "16850 [D loss: 0.054231, acc.: 98.83%] [G loss: 5.349000]\n",
            "16851 [D loss: 0.075232, acc.: 97.46%] [G loss: 5.268085]\n",
            "16852 [D loss: 0.066863, acc.: 98.05%] [G loss: 4.208844]\n",
            "16853 [D loss: 0.060242, acc.: 98.83%] [G loss: 6.437489]\n",
            "16854 [D loss: 0.093548, acc.: 97.46%] [G loss: 5.518976]\n",
            "16855 [D loss: 0.036103, acc.: 99.22%] [G loss: 5.266202]\n",
            "16856 [D loss: 0.136524, acc.: 96.88%] [G loss: 5.042586]\n",
            "16857 [D loss: 0.062438, acc.: 97.46%] [G loss: 4.754410]\n",
            "16858 [D loss: 0.064518, acc.: 98.83%] [G loss: 4.591650]\n",
            "16859 [D loss: 0.061285, acc.: 98.44%] [G loss: 4.796689]\n",
            "16860 [D loss: 0.111315, acc.: 95.51%] [G loss: 6.197512]\n",
            "16861 [D loss: 0.137203, acc.: 97.27%] [G loss: 4.106738]\n",
            "16862 [D loss: 0.089389, acc.: 97.46%] [G loss: 6.425238]\n",
            "16863 [D loss: 0.146977, acc.: 95.51%] [G loss: 4.961743]\n",
            "16864 [D loss: 0.102349, acc.: 97.46%] [G loss: 8.581332]\n",
            "16865 [D loss: 0.220763, acc.: 96.09%] [G loss: 5.612844]\n",
            "16866 [D loss: 0.046167, acc.: 98.05%] [G loss: 5.500306]\n",
            "16867 [D loss: 0.061025, acc.: 97.85%] [G loss: 5.170248]\n",
            "16868 [D loss: 0.126029, acc.: 96.88%] [G loss: 6.381585]\n",
            "16869 [D loss: 0.086013, acc.: 96.68%] [G loss: 5.123123]\n",
            "16870 [D loss: 0.067378, acc.: 97.27%] [G loss: 5.452600]\n",
            "16871 [D loss: 0.073276, acc.: 98.05%] [G loss: 5.521161]\n",
            "16872 [D loss: 0.075888, acc.: 96.88%] [G loss: 6.863236]\n",
            "16873 [D loss: 0.143605, acc.: 95.51%] [G loss: 4.953422]\n",
            "16874 [D loss: 0.053903, acc.: 98.24%] [G loss: 4.186948]\n",
            "16875 [D loss: 0.067348, acc.: 98.44%] [G loss: 6.743334]\n",
            "16876 [D loss: 0.076659, acc.: 97.27%] [G loss: 5.242399]\n",
            "16877 [D loss: 0.084760, acc.: 98.05%] [G loss: 5.417159]\n",
            "16878 [D loss: 0.039631, acc.: 99.02%] [G loss: 5.169130]\n",
            "16879 [D loss: 0.187531, acc.: 96.68%] [G loss: 5.842193]\n",
            "16880 [D loss: 0.093833, acc.: 97.66%] [G loss: 5.526144]\n",
            "16881 [D loss: 0.116060, acc.: 96.88%] [G loss: 5.098407]\n",
            "16882 [D loss: 0.028516, acc.: 99.02%] [G loss: 5.904251]\n",
            "16883 [D loss: 0.064725, acc.: 98.24%] [G loss: 4.691585]\n",
            "16884 [D loss: 0.074275, acc.: 97.66%] [G loss: 5.094493]\n",
            "16885 [D loss: 0.062071, acc.: 98.44%] [G loss: 5.053665]\n",
            "16886 [D loss: 0.044023, acc.: 98.83%] [G loss: 5.540911]\n",
            "16887 [D loss: 0.077372, acc.: 98.05%] [G loss: 5.125409]\n",
            "16888 [D loss: 0.116079, acc.: 97.46%] [G loss: 10.467293]\n",
            "16889 [D loss: 0.271285, acc.: 94.53%] [G loss: 5.432002]\n",
            "16890 [D loss: 0.120735, acc.: 96.88%] [G loss: 7.436415]\n",
            "16891 [D loss: 0.045109, acc.: 98.63%] [G loss: 5.667748]\n",
            "16892 [D loss: 0.183404, acc.: 95.31%] [G loss: 6.135470]\n",
            "16893 [D loss: 0.057271, acc.: 98.63%] [G loss: 7.857004]\n",
            "16894 [D loss: 0.203526, acc.: 96.48%] [G loss: 4.871657]\n",
            "16895 [D loss: 0.059557, acc.: 97.66%] [G loss: 5.014603]\n",
            "16896 [D loss: 0.085956, acc.: 97.66%] [G loss: 4.745102]\n",
            "16897 [D loss: 0.054263, acc.: 98.83%] [G loss: 4.594408]\n",
            "16898 [D loss: 0.090261, acc.: 97.66%] [G loss: 5.201621]\n",
            "16899 [D loss: 0.098707, acc.: 97.85%] [G loss: 5.241302]\n",
            "16900 [D loss: 0.074076, acc.: 97.66%] [G loss: 4.660901]\n",
            "16901 [D loss: 0.050719, acc.: 98.44%] [G loss: 5.308965]\n",
            "16902 [D loss: 0.074442, acc.: 98.44%] [G loss: 4.842078]\n",
            "16903 [D loss: 0.084039, acc.: 96.09%] [G loss: 5.711980]\n",
            "16904 [D loss: 0.074639, acc.: 98.24%] [G loss: 5.194267]\n",
            "16905 [D loss: 0.054882, acc.: 98.63%] [G loss: 5.127495]\n",
            "16906 [D loss: 0.033458, acc.: 99.22%] [G loss: 5.265247]\n",
            "16907 [D loss: 0.138530, acc.: 96.68%] [G loss: 6.628634]\n",
            "16908 [D loss: 0.140409, acc.: 95.51%] [G loss: 5.469506]\n",
            "16909 [D loss: 0.095591, acc.: 97.66%] [G loss: 4.903660]\n",
            "16910 [D loss: 0.071731, acc.: 97.85%] [G loss: 4.615497]\n",
            "16911 [D loss: 0.128946, acc.: 96.48%] [G loss: 7.498233]\n",
            "16912 [D loss: 0.057637, acc.: 97.46%] [G loss: 6.872370]\n",
            "16913 [D loss: 0.071323, acc.: 97.85%] [G loss: 4.672458]\n",
            "16914 [D loss: 0.039254, acc.: 99.02%] [G loss: 4.890077]\n",
            "16915 [D loss: 0.127945, acc.: 96.48%] [G loss: 6.015689]\n",
            "16916 [D loss: 0.096037, acc.: 97.85%] [G loss: 5.114285]\n",
            "16917 [D loss: 0.087909, acc.: 97.85%] [G loss: 6.130347]\n",
            "16918 [D loss: 0.077592, acc.: 98.05%] [G loss: 5.566251]\n",
            "16919 [D loss: 0.075552, acc.: 98.44%] [G loss: 5.504240]\n",
            "16920 [D loss: 0.095535, acc.: 97.46%] [G loss: 4.977499]\n",
            "16921 [D loss: 0.067469, acc.: 97.85%] [G loss: 5.198376]\n",
            "16922 [D loss: 0.066207, acc.: 97.66%] [G loss: 5.116370]\n",
            "16923 [D loss: 0.051377, acc.: 98.05%] [G loss: 5.645242]\n",
            "16924 [D loss: 0.097951, acc.: 98.24%] [G loss: 5.283477]\n",
            "16925 [D loss: 0.053090, acc.: 98.83%] [G loss: 4.593249]\n",
            "16926 [D loss: 0.072275, acc.: 98.44%] [G loss: 4.619048]\n",
            "16927 [D loss: 0.073415, acc.: 98.05%] [G loss: 4.930883]\n",
            "16928 [D loss: 0.056297, acc.: 98.83%] [G loss: 5.444316]\n",
            "16929 [D loss: 0.058232, acc.: 98.83%] [G loss: 4.649679]\n",
            "16930 [D loss: 0.058983, acc.: 98.05%] [G loss: 5.257773]\n",
            "16931 [D loss: 0.040099, acc.: 99.02%] [G loss: 5.731098]\n",
            "16932 [D loss: 0.093212, acc.: 97.85%] [G loss: 4.908356]\n",
            "16933 [D loss: 0.106603, acc.: 97.27%] [G loss: 5.531524]\n",
            "16934 [D loss: 0.095203, acc.: 97.66%] [G loss: 5.380535]\n",
            "16935 [D loss: 0.099180, acc.: 98.24%] [G loss: 4.600352]\n",
            "16936 [D loss: 0.139207, acc.: 95.70%] [G loss: 8.004350]\n",
            "16937 [D loss: 0.215533, acc.: 94.92%] [G loss: 6.746981]\n",
            "16938 [D loss: 0.119010, acc.: 97.66%] [G loss: 5.114815]\n",
            "16939 [D loss: 0.057830, acc.: 98.44%] [G loss: 5.445302]\n",
            "16940 [D loss: 0.078547, acc.: 97.46%] [G loss: 4.978374]\n",
            "16941 [D loss: 0.057912, acc.: 98.44%] [G loss: 4.816443]\n",
            "16942 [D loss: 0.106708, acc.: 96.48%] [G loss: 5.462368]\n",
            "16943 [D loss: 0.070135, acc.: 98.44%] [G loss: 5.101167]\n",
            "16944 [D loss: 0.054372, acc.: 98.83%] [G loss: 5.471379]\n",
            "16945 [D loss: 0.209696, acc.: 95.51%] [G loss: 6.675117]\n",
            "16946 [D loss: 0.121072, acc.: 96.68%] [G loss: 6.147835]\n",
            "16947 [D loss: 0.144884, acc.: 97.07%] [G loss: 6.368227]\n",
            "16948 [D loss: 0.172632, acc.: 96.09%] [G loss: 13.893433]\n",
            "16949 [D loss: 0.385354, acc.: 95.70%] [G loss: 6.442583]\n",
            "16950 [D loss: 0.206547, acc.: 96.68%] [G loss: 6.332011]\n",
            "16951 [D loss: 0.038332, acc.: 98.83%] [G loss: 12.751896]\n",
            "16952 [D loss: 0.245342, acc.: 90.62%] [G loss: 9.562109]\n",
            "16953 [D loss: 0.250937, acc.: 96.68%] [G loss: 12.270533]\n",
            "16954 [D loss: 0.308857, acc.: 96.68%] [G loss: 7.249492]\n",
            "16955 [D loss: 0.193542, acc.: 95.31%] [G loss: 5.172893]\n",
            "16956 [D loss: 0.097565, acc.: 96.48%] [G loss: 5.658716]\n",
            "16957 [D loss: 0.088980, acc.: 95.90%] [G loss: 6.060078]\n",
            "16958 [D loss: 0.152813, acc.: 94.14%] [G loss: 5.863555]\n",
            "16959 [D loss: 0.078366, acc.: 98.05%] [G loss: 5.277139]\n",
            "16960 [D loss: 0.176849, acc.: 94.34%] [G loss: 6.592700]\n",
            "16961 [D loss: 0.057448, acc.: 98.63%] [G loss: 5.511262]\n",
            "16962 [D loss: 0.095751, acc.: 98.24%] [G loss: 5.392432]\n",
            "16963 [D loss: 0.119326, acc.: 96.68%] [G loss: 6.902884]\n",
            "16964 [D loss: 0.082557, acc.: 97.85%] [G loss: 6.703202]\n",
            "16965 [D loss: 0.182813, acc.: 96.29%] [G loss: 5.386462]\n",
            "16966 [D loss: 0.095900, acc.: 97.66%] [G loss: 4.505953]\n",
            "16967 [D loss: 0.049636, acc.: 98.63%] [G loss: 6.386938]\n",
            "16968 [D loss: 0.157459, acc.: 96.48%] [G loss: 5.574885]\n",
            "16969 [D loss: 0.116375, acc.: 97.27%] [G loss: 5.195981]\n",
            "16970 [D loss: 0.059475, acc.: 98.24%] [G loss: 4.647656]\n",
            "16971 [D loss: 0.093018, acc.: 96.48%] [G loss: 5.826456]\n",
            "16972 [D loss: 0.056499, acc.: 97.46%] [G loss: 4.677360]\n",
            "16973 [D loss: 0.046371, acc.: 99.02%] [G loss: 5.011931]\n",
            "16974 [D loss: 0.037027, acc.: 98.83%] [G loss: 4.832603]\n",
            "16975 [D loss: 0.106345, acc.: 97.07%] [G loss: 5.068450]\n",
            "16976 [D loss: 0.107898, acc.: 96.88%] [G loss: 4.320072]\n",
            "16977 [D loss: 0.055824, acc.: 98.63%] [G loss: 4.548087]\n",
            "16978 [D loss: 0.067325, acc.: 98.24%] [G loss: 4.636240]\n",
            "16979 [D loss: 0.072036, acc.: 97.85%] [G loss: 4.739323]\n",
            "16980 [D loss: 0.052279, acc.: 99.02%] [G loss: 5.265753]\n",
            "16981 [D loss: 0.047335, acc.: 99.02%] [G loss: 4.790578]\n",
            "16982 [D loss: 0.084951, acc.: 96.29%] [G loss: 5.939085]\n",
            "16983 [D loss: 0.131671, acc.: 96.09%] [G loss: 5.125069]\n",
            "16984 [D loss: 0.059901, acc.: 98.63%] [G loss: 5.495254]\n",
            "16985 [D loss: 0.171831, acc.: 94.53%] [G loss: 6.537350]\n",
            "16986 [D loss: 0.063094, acc.: 98.24%] [G loss: 6.252411]\n",
            "16987 [D loss: 0.027921, acc.: 99.02%] [G loss: 5.676349]\n",
            "16988 [D loss: 0.071716, acc.: 98.05%] [G loss: 5.212913]\n",
            "16989 [D loss: 0.070650, acc.: 98.44%] [G loss: 4.658279]\n",
            "16990 [D loss: 0.097654, acc.: 96.68%] [G loss: 5.147880]\n",
            "16991 [D loss: 0.052351, acc.: 98.63%] [G loss: 5.033250]\n",
            "16992 [D loss: 0.074790, acc.: 98.44%] [G loss: 4.583514]\n",
            "16993 [D loss: 0.046419, acc.: 98.63%] [G loss: 5.308843]\n",
            "16994 [D loss: 0.078893, acc.: 97.27%] [G loss: 4.551576]\n",
            "16995 [D loss: 0.068627, acc.: 98.05%] [G loss: 4.522500]\n",
            "16996 [D loss: 0.086751, acc.: 97.85%] [G loss: 4.445366]\n",
            "16997 [D loss: 0.095214, acc.: 95.90%] [G loss: 4.862893]\n",
            "16998 [D loss: 0.061932, acc.: 97.85%] [G loss: 5.024500]\n",
            "16999 [D loss: 0.068111, acc.: 97.85%] [G loss: 5.554963]\n",
            "17000 [D loss: 0.102360, acc.: 95.70%] [G loss: 6.229193]\n",
            "17001 [D loss: 0.121519, acc.: 96.09%] [G loss: 5.794078]\n",
            "17002 [D loss: 0.083237, acc.: 97.46%] [G loss: 4.656128]\n",
            "17003 [D loss: 0.081565, acc.: 97.66%] [G loss: 5.179890]\n",
            "17004 [D loss: 0.100349, acc.: 97.46%] [G loss: 4.817200]\n",
            "17005 [D loss: 0.041913, acc.: 98.83%] [G loss: 5.144566]\n",
            "17006 [D loss: 0.081783, acc.: 98.24%] [G loss: 4.466475]\n",
            "17007 [D loss: 0.159976, acc.: 95.31%] [G loss: 5.969475]\n",
            "17008 [D loss: 0.061715, acc.: 97.85%] [G loss: 6.142737]\n",
            "17009 [D loss: 0.097166, acc.: 97.66%] [G loss: 7.200414]\n",
            "17010 [D loss: 0.131700, acc.: 95.51%] [G loss: 4.301161]\n",
            "17011 [D loss: 0.125114, acc.: 93.95%] [G loss: 5.272244]\n",
            "17012 [D loss: 0.045059, acc.: 98.44%] [G loss: 5.415040]\n",
            "17013 [D loss: 0.081095, acc.: 97.07%] [G loss: 4.595059]\n",
            "17014 [D loss: 0.082639, acc.: 97.66%] [G loss: 5.625726]\n",
            "17015 [D loss: 0.110236, acc.: 97.27%] [G loss: 4.804289]\n",
            "17016 [D loss: 0.213693, acc.: 93.95%] [G loss: 6.417695]\n",
            "17017 [D loss: 0.095912, acc.: 97.07%] [G loss: 7.626248]\n",
            "17018 [D loss: 0.150562, acc.: 97.27%] [G loss: 5.655603]\n",
            "17019 [D loss: 0.099029, acc.: 96.29%] [G loss: 5.842833]\n",
            "17020 [D loss: 0.104765, acc.: 97.07%] [G loss: 5.670175]\n",
            "17021 [D loss: 0.078512, acc.: 97.07%] [G loss: 5.482692]\n",
            "17022 [D loss: 0.143589, acc.: 94.53%] [G loss: 7.570913]\n",
            "17023 [D loss: 0.118035, acc.: 97.46%] [G loss: 4.647141]\n",
            "17024 [D loss: 0.133345, acc.: 97.27%] [G loss: 6.307505]\n",
            "17025 [D loss: 0.026646, acc.: 98.63%] [G loss: 7.904486]\n",
            "17026 [D loss: 0.059831, acc.: 97.07%] [G loss: 4.932506]\n",
            "17027 [D loss: 0.036795, acc.: 99.22%] [G loss: 6.041737]\n",
            "17028 [D loss: 0.075408, acc.: 97.27%] [G loss: 5.825479]\n",
            "17029 [D loss: 0.047641, acc.: 98.44%] [G loss: 4.871302]\n",
            "17030 [D loss: 0.090169, acc.: 96.68%] [G loss: 5.323709]\n",
            "17031 [D loss: 0.126975, acc.: 95.70%] [G loss: 7.117608]\n",
            "17032 [D loss: 0.093806, acc.: 97.27%] [G loss: 4.551818]\n",
            "17033 [D loss: 0.066987, acc.: 98.63%] [G loss: 5.933471]\n",
            "17034 [D loss: 0.064446, acc.: 97.66%] [G loss: 4.348513]\n",
            "17035 [D loss: 0.067297, acc.: 98.05%] [G loss: 5.231368]\n",
            "17036 [D loss: 0.045156, acc.: 98.83%] [G loss: 6.372828]\n",
            "17037 [D loss: 0.056220, acc.: 98.63%] [G loss: 4.477959]\n",
            "17038 [D loss: 0.095118, acc.: 95.70%] [G loss: 5.261866]\n",
            "17039 [D loss: 0.095483, acc.: 98.05%] [G loss: 4.753755]\n",
            "17040 [D loss: 0.147763, acc.: 95.90%] [G loss: 5.883894]\n",
            "17041 [D loss: 0.154385, acc.: 95.70%] [G loss: 5.650752]\n",
            "17042 [D loss: 0.044991, acc.: 98.83%] [G loss: 4.697347]\n",
            "17043 [D loss: 0.170479, acc.: 95.51%] [G loss: 6.963743]\n",
            "17044 [D loss: 0.084683, acc.: 97.85%] [G loss: 6.555706]\n",
            "17045 [D loss: 0.102024, acc.: 96.88%] [G loss: 5.203616]\n",
            "17046 [D loss: 0.091480, acc.: 97.46%] [G loss: 5.151962]\n",
            "17047 [D loss: 0.065243, acc.: 98.24%] [G loss: 5.136741]\n",
            "17048 [D loss: 0.089321, acc.: 96.48%] [G loss: 6.148202]\n",
            "17049 [D loss: 0.055889, acc.: 98.83%] [G loss: 5.400921]\n",
            "17050 [D loss: 0.161075, acc.: 97.07%] [G loss: 6.174543]\n",
            "17051 [D loss: 0.182047, acc.: 96.68%] [G loss: 7.570363]\n",
            "17052 [D loss: 0.163133, acc.: 97.66%] [G loss: 6.891161]\n",
            "17053 [D loss: 0.165083, acc.: 96.29%] [G loss: 5.892431]\n",
            "17054 [D loss: 0.068652, acc.: 97.46%] [G loss: 4.411328]\n",
            "17055 [D loss: 0.082582, acc.: 96.48%] [G loss: 4.968806]\n",
            "17056 [D loss: 0.070154, acc.: 97.66%] [G loss: 4.699080]\n",
            "17057 [D loss: 0.167059, acc.: 93.16%] [G loss: 4.985407]\n",
            "17058 [D loss: 0.036542, acc.: 99.22%] [G loss: 5.442395]\n",
            "17059 [D loss: 0.076321, acc.: 98.63%] [G loss: 5.041575]\n",
            "17060 [D loss: 0.099299, acc.: 97.46%] [G loss: 6.253047]\n",
            "17061 [D loss: 0.093019, acc.: 97.85%] [G loss: 5.075813]\n",
            "17062 [D loss: 0.071510, acc.: 97.27%] [G loss: 6.184340]\n",
            "17063 [D loss: 0.081881, acc.: 98.05%] [G loss: 5.254331]\n",
            "17064 [D loss: 0.081156, acc.: 98.05%] [G loss: 5.449902]\n",
            "17065 [D loss: 0.099804, acc.: 96.48%] [G loss: 6.697278]\n",
            "17066 [D loss: 0.307747, acc.: 91.21%] [G loss: 8.859511]\n",
            "17067 [D loss: 0.197641, acc.: 97.27%] [G loss: 8.117557]\n",
            "17068 [D loss: 0.212932, acc.: 96.48%] [G loss: 5.501252]\n",
            "17069 [D loss: 0.099510, acc.: 97.85%] [G loss: 4.688285]\n",
            "17070 [D loss: 0.079949, acc.: 98.05%] [G loss: 7.304355]\n",
            "17071 [D loss: 0.091521, acc.: 97.27%] [G loss: 4.597169]\n",
            "17072 [D loss: 0.071378, acc.: 97.66%] [G loss: 5.897486]\n",
            "17073 [D loss: 0.052893, acc.: 97.46%] [G loss: 4.282215]\n",
            "17074 [D loss: 0.084916, acc.: 97.07%] [G loss: 4.257673]\n",
            "17075 [D loss: 0.068245, acc.: 98.05%] [G loss: 4.468898]\n",
            "17076 [D loss: 0.072046, acc.: 98.44%] [G loss: 4.770658]\n",
            "17077 [D loss: 0.055113, acc.: 98.05%] [G loss: 6.486792]\n",
            "17078 [D loss: 0.098549, acc.: 97.46%] [G loss: 4.976402]\n",
            "17079 [D loss: 0.100972, acc.: 97.66%] [G loss: 5.472507]\n",
            "17080 [D loss: 0.050404, acc.: 98.63%] [G loss: 4.397213]\n",
            "17081 [D loss: 0.081528, acc.: 97.85%] [G loss: 5.060469]\n",
            "17082 [D loss: 0.112124, acc.: 96.88%] [G loss: 4.851284]\n",
            "17083 [D loss: 0.047477, acc.: 98.44%] [G loss: 4.640591]\n",
            "17084 [D loss: 0.062876, acc.: 98.05%] [G loss: 3.975654]\n",
            "17085 [D loss: 0.087872, acc.: 96.29%] [G loss: 6.213748]\n",
            "17086 [D loss: 0.062971, acc.: 98.05%] [G loss: 5.073359]\n",
            "17087 [D loss: 0.147404, acc.: 96.48%] [G loss: 5.583288]\n",
            "17088 [D loss: 0.043830, acc.: 98.63%] [G loss: 4.507514]\n",
            "17089 [D loss: 0.113235, acc.: 96.68%] [G loss: 5.886652]\n",
            "17090 [D loss: 0.111604, acc.: 96.48%] [G loss: 5.418829]\n",
            "17091 [D loss: 0.062260, acc.: 98.44%] [G loss: 5.098510]\n",
            "17092 [D loss: 0.126323, acc.: 97.46%] [G loss: 4.360625]\n",
            "17093 [D loss: 0.101349, acc.: 95.70%] [G loss: 5.307926]\n",
            "17094 [D loss: 0.051474, acc.: 98.24%] [G loss: 5.645810]\n",
            "17095 [D loss: 0.195831, acc.: 93.16%] [G loss: 4.398418]\n",
            "17096 [D loss: 0.074732, acc.: 96.88%] [G loss: 5.765867]\n",
            "17097 [D loss: 0.091607, acc.: 96.09%] [G loss: 4.598258]\n",
            "17098 [D loss: 0.063994, acc.: 98.63%] [G loss: 5.607844]\n",
            "17099 [D loss: 0.066974, acc.: 98.05%] [G loss: 5.249632]\n",
            "17100 [D loss: 0.068382, acc.: 98.44%] [G loss: 5.054831]\n",
            "17101 [D loss: 0.073569, acc.: 97.66%] [G loss: 4.913001]\n",
            "17102 [D loss: 0.054462, acc.: 97.66%] [G loss: 5.802225]\n",
            "17103 [D loss: 0.092052, acc.: 97.85%] [G loss: 4.981317]\n",
            "17104 [D loss: 0.096667, acc.: 97.66%] [G loss: 5.423688]\n",
            "17105 [D loss: 0.098435, acc.: 97.07%] [G loss: 4.869689]\n",
            "17106 [D loss: 0.116062, acc.: 97.46%] [G loss: 5.824962]\n",
            "17107 [D loss: 0.130897, acc.: 95.12%] [G loss: 6.647323]\n",
            "17108 [D loss: 0.139008, acc.: 96.29%] [G loss: 5.250131]\n",
            "17109 [D loss: 0.081207, acc.: 97.27%] [G loss: 5.577276]\n",
            "17110 [D loss: 0.100985, acc.: 97.27%] [G loss: 6.186486]\n",
            "17111 [D loss: 0.194703, acc.: 94.53%] [G loss: 6.672490]\n",
            "17112 [D loss: 0.073453, acc.: 97.27%] [G loss: 5.707755]\n",
            "17113 [D loss: 0.100283, acc.: 98.05%] [G loss: 4.522180]\n",
            "17114 [D loss: 0.090893, acc.: 95.90%] [G loss: 6.687425]\n",
            "17115 [D loss: 0.131033, acc.: 94.14%] [G loss: 6.095653]\n",
            "17116 [D loss: 0.112334, acc.: 96.68%] [G loss: 5.351411]\n",
            "17117 [D loss: 0.085761, acc.: 95.51%] [G loss: 5.626518]\n",
            "17118 [D loss: 0.103405, acc.: 97.85%] [G loss: 5.252445]\n",
            "17119 [D loss: 0.108590, acc.: 96.48%] [G loss: 7.344033]\n",
            "17120 [D loss: 0.138098, acc.: 95.51%] [G loss: 4.758576]\n",
            "17121 [D loss: 0.124964, acc.: 94.92%] [G loss: 6.088859]\n",
            "17122 [D loss: 0.077980, acc.: 97.07%] [G loss: 6.814996]\n",
            "17123 [D loss: 0.069773, acc.: 98.05%] [G loss: 6.261835]\n",
            "17124 [D loss: 0.198500, acc.: 96.29%] [G loss: 5.631986]\n",
            "17125 [D loss: 0.056287, acc.: 97.46%] [G loss: 5.849425]\n",
            "17126 [D loss: 0.076187, acc.: 98.83%] [G loss: 7.575989]\n",
            "17127 [D loss: 0.201528, acc.: 96.68%] [G loss: 7.230102]\n",
            "17128 [D loss: 0.134800, acc.: 95.70%] [G loss: 7.752784]\n",
            "17129 [D loss: 0.065476, acc.: 98.05%] [G loss: 5.777220]\n",
            "17130 [D loss: 0.050913, acc.: 98.44%] [G loss: 4.010894]\n",
            "17131 [D loss: 0.075405, acc.: 97.27%] [G loss: 5.438726]\n",
            "17132 [D loss: 0.069556, acc.: 97.66%] [G loss: 4.439420]\n",
            "17133 [D loss: 0.081065, acc.: 97.46%] [G loss: 5.102687]\n",
            "17134 [D loss: 0.035425, acc.: 98.83%] [G loss: 6.093789]\n",
            "17135 [D loss: 0.055948, acc.: 98.05%] [G loss: 4.343468]\n",
            "17136 [D loss: 0.079070, acc.: 96.88%] [G loss: 4.109704]\n",
            "17137 [D loss: 0.104617, acc.: 97.27%] [G loss: 5.565990]\n",
            "17138 [D loss: 0.082271, acc.: 95.51%] [G loss: 7.281137]\n",
            "17139 [D loss: 0.048469, acc.: 98.44%] [G loss: 6.270576]\n",
            "17140 [D loss: 0.111669, acc.: 96.48%] [G loss: 5.374632]\n",
            "17141 [D loss: 0.050219, acc.: 98.44%] [G loss: 5.493775]\n",
            "17142 [D loss: 0.157281, acc.: 95.31%] [G loss: 4.603324]\n",
            "17143 [D loss: 0.080216, acc.: 96.88%] [G loss: 6.996556]\n",
            "17144 [D loss: 0.189142, acc.: 95.70%] [G loss: 4.305854]\n",
            "17145 [D loss: 0.050880, acc.: 98.05%] [G loss: 7.181924]\n",
            "17146 [D loss: 0.171270, acc.: 95.12%] [G loss: 4.817124]\n",
            "17147 [D loss: 0.059306, acc.: 98.05%] [G loss: 5.383214]\n",
            "17148 [D loss: 0.151006, acc.: 95.70%] [G loss: 5.003469]\n",
            "17149 [D loss: 0.061155, acc.: 97.46%] [G loss: 4.725839]\n",
            "17150 [D loss: 0.113443, acc.: 96.48%] [G loss: 5.507080]\n",
            "17151 [D loss: 0.092197, acc.: 96.48%] [G loss: 4.694913]\n",
            "17152 [D loss: 0.082003, acc.: 97.85%] [G loss: 4.759857]\n",
            "17153 [D loss: 0.070027, acc.: 98.05%] [G loss: 5.142132]\n",
            "17154 [D loss: 0.050130, acc.: 98.05%] [G loss: 4.537025]\n",
            "17155 [D loss: 0.091075, acc.: 97.07%] [G loss: 4.518806]\n",
            "17156 [D loss: 0.064829, acc.: 97.46%] [G loss: 4.509839]\n",
            "17157 [D loss: 0.087515, acc.: 97.27%] [G loss: 4.817986]\n",
            "17158 [D loss: 0.099736, acc.: 97.07%] [G loss: 4.767906]\n",
            "17159 [D loss: 0.135271, acc.: 94.92%] [G loss: 5.353090]\n",
            "17160 [D loss: 0.100151, acc.: 97.07%] [G loss: 4.241140]\n",
            "17161 [D loss: 0.050353, acc.: 98.05%] [G loss: 5.229582]\n",
            "17162 [D loss: 0.050870, acc.: 98.63%] [G loss: 4.675543]\n",
            "17163 [D loss: 0.147722, acc.: 95.90%] [G loss: 5.727435]\n",
            "17164 [D loss: 0.100972, acc.: 97.27%] [G loss: 4.693164]\n",
            "17165 [D loss: 0.165870, acc.: 93.55%] [G loss: 5.255723]\n",
            "17166 [D loss: 0.124981, acc.: 97.07%] [G loss: 6.090542]\n",
            "17167 [D loss: 0.102596, acc.: 96.29%] [G loss: 5.710512]\n",
            "17168 [D loss: 0.069162, acc.: 97.66%] [G loss: 4.857670]\n",
            "17169 [D loss: 0.072262, acc.: 97.66%] [G loss: 4.773262]\n",
            "17170 [D loss: 0.222287, acc.: 93.95%] [G loss: 8.074647]\n",
            "17171 [D loss: 0.096524, acc.: 96.68%] [G loss: 5.593840]\n",
            "17172 [D loss: 0.062461, acc.: 98.05%] [G loss: 4.850299]\n",
            "17173 [D loss: 0.043530, acc.: 99.02%] [G loss: 4.612621]\n",
            "17174 [D loss: 0.058943, acc.: 98.44%] [G loss: 6.447871]\n",
            "17175 [D loss: 0.049742, acc.: 98.63%] [G loss: 5.825709]\n",
            "17176 [D loss: 0.051358, acc.: 98.05%] [G loss: 5.059171]\n",
            "17177 [D loss: 0.048840, acc.: 98.63%] [G loss: 5.358252]\n",
            "17178 [D loss: 0.062349, acc.: 98.44%] [G loss: 4.273272]\n",
            "17179 [D loss: 0.089443, acc.: 97.66%] [G loss: 4.342082]\n",
            "17180 [D loss: 0.127018, acc.: 96.09%] [G loss: 5.504780]\n",
            "17181 [D loss: 0.064391, acc.: 97.85%] [G loss: 6.725601]\n",
            "17182 [D loss: 0.046481, acc.: 98.44%] [G loss: 4.691301]\n",
            "17183 [D loss: 0.060099, acc.: 98.83%] [G loss: 5.354200]\n",
            "17184 [D loss: 0.068798, acc.: 97.85%] [G loss: 6.330406]\n",
            "17185 [D loss: 0.057563, acc.: 98.63%] [G loss: 5.151066]\n",
            "17186 [D loss: 0.076928, acc.: 97.85%] [G loss: 4.807322]\n",
            "17187 [D loss: 0.067321, acc.: 97.85%] [G loss: 4.910011]\n",
            "17188 [D loss: 0.053166, acc.: 98.63%] [G loss: 5.048003]\n",
            "17189 [D loss: 0.049571, acc.: 98.44%] [G loss: 6.154501]\n",
            "17190 [D loss: 0.046387, acc.: 98.05%] [G loss: 5.306218]\n",
            "17191 [D loss: 0.085278, acc.: 96.88%] [G loss: 5.081369]\n",
            "17192 [D loss: 0.077339, acc.: 97.27%] [G loss: 4.774915]\n",
            "17193 [D loss: 0.088924, acc.: 97.27%] [G loss: 4.674937]\n",
            "17194 [D loss: 0.096238, acc.: 96.68%] [G loss: 5.877096]\n",
            "17195 [D loss: 0.078093, acc.: 97.66%] [G loss: 5.460895]\n",
            "17196 [D loss: 0.055233, acc.: 98.63%] [G loss: 5.245340]\n",
            "17197 [D loss: 0.079771, acc.: 98.05%] [G loss: 6.477908]\n",
            "17198 [D loss: 0.111579, acc.: 96.09%] [G loss: 5.966746]\n",
            "17199 [D loss: 0.257553, acc.: 94.92%] [G loss: 7.106222]\n",
            "17200 [D loss: 0.071843, acc.: 97.07%] [G loss: 5.571996]\n",
            "17201 [D loss: 0.082706, acc.: 98.05%] [G loss: 6.300653]\n",
            "17202 [D loss: 0.132820, acc.: 97.46%] [G loss: 6.232953]\n",
            "17203 [D loss: 0.260446, acc.: 94.34%] [G loss: 7.030269]\n",
            "17204 [D loss: 0.141406, acc.: 95.51%] [G loss: 6.837400]\n",
            "17205 [D loss: 0.177409, acc.: 96.29%] [G loss: 4.423807]\n",
            "17206 [D loss: 0.078040, acc.: 97.46%] [G loss: 5.288699]\n",
            "17207 [D loss: 0.078016, acc.: 97.66%] [G loss: 5.194849]\n",
            "17208 [D loss: 0.100053, acc.: 96.88%] [G loss: 4.905866]\n",
            "17209 [D loss: 0.110414, acc.: 97.07%] [G loss: 4.466851]\n",
            "17210 [D loss: 0.083763, acc.: 96.68%] [G loss: 6.082796]\n",
            "17211 [D loss: 0.090176, acc.: 96.68%] [G loss: 5.213877]\n",
            "17212 [D loss: 0.106534, acc.: 96.68%] [G loss: 4.577250]\n",
            "17213 [D loss: 0.077351, acc.: 97.07%] [G loss: 5.537175]\n",
            "17214 [D loss: 0.100993, acc.: 95.51%] [G loss: 5.029076]\n",
            "17215 [D loss: 0.105351, acc.: 97.46%] [G loss: 4.694561]\n",
            "17216 [D loss: 0.094690, acc.: 97.66%] [G loss: 4.410680]\n",
            "17217 [D loss: 0.147822, acc.: 95.12%] [G loss: 4.202801]\n",
            "17218 [D loss: 0.042973, acc.: 99.02%] [G loss: 4.893731]\n",
            "17219 [D loss: 0.161990, acc.: 93.75%] [G loss: 4.188641]\n",
            "17220 [D loss: 0.094718, acc.: 96.48%] [G loss: 4.954395]\n",
            "17221 [D loss: 0.087031, acc.: 97.07%] [G loss: 4.917335]\n",
            "17222 [D loss: 0.158995, acc.: 95.31%] [G loss: 4.661132]\n",
            "17223 [D loss: 0.138224, acc.: 95.70%] [G loss: 5.006442]\n",
            "17224 [D loss: 0.114078, acc.: 97.07%] [G loss: 5.285105]\n",
            "17225 [D loss: 0.078490, acc.: 98.05%] [G loss: 5.057053]\n",
            "17226 [D loss: 0.151322, acc.: 96.68%] [G loss: 4.881730]\n",
            "17227 [D loss: 0.073160, acc.: 97.46%] [G loss: 5.233879]\n",
            "17228 [D loss: 0.061250, acc.: 98.63%] [G loss: 5.510907]\n",
            "17229 [D loss: 0.092701, acc.: 96.68%] [G loss: 5.776206]\n",
            "17230 [D loss: 0.154009, acc.: 94.53%] [G loss: 4.412770]\n",
            "17231 [D loss: 0.064522, acc.: 98.05%] [G loss: 5.174956]\n",
            "17232 [D loss: 0.118206, acc.: 97.27%] [G loss: 6.628084]\n",
            "17233 [D loss: 0.156593, acc.: 96.88%] [G loss: 4.460344]\n",
            "17234 [D loss: 0.083003, acc.: 96.68%] [G loss: 7.094821]\n",
            "17235 [D loss: 0.114807, acc.: 97.66%] [G loss: 4.583778]\n",
            "17236 [D loss: 0.086667, acc.: 97.66%] [G loss: 4.355513]\n",
            "17237 [D loss: 0.031593, acc.: 98.63%] [G loss: 5.412327]\n",
            "17238 [D loss: 0.050011, acc.: 98.05%] [G loss: 4.035722]\n",
            "17239 [D loss: 0.079666, acc.: 97.46%] [G loss: 5.285463]\n",
            "17240 [D loss: 0.045390, acc.: 98.63%] [G loss: 6.232749]\n",
            "17241 [D loss: 0.128924, acc.: 96.68%] [G loss: 4.935789]\n",
            "17242 [D loss: 0.065008, acc.: 97.85%] [G loss: 4.674843]\n",
            "17243 [D loss: 0.170553, acc.: 95.90%] [G loss: 6.504476]\n",
            "17244 [D loss: 0.076077, acc.: 97.66%] [G loss: 5.831974]\n",
            "17245 [D loss: 0.072767, acc.: 97.27%] [G loss: 4.386497]\n",
            "17246 [D loss: 0.055472, acc.: 97.46%] [G loss: 5.055593]\n",
            "17247 [D loss: 0.058279, acc.: 98.44%] [G loss: 4.729767]\n",
            "17248 [D loss: 0.041027, acc.: 99.22%] [G loss: 4.618518]\n",
            "17249 [D loss: 0.084596, acc.: 97.85%] [G loss: 4.773747]\n",
            "17250 [D loss: 0.101225, acc.: 97.66%] [G loss: 5.091614]\n",
            "17251 [D loss: 0.098255, acc.: 96.68%] [G loss: 5.324429]\n",
            "17252 [D loss: 0.050719, acc.: 98.44%] [G loss: 5.245555]\n",
            "17253 [D loss: 0.119306, acc.: 97.27%] [G loss: 5.172906]\n",
            "17254 [D loss: 0.060040, acc.: 98.44%] [G loss: 5.840633]\n",
            "17255 [D loss: 0.086805, acc.: 97.66%] [G loss: 4.265096]\n",
            "17256 [D loss: 0.102935, acc.: 95.12%] [G loss: 4.708189]\n",
            "17257 [D loss: 0.071551, acc.: 97.85%] [G loss: 5.243721]\n",
            "17258 [D loss: 0.133338, acc.: 97.46%] [G loss: 6.061168]\n",
            "17259 [D loss: 0.042028, acc.: 98.83%] [G loss: 5.187521]\n",
            "17260 [D loss: 0.105290, acc.: 97.46%] [G loss: 4.587257]\n",
            "17261 [D loss: 0.036790, acc.: 99.02%] [G loss: 5.505295]\n",
            "17262 [D loss: 0.116892, acc.: 97.27%] [G loss: 5.332329]\n",
            "17263 [D loss: 0.040188, acc.: 98.24%] [G loss: 5.851698]\n",
            "17264 [D loss: 0.093723, acc.: 95.70%] [G loss: 5.636792]\n",
            "17265 [D loss: 0.070043, acc.: 98.05%] [G loss: 4.992607]\n",
            "17266 [D loss: 0.096996, acc.: 97.07%] [G loss: 4.624507]\n",
            "17267 [D loss: 0.091214, acc.: 97.07%] [G loss: 4.306240]\n",
            "17268 [D loss: 0.074128, acc.: 97.85%] [G loss: 4.429986]\n",
            "17269 [D loss: 0.087284, acc.: 96.88%] [G loss: 4.872080]\n",
            "17270 [D loss: 0.070341, acc.: 97.07%] [G loss: 4.704955]\n",
            "17271 [D loss: 0.072748, acc.: 97.46%] [G loss: 5.018256]\n",
            "17272 [D loss: 0.070888, acc.: 98.44%] [G loss: 5.206581]\n",
            "17273 [D loss: 0.090395, acc.: 97.85%] [G loss: 6.234403]\n",
            "17274 [D loss: 0.078404, acc.: 98.24%] [G loss: 5.458908]\n",
            "17275 [D loss: 0.155327, acc.: 97.27%] [G loss: 5.672171]\n",
            "17276 [D loss: 0.068338, acc.: 96.48%] [G loss: 5.009838]\n",
            "17277 [D loss: 0.142567, acc.: 96.48%] [G loss: 7.925014]\n",
            "17278 [D loss: 0.390374, acc.: 92.77%] [G loss: 9.667873]\n",
            "17279 [D loss: 0.224214, acc.: 95.70%] [G loss: 6.203826]\n",
            "17280 [D loss: 0.138253, acc.: 94.92%] [G loss: 5.096622]\n",
            "17281 [D loss: 0.175295, acc.: 95.51%] [G loss: 4.709414]\n",
            "17282 [D loss: 0.109184, acc.: 96.88%] [G loss: 5.187645]\n",
            "17283 [D loss: 0.082466, acc.: 98.05%] [G loss: 5.818359]\n",
            "17284 [D loss: 0.101910, acc.: 97.27%] [G loss: 5.046783]\n",
            "17285 [D loss: 0.069215, acc.: 98.05%] [G loss: 4.590587]\n",
            "17286 [D loss: 0.065504, acc.: 98.44%] [G loss: 4.661437]\n",
            "17287 [D loss: 0.263888, acc.: 93.36%] [G loss: 6.479219]\n",
            "17288 [D loss: 0.161951, acc.: 96.48%] [G loss: 7.800564]\n",
            "17289 [D loss: 0.211663, acc.: 95.12%] [G loss: 5.039492]\n",
            "17290 [D loss: 0.063884, acc.: 98.05%] [G loss: 4.895772]\n",
            "17291 [D loss: 0.106651, acc.: 96.09%] [G loss: 4.757701]\n",
            "17292 [D loss: 0.094695, acc.: 97.07%] [G loss: 4.169974]\n",
            "17293 [D loss: 0.079463, acc.: 96.68%] [G loss: 4.976164]\n",
            "17294 [D loss: 0.079545, acc.: 98.05%] [G loss: 4.626645]\n",
            "17295 [D loss: 0.107797, acc.: 97.85%] [G loss: 4.461768]\n",
            "17296 [D loss: 0.111819, acc.: 97.27%] [G loss: 4.246917]\n",
            "17297 [D loss: 0.067093, acc.: 98.05%] [G loss: 4.514038]\n",
            "17298 [D loss: 0.126514, acc.: 96.68%] [G loss: 4.016031]\n",
            "17299 [D loss: 0.072931, acc.: 97.85%] [G loss: 4.255874]\n",
            "17300 [D loss: 0.120933, acc.: 97.27%] [G loss: 4.954882]\n",
            "17301 [D loss: 0.056243, acc.: 97.85%] [G loss: 5.354862]\n",
            "17302 [D loss: 0.083539, acc.: 97.27%] [G loss: 4.354135]\n",
            "17303 [D loss: 0.094312, acc.: 96.88%] [G loss: 4.783138]\n",
            "17304 [D loss: 0.068367, acc.: 96.88%] [G loss: 4.627721]\n",
            "17305 [D loss: 0.085490, acc.: 96.88%] [G loss: 4.701884]\n",
            "17306 [D loss: 0.122092, acc.: 95.70%] [G loss: 4.888473]\n",
            "17307 [D loss: 0.055120, acc.: 98.24%] [G loss: 5.454849]\n",
            "17308 [D loss: 0.130260, acc.: 97.46%] [G loss: 4.871924]\n",
            "17309 [D loss: 0.053387, acc.: 98.24%] [G loss: 5.472702]\n",
            "17310 [D loss: 0.106860, acc.: 98.05%] [G loss: 5.242080]\n",
            "17311 [D loss: 0.112991, acc.: 96.09%] [G loss: 5.292185]\n",
            "17312 [D loss: 0.087861, acc.: 97.85%] [G loss: 5.543134]\n",
            "17313 [D loss: 0.045120, acc.: 98.24%] [G loss: 4.350909]\n",
            "17314 [D loss: 0.091777, acc.: 97.07%] [G loss: 5.366631]\n",
            "17315 [D loss: 0.109600, acc.: 96.48%] [G loss: 6.704291]\n",
            "17316 [D loss: 0.093106, acc.: 97.27%] [G loss: 5.433784]\n",
            "17317 [D loss: 0.109575, acc.: 96.09%] [G loss: 5.228773]\n",
            "17318 [D loss: 0.063470, acc.: 96.88%] [G loss: 5.774079]\n",
            "17319 [D loss: 0.076193, acc.: 97.46%] [G loss: 4.854749]\n",
            "17320 [D loss: 0.112038, acc.: 96.88%] [G loss: 4.382744]\n",
            "17321 [D loss: 0.073835, acc.: 98.05%] [G loss: 4.596171]\n",
            "17322 [D loss: 0.108630, acc.: 95.70%] [G loss: 4.604230]\n",
            "17323 [D loss: 0.078580, acc.: 97.85%] [G loss: 4.621321]\n",
            "17324 [D loss: 0.113905, acc.: 95.70%] [G loss: 4.169718]\n",
            "17325 [D loss: 0.054901, acc.: 98.63%] [G loss: 4.617562]\n",
            "17326 [D loss: 0.094693, acc.: 96.68%] [G loss: 5.221230]\n",
            "17327 [D loss: 0.083932, acc.: 97.27%] [G loss: 4.605489]\n",
            "17328 [D loss: 0.056366, acc.: 98.63%] [G loss: 5.522830]\n",
            "17329 [D loss: 0.051776, acc.: 98.83%] [G loss: 7.630820]\n",
            "17330 [D loss: 0.089493, acc.: 97.27%] [G loss: 3.858801]\n",
            "17331 [D loss: 0.041862, acc.: 98.24%] [G loss: 7.155364]\n",
            "17332 [D loss: 0.081322, acc.: 96.68%] [G loss: 5.886984]\n",
            "17333 [D loss: 0.084056, acc.: 97.27%] [G loss: 4.408661]\n",
            "17334 [D loss: 0.085711, acc.: 97.27%] [G loss: 4.226759]\n",
            "17335 [D loss: 0.092929, acc.: 96.29%] [G loss: 5.089614]\n",
            "17336 [D loss: 0.066762, acc.: 98.24%] [G loss: 5.531103]\n",
            "17337 [D loss: 0.146444, acc.: 95.90%] [G loss: 4.761708]\n",
            "17338 [D loss: 0.131003, acc.: 95.31%] [G loss: 4.848266]\n",
            "17339 [D loss: 0.052983, acc.: 98.83%] [G loss: 4.842178]\n",
            "17340 [D loss: 0.064743, acc.: 98.05%] [G loss: 5.046317]\n",
            "17341 [D loss: 0.078711, acc.: 98.05%] [G loss: 4.165870]\n",
            "17342 [D loss: 0.052717, acc.: 98.63%] [G loss: 4.410728]\n",
            "17343 [D loss: 0.108515, acc.: 97.07%] [G loss: 5.249475]\n",
            "17344 [D loss: 0.055937, acc.: 98.63%] [G loss: 4.917917]\n",
            "17345 [D loss: 0.071009, acc.: 97.07%] [G loss: 4.543574]\n",
            "17346 [D loss: 0.124288, acc.: 96.09%] [G loss: 4.523485]\n",
            "17347 [D loss: 0.115543, acc.: 96.48%] [G loss: 5.290980]\n",
            "17348 [D loss: 0.107165, acc.: 97.66%] [G loss: 4.582643]\n",
            "17349 [D loss: 0.063170, acc.: 98.63%] [G loss: 4.820512]\n",
            "17350 [D loss: 0.032759, acc.: 98.83%] [G loss: 5.194129]\n",
            "17351 [D loss: 0.131637, acc.: 97.07%] [G loss: 5.297610]\n",
            "17352 [D loss: 0.048611, acc.: 98.44%] [G loss: 5.628655]\n",
            "17353 [D loss: 0.112698, acc.: 97.27%] [G loss: 4.353290]\n",
            "17354 [D loss: 0.118074, acc.: 96.68%] [G loss: 5.075536]\n",
            "17355 [D loss: 0.094749, acc.: 97.27%] [G loss: 4.424477]\n",
            "17356 [D loss: 0.110225, acc.: 97.46%] [G loss: 5.630042]\n",
            "17357 [D loss: 0.095869, acc.: 98.05%] [G loss: 4.714629]\n",
            "17358 [D loss: 0.137666, acc.: 95.12%] [G loss: 5.465767]\n",
            "17359 [D loss: 0.106104, acc.: 96.68%] [G loss: 4.643683]\n",
            "17360 [D loss: 0.113690, acc.: 96.88%] [G loss: 5.698852]\n",
            "17361 [D loss: 0.150092, acc.: 94.73%] [G loss: 5.813886]\n",
            "17362 [D loss: 0.124984, acc.: 96.68%] [G loss: 5.628549]\n",
            "17363 [D loss: 0.043224, acc.: 99.22%] [G loss: 5.707144]\n",
            "17364 [D loss: 0.092839, acc.: 97.27%] [G loss: 4.697830]\n",
            "17365 [D loss: 0.100927, acc.: 97.07%] [G loss: 5.647151]\n",
            "17366 [D loss: 0.042841, acc.: 99.41%] [G loss: 5.371028]\n",
            "17367 [D loss: 0.076389, acc.: 98.24%] [G loss: 5.536143]\n",
            "17368 [D loss: 0.071860, acc.: 98.05%] [G loss: 5.530821]\n",
            "17369 [D loss: 0.057763, acc.: 97.85%] [G loss: 5.529470]\n",
            "17370 [D loss: 0.154973, acc.: 97.07%] [G loss: 5.894009]\n",
            "17371 [D loss: 0.156322, acc.: 94.53%] [G loss: 5.201252]\n",
            "17372 [D loss: 0.056523, acc.: 99.02%] [G loss: 5.028810]\n",
            "17373 [D loss: 0.135731, acc.: 96.09%] [G loss: 4.276458]\n",
            "17374 [D loss: 0.065349, acc.: 98.05%] [G loss: 5.695483]\n",
            "17375 [D loss: 0.063936, acc.: 97.46%] [G loss: 5.336866]\n",
            "17376 [D loss: 0.164963, acc.: 95.70%] [G loss: 4.986528]\n",
            "17377 [D loss: 0.113988, acc.: 97.27%] [G loss: 4.087482]\n",
            "17378 [D loss: 0.103949, acc.: 96.09%] [G loss: 5.677009]\n",
            "17379 [D loss: 0.092829, acc.: 98.24%] [G loss: 5.726381]\n",
            "17380 [D loss: 0.114443, acc.: 96.29%] [G loss: 5.854293]\n",
            "17381 [D loss: 0.097408, acc.: 95.12%] [G loss: 5.610724]\n",
            "17382 [D loss: 0.096927, acc.: 97.46%] [G loss: 5.462791]\n",
            "17383 [D loss: 0.086512, acc.: 96.29%] [G loss: 5.390687]\n",
            "17384 [D loss: 0.064509, acc.: 97.85%] [G loss: 4.866720]\n",
            "17385 [D loss: 0.101075, acc.: 97.07%] [G loss: 5.375386]\n",
            "17386 [D loss: 0.134184, acc.: 94.73%] [G loss: 7.006118]\n",
            "17387 [D loss: 0.110348, acc.: 97.07%] [G loss: 5.618398]\n",
            "17388 [D loss: 0.136449, acc.: 96.09%] [G loss: 8.017035]\n",
            "17389 [D loss: 0.192237, acc.: 94.73%] [G loss: 10.689066]\n",
            "17390 [D loss: 0.439019, acc.: 95.31%] [G loss: 8.872191]\n",
            "17391 [D loss: 0.103200, acc.: 98.05%] [G loss: 7.993372]\n",
            "17392 [D loss: 0.143479, acc.: 95.12%] [G loss: 7.630022]\n",
            "17393 [D loss: 0.211846, acc.: 95.51%] [G loss: 4.904381]\n",
            "17394 [D loss: 0.095621, acc.: 97.07%] [G loss: 4.620722]\n",
            "17395 [D loss: 0.097205, acc.: 96.48%] [G loss: 6.145607]\n",
            "17396 [D loss: 0.096919, acc.: 96.48%] [G loss: 6.505743]\n",
            "17397 [D loss: 0.080658, acc.: 97.66%] [G loss: 5.390114]\n",
            "17398 [D loss: 0.070426, acc.: 97.85%] [G loss: 6.159747]\n",
            "17399 [D loss: 0.115544, acc.: 98.05%] [G loss: 4.895066]\n",
            "17400 [D loss: 0.112397, acc.: 97.66%] [G loss: 6.078377]\n",
            "17401 [D loss: 0.114414, acc.: 96.68%] [G loss: 5.510595]\n",
            "17402 [D loss: 0.184835, acc.: 93.75%] [G loss: 6.513721]\n",
            "17403 [D loss: 0.137756, acc.: 97.27%] [G loss: 6.832082]\n",
            "17404 [D loss: 0.180464, acc.: 96.88%] [G loss: 4.787891]\n",
            "17405 [D loss: 0.166063, acc.: 94.53%] [G loss: 5.164812]\n",
            "17406 [D loss: 0.095800, acc.: 96.48%] [G loss: 4.964821]\n",
            "17407 [D loss: 0.086110, acc.: 98.24%] [G loss: 4.181538]\n",
            "17408 [D loss: 0.088833, acc.: 96.68%] [G loss: 6.253624]\n",
            "17409 [D loss: 0.118239, acc.: 96.09%] [G loss: 5.863058]\n",
            "17410 [D loss: 0.142380, acc.: 96.48%] [G loss: 5.111683]\n",
            "17411 [D loss: 0.106571, acc.: 97.27%] [G loss: 4.712187]\n",
            "17412 [D loss: 0.111784, acc.: 97.46%] [G loss: 5.254628]\n",
            "17413 [D loss: 0.061312, acc.: 98.24%] [G loss: 5.472914]\n",
            "17414 [D loss: 0.106238, acc.: 95.12%] [G loss: 5.761157]\n",
            "17415 [D loss: 0.047996, acc.: 98.83%] [G loss: 6.143125]\n",
            "17416 [D loss: 0.120656, acc.: 97.46%] [G loss: 4.504427]\n",
            "17417 [D loss: 0.111607, acc.: 96.48%] [G loss: 5.280717]\n",
            "17418 [D loss: 0.095994, acc.: 97.46%] [G loss: 4.727638]\n",
            "17419 [D loss: 0.093048, acc.: 96.88%] [G loss: 5.213955]\n",
            "17420 [D loss: 0.106442, acc.: 96.68%] [G loss: 5.931452]\n",
            "17421 [D loss: 0.076998, acc.: 97.66%] [G loss: 5.907541]\n",
            "17422 [D loss: 0.090529, acc.: 97.07%] [G loss: 5.588017]\n",
            "17423 [D loss: 0.138141, acc.: 96.48%] [G loss: 5.559821]\n",
            "17424 [D loss: 0.035915, acc.: 99.02%] [G loss: 5.876960]\n",
            "17425 [D loss: 0.107211, acc.: 96.88%] [G loss: 5.151459]\n",
            "17426 [D loss: 0.112510, acc.: 97.27%] [G loss: 4.687913]\n",
            "17427 [D loss: 0.095938, acc.: 97.07%] [G loss: 5.599692]\n",
            "17428 [D loss: 0.047971, acc.: 98.63%] [G loss: 5.570631]\n",
            "17429 [D loss: 0.058943, acc.: 97.66%] [G loss: 4.769829]\n",
            "17430 [D loss: 0.065805, acc.: 97.07%] [G loss: 5.743263]\n",
            "17431 [D loss: 0.075739, acc.: 97.46%] [G loss: 5.328101]\n",
            "17432 [D loss: 0.092009, acc.: 97.27%] [G loss: 4.751114]\n",
            "17433 [D loss: 0.080184, acc.: 97.85%] [G loss: 4.976663]\n",
            "17434 [D loss: 0.104618, acc.: 96.09%] [G loss: 4.630066]\n",
            "17435 [D loss: 0.085452, acc.: 96.48%] [G loss: 6.608972]\n",
            "17436 [D loss: 0.141188, acc.: 94.14%] [G loss: 5.151218]\n",
            "17437 [D loss: 0.070124, acc.: 98.44%] [G loss: 4.473825]\n",
            "17438 [D loss: 0.075326, acc.: 98.24%] [G loss: 4.951686]\n",
            "17439 [D loss: 0.090598, acc.: 96.88%] [G loss: 4.770354]\n",
            "17440 [D loss: 0.056811, acc.: 98.24%] [G loss: 5.586327]\n",
            "17441 [D loss: 0.097049, acc.: 97.66%] [G loss: 4.941097]\n",
            "17442 [D loss: 0.058255, acc.: 98.24%] [G loss: 5.344125]\n",
            "17443 [D loss: 0.129252, acc.: 96.09%] [G loss: 4.780256]\n",
            "17444 [D loss: 0.053380, acc.: 98.44%] [G loss: 5.333823]\n",
            "17445 [D loss: 0.084336, acc.: 97.66%] [G loss: 4.460536]\n",
            "17446 [D loss: 0.116455, acc.: 96.29%] [G loss: 6.517069]\n",
            "17447 [D loss: 0.080757, acc.: 97.07%] [G loss: 4.840614]\n",
            "17448 [D loss: 0.110222, acc.: 96.48%] [G loss: 5.562864]\n",
            "17449 [D loss: 0.179417, acc.: 93.16%] [G loss: 5.091341]\n",
            "17450 [D loss: 0.142710, acc.: 96.68%] [G loss: 4.498317]\n",
            "17451 [D loss: 0.121945, acc.: 94.92%] [G loss: 4.891782]\n",
            "17452 [D loss: 0.086620, acc.: 96.88%] [G loss: 5.482127]\n",
            "17453 [D loss: 0.127068, acc.: 97.66%] [G loss: 4.332071]\n",
            "17454 [D loss: 0.063048, acc.: 97.85%] [G loss: 5.625442]\n",
            "17455 [D loss: 0.089774, acc.: 97.07%] [G loss: 4.622326]\n",
            "17456 [D loss: 0.093241, acc.: 96.48%] [G loss: 5.389853]\n",
            "17457 [D loss: 0.087453, acc.: 97.46%] [G loss: 4.860452]\n",
            "17458 [D loss: 0.079357, acc.: 98.05%] [G loss: 4.334225]\n",
            "17459 [D loss: 0.083968, acc.: 96.68%] [G loss: 4.604376]\n",
            "17460 [D loss: 0.052595, acc.: 98.44%] [G loss: 5.590011]\n",
            "17461 [D loss: 0.083682, acc.: 97.46%] [G loss: 5.236565]\n",
            "17462 [D loss: 0.062825, acc.: 97.07%] [G loss: 5.182794]\n",
            "17463 [D loss: 0.051318, acc.: 98.24%] [G loss: 5.444478]\n",
            "17464 [D loss: 0.116288, acc.: 96.48%] [G loss: 5.484214]\n",
            "17465 [D loss: 0.097422, acc.: 96.68%] [G loss: 5.350144]\n",
            "17466 [D loss: 0.063701, acc.: 97.27%] [G loss: 6.669453]\n",
            "17467 [D loss: 0.107398, acc.: 96.29%] [G loss: 6.878017]\n",
            "17468 [D loss: 0.107430, acc.: 95.70%] [G loss: 5.693808]\n",
            "17469 [D loss: 0.083119, acc.: 97.27%] [G loss: 5.013848]\n",
            "17470 [D loss: 0.159288, acc.: 96.09%] [G loss: 6.486749]\n",
            "17471 [D loss: 0.118964, acc.: 95.51%] [G loss: 6.129805]\n",
            "17472 [D loss: 0.073596, acc.: 98.05%] [G loss: 7.070884]\n",
            "17473 [D loss: 0.119528, acc.: 97.46%] [G loss: 5.143343]\n",
            "17474 [D loss: 0.129725, acc.: 97.07%] [G loss: 6.504070]\n",
            "17475 [D loss: 0.070335, acc.: 97.85%] [G loss: 4.802669]\n",
            "17476 [D loss: 0.033822, acc.: 98.83%] [G loss: 5.702257]\n",
            "17477 [D loss: 0.145242, acc.: 94.14%] [G loss: 6.129382]\n",
            "17478 [D loss: 0.106053, acc.: 97.07%] [G loss: 4.948267]\n",
            "17479 [D loss: 0.105864, acc.: 97.27%] [G loss: 6.849198]\n",
            "17480 [D loss: 0.156232, acc.: 95.51%] [G loss: 5.741058]\n",
            "17481 [D loss: 0.081663, acc.: 97.85%] [G loss: 6.413231]\n",
            "17482 [D loss: 0.110748, acc.: 97.66%] [G loss: 4.668305]\n",
            "17483 [D loss: 0.108441, acc.: 96.88%] [G loss: 5.377102]\n",
            "17484 [D loss: 0.069708, acc.: 98.44%] [G loss: 5.385164]\n",
            "17485 [D loss: 0.094739, acc.: 97.66%] [G loss: 4.662860]\n",
            "17486 [D loss: 0.084507, acc.: 97.66%] [G loss: 6.034261]\n",
            "17487 [D loss: 0.137493, acc.: 97.27%] [G loss: 5.118060]\n",
            "17488 [D loss: 0.083014, acc.: 97.66%] [G loss: 4.820820]\n",
            "17489 [D loss: 0.152773, acc.: 95.31%] [G loss: 5.448847]\n",
            "17490 [D loss: 0.083894, acc.: 97.85%] [G loss: 5.054104]\n",
            "17491 [D loss: 0.116813, acc.: 96.09%] [G loss: 6.962698]\n",
            "17492 [D loss: 0.523150, acc.: 88.87%] [G loss: 12.657785]\n",
            "17493 [D loss: 1.001526, acc.: 90.82%] [G loss: 13.242343]\n",
            "17494 [D loss: 0.412735, acc.: 95.31%] [G loss: 8.551520]\n",
            "17495 [D loss: 1.152118, acc.: 60.55%] [G loss: 51.438423]\n",
            "17496 [D loss: 3.810760, acc.: 80.66%] [G loss: 51.852139]\n",
            "17497 [D loss: 3.655746, acc.: 85.94%] [G loss: 2.089252]\n",
            "17498 [D loss: 4.550542, acc.: 50.20%] [G loss: 102.730919]\n",
            "17499 [D loss: 83.366035, acc.: 30.27%] [G loss: 50.383648]\n",
            "17500 [D loss: 31.329580, acc.: 57.03%] [G loss: 119.549683]\n",
            "17501 [D loss: 70.350780, acc.: 44.34%] [G loss: 274.569153]\n",
            "17502 [D loss: 98.536888, acc.: 51.37%] [G loss: 586.784851]\n",
            "17503 [D loss: 238.609612, acc.: 53.12%] [G loss: 5.534083]\n",
            "17504 [D loss: 55.480264, acc.: 73.83%] [G loss: 203.039352]\n",
            "17505 [D loss: 61.202201, acc.: 41.99%] [G loss: 160.555527]\n",
            "17506 [D loss: 11.738695, acc.: 79.10%] [G loss: 319.588135]\n",
            "17507 [D loss: 13.665212, acc.: 89.45%] [G loss: 97.651436]\n",
            "17508 [D loss: 15.001657, acc.: 67.19%] [G loss: 158.293579]\n",
            "17509 [D loss: 17.719253, acc.: 73.24%] [G loss: 58.051105]\n",
            "17510 [D loss: 8.571275, acc.: 81.64%] [G loss: 52.808590]\n",
            "17511 [D loss: 2.207526, acc.: 93.16%] [G loss: 91.612885]\n",
            "17512 [D loss: 8.030859, acc.: 76.56%] [G loss: 132.060226]\n",
            "17513 [D loss: 17.070661, acc.: 71.68%] [G loss: 86.989754]\n",
            "17514 [D loss: 0.981612, acc.: 92.58%] [G loss: 73.089287]\n",
            "17515 [D loss: 2.191750, acc.: 89.06%] [G loss: 68.922485]\n",
            "17516 [D loss: 4.920912, acc.: 83.98%] [G loss: 79.178421]\n",
            "17517 [D loss: 12.120286, acc.: 54.49%] [G loss: 143.536682]\n",
            "17518 [D loss: 19.512728, acc.: 60.16%] [G loss: 34.382603]\n",
            "17519 [D loss: 4.143745, acc.: 78.12%] [G loss: 60.474556]\n",
            "17520 [D loss: 2.556796, acc.: 93.16%] [G loss: 53.836552]\n",
            "17521 [D loss: 3.762199, acc.: 83.98%] [G loss: 37.163799]\n",
            "17522 [D loss: 1.273941, acc.: 91.02%] [G loss: 20.691219]\n",
            "17523 [D loss: 3.219649, acc.: 86.91%] [G loss: 51.437866]\n",
            "17524 [D loss: 4.101111, acc.: 78.71%] [G loss: 67.871284]\n",
            "17525 [D loss: 7.391756, acc.: 63.28%] [G loss: 30.326813]\n",
            "17526 [D loss: 4.363981, acc.: 81.84%] [G loss: 24.200026]\n",
            "17527 [D loss: 1.173980, acc.: 90.23%] [G loss: 13.839739]\n",
            "17528 [D loss: 0.720015, acc.: 91.99%] [G loss: 32.228470]\n",
            "17529 [D loss: 0.924573, acc.: 92.38%] [G loss: 23.511892]\n",
            "17530 [D loss: 1.606521, acc.: 79.88%] [G loss: 41.739174]\n",
            "17531 [D loss: 1.657594, acc.: 86.72%] [G loss: 23.269941]\n",
            "17532 [D loss: 0.741629, acc.: 87.50%] [G loss: 37.360302]\n",
            "17533 [D loss: 0.805284, acc.: 92.97%] [G loss: 27.404430]\n",
            "17534 [D loss: 1.527706, acc.: 90.23%] [G loss: 37.217766]\n",
            "17535 [D loss: 1.755407, acc.: 77.73%] [G loss: 35.689781]\n",
            "17536 [D loss: 0.867033, acc.: 90.43%] [G loss: 14.657260]\n",
            "17537 [D loss: 2.700136, acc.: 58.01%] [G loss: 77.866455]\n",
            "17538 [D loss: 14.445500, acc.: 62.70%] [G loss: 21.175434]\n",
            "17539 [D loss: 2.538254, acc.: 84.18%] [G loss: 20.082521]\n",
            "17540 [D loss: 0.280988, acc.: 95.90%] [G loss: 25.415312]\n",
            "17541 [D loss: 0.226218, acc.: 94.73%] [G loss: 20.929188]\n",
            "17542 [D loss: 0.047758, acc.: 98.63%] [G loss: 17.356949]\n",
            "17543 [D loss: 0.234761, acc.: 94.73%] [G loss: 20.640379]\n",
            "17544 [D loss: 0.405647, acc.: 91.80%] [G loss: 13.260833]\n",
            "17545 [D loss: 0.795744, acc.: 90.23%] [G loss: 16.761196]\n",
            "17546 [D loss: 0.120045, acc.: 96.88%] [G loss: 16.795656]\n",
            "17547 [D loss: 0.577940, acc.: 86.13%] [G loss: 17.698130]\n",
            "17548 [D loss: 0.192833, acc.: 96.68%] [G loss: 17.443428]\n",
            "17549 [D loss: 0.277852, acc.: 93.55%] [G loss: 14.011040]\n",
            "17550 [D loss: 0.115472, acc.: 95.12%] [G loss: 11.321899]\n",
            "17551 [D loss: 0.146172, acc.: 94.73%] [G loss: 14.813443]\n",
            "17552 [D loss: 2.625768, acc.: 58.59%] [G loss: 29.271437]\n",
            "17553 [D loss: 4.190378, acc.: 72.07%] [G loss: 18.266098]\n",
            "17554 [D loss: 0.319392, acc.: 95.51%] [G loss: 13.913203]\n",
            "17555 [D loss: 0.347754, acc.: 94.92%] [G loss: 11.949613]\n",
            "17556 [D loss: 0.207392, acc.: 95.31%] [G loss: 15.759037]\n",
            "17557 [D loss: 0.143371, acc.: 93.55%] [G loss: 16.620995]\n",
            "17558 [D loss: 0.048763, acc.: 98.44%] [G loss: 13.270294]\n",
            "17559 [D loss: 0.015488, acc.: 99.80%] [G loss: 10.811615]\n",
            "17560 [D loss: 0.071721, acc.: 98.05%] [G loss: 12.198156]\n",
            "17561 [D loss: 0.236135, acc.: 93.75%] [G loss: 10.886707]\n",
            "17562 [D loss: 0.234401, acc.: 93.55%] [G loss: 15.074309]\n",
            "17563 [D loss: 0.739653, acc.: 78.32%] [G loss: 18.196232]\n",
            "17564 [D loss: 0.999459, acc.: 87.70%] [G loss: 10.462744]\n",
            "17565 [D loss: 0.235582, acc.: 97.46%] [G loss: 9.036117]\n",
            "17566 [D loss: 0.060324, acc.: 97.46%] [G loss: 12.260235]\n",
            "17567 [D loss: 0.489399, acc.: 85.94%] [G loss: 15.617578]\n",
            "17568 [D loss: 1.091909, acc.: 85.74%] [G loss: 11.228943]\n",
            "17569 [D loss: 0.274955, acc.: 95.70%] [G loss: 16.334362]\n",
            "17570 [D loss: 0.316293, acc.: 88.28%] [G loss: 19.984442]\n",
            "17571 [D loss: 0.442080, acc.: 91.80%] [G loss: 9.771911]\n",
            "17572 [D loss: 0.151143, acc.: 94.53%] [G loss: 14.116037]\n",
            "17573 [D loss: 0.041588, acc.: 98.24%] [G loss: 14.039225]\n",
            "17574 [D loss: 0.097545, acc.: 95.70%] [G loss: 13.980327]\n",
            "17575 [D loss: 0.191633, acc.: 95.31%] [G loss: 14.698514]\n",
            "17576 [D loss: 2.595943, acc.: 84.57%] [G loss: 15.640329]\n",
            "17577 [D loss: 0.357107, acc.: 88.67%] [G loss: 17.744072]\n",
            "17578 [D loss: 0.552563, acc.: 91.80%] [G loss: 9.133034]\n",
            "17579 [D loss: 0.295407, acc.: 89.84%] [G loss: 17.894773]\n",
            "17580 [D loss: 0.599704, acc.: 94.34%] [G loss: 12.367070]\n",
            "17581 [D loss: 0.042242, acc.: 99.61%] [G loss: 10.247240]\n",
            "17582 [D loss: 0.013828, acc.: 99.41%] [G loss: 9.119942]\n",
            "17583 [D loss: 0.019922, acc.: 99.22%] [G loss: 10.288624]\n",
            "17584 [D loss: 0.009513, acc.: 100.00%] [G loss: 9.772235]\n",
            "17585 [D loss: 0.020550, acc.: 99.61%] [G loss: 9.285998]\n",
            "17586 [D loss: 0.158878, acc.: 94.14%] [G loss: 11.118620]\n",
            "17587 [D loss: 0.379105, acc.: 93.75%] [G loss: 9.814182]\n",
            "17588 [D loss: 0.682203, acc.: 88.28%] [G loss: 10.188770]\n",
            "17589 [D loss: 0.487482, acc.: 91.80%] [G loss: 7.055347]\n",
            "17590 [D loss: 0.060218, acc.: 97.27%] [G loss: 8.829496]\n",
            "17591 [D loss: 0.710652, acc.: 73.44%] [G loss: 21.880516]\n",
            "17592 [D loss: 2.412857, acc.: 77.54%] [G loss: 12.947382]\n",
            "17593 [D loss: 0.284827, acc.: 94.73%] [G loss: 12.204974]\n",
            "17594 [D loss: 0.138323, acc.: 96.09%] [G loss: 13.033567]\n",
            "17595 [D loss: 0.055288, acc.: 98.05%] [G loss: 11.934597]\n",
            "17596 [D loss: 0.057873, acc.: 96.88%] [G loss: 10.309064]\n",
            "17597 [D loss: 0.053529, acc.: 98.44%] [G loss: 11.190287]\n",
            "17598 [D loss: 0.037241, acc.: 99.02%] [G loss: 10.699015]\n",
            "17599 [D loss: 0.123429, acc.: 94.92%] [G loss: 11.861587]\n",
            "17600 [D loss: 0.080360, acc.: 96.29%] [G loss: 13.823586]\n",
            "17601 [D loss: 0.165832, acc.: 95.51%] [G loss: 9.038735]\n",
            "17602 [D loss: 0.136086, acc.: 95.70%] [G loss: 12.225536]\n",
            "17603 [D loss: 0.560495, acc.: 92.19%] [G loss: 12.026157]\n",
            "17604 [D loss: 0.041210, acc.: 99.02%] [G loss: 9.885620]\n",
            "17605 [D loss: 0.125603, acc.: 95.51%] [G loss: 11.130772]\n",
            "17606 [D loss: 0.175649, acc.: 94.92%] [G loss: 8.432577]\n",
            "17607 [D loss: 0.053252, acc.: 98.05%] [G loss: 8.269998]\n",
            "17608 [D loss: 0.104195, acc.: 95.90%] [G loss: 9.833240]\n",
            "17609 [D loss: 0.172751, acc.: 93.36%] [G loss: 7.331036]\n",
            "17610 [D loss: 0.063234, acc.: 97.85%] [G loss: 9.299137]\n",
            "17611 [D loss: 0.016002, acc.: 99.02%] [G loss: 10.246109]\n",
            "17612 [D loss: 0.090324, acc.: 96.48%] [G loss: 9.388237]\n",
            "17613 [D loss: 0.132160, acc.: 95.70%] [G loss: 8.416995]\n",
            "17614 [D loss: 0.025265, acc.: 99.41%] [G loss: 8.893264]\n",
            "17615 [D loss: 0.163934, acc.: 91.41%] [G loss: 11.517447]\n",
            "17616 [D loss: 0.092787, acc.: 96.88%] [G loss: 9.238189]\n",
            "17617 [D loss: 0.061942, acc.: 97.66%] [G loss: 9.305272]\n",
            "17618 [D loss: 0.128838, acc.: 96.48%] [G loss: 10.001474]\n",
            "17619 [D loss: 0.199171, acc.: 92.38%] [G loss: 6.285681]\n",
            "17620 [D loss: 0.194056, acc.: 94.53%] [G loss: 10.881404]\n",
            "17621 [D loss: 0.514230, acc.: 88.09%] [G loss: 12.050227]\n",
            "17622 [D loss: 0.140441, acc.: 95.31%] [G loss: 18.435703]\n",
            "17623 [D loss: 0.381254, acc.: 93.16%] [G loss: 17.142338]\n",
            "17624 [D loss: 0.064725, acc.: 98.44%] [G loss: 15.453938]\n",
            "17625 [D loss: 0.187777, acc.: 94.73%] [G loss: 16.407269]\n",
            "17626 [D loss: 0.120813, acc.: 95.70%] [G loss: 11.753508]\n",
            "17627 [D loss: 0.182494, acc.: 95.70%] [G loss: 9.544163]\n",
            "17628 [D loss: 0.157036, acc.: 95.90%] [G loss: 11.678672]\n",
            "17629 [D loss: 0.082075, acc.: 95.90%] [G loss: 10.019121]\n",
            "17630 [D loss: 0.062240, acc.: 98.24%] [G loss: 13.936001]\n",
            "17631 [D loss: 0.050396, acc.: 97.66%] [G loss: 8.464338]\n",
            "17632 [D loss: 0.116186, acc.: 96.09%] [G loss: 12.312225]\n",
            "17633 [D loss: 0.066276, acc.: 97.66%] [G loss: 12.196408]\n",
            "17634 [D loss: 0.133255, acc.: 96.68%] [G loss: 11.602798]\n",
            "17635 [D loss: 0.153056, acc.: 95.90%] [G loss: 13.597378]\n",
            "17636 [D loss: 0.109325, acc.: 95.31%] [G loss: 12.138441]\n",
            "17637 [D loss: 0.140452, acc.: 95.70%] [G loss: 6.263456]\n",
            "17638 [D loss: 0.010367, acc.: 99.61%] [G loss: 11.148073]\n",
            "17639 [D loss: 0.046676, acc.: 98.44%] [G loss: 8.009623]\n",
            "17640 [D loss: 0.088879, acc.: 96.29%] [G loss: 12.372922]\n",
            "17641 [D loss: 0.031320, acc.: 99.41%] [G loss: 16.949070]\n",
            "17642 [D loss: 0.047254, acc.: 99.22%] [G loss: 10.193768]\n",
            "17643 [D loss: 0.036789, acc.: 98.63%] [G loss: 11.391676]\n",
            "17644 [D loss: 0.094812, acc.: 96.09%] [G loss: 10.618762]\n",
            "17645 [D loss: 0.050195, acc.: 99.41%] [G loss: 8.622499]\n",
            "17646 [D loss: 0.639052, acc.: 91.80%] [G loss: 11.302629]\n",
            "17647 [D loss: 0.361320, acc.: 91.80%] [G loss: 9.137926]\n",
            "17648 [D loss: 0.012105, acc.: 100.00%] [G loss: 10.081975]\n",
            "17649 [D loss: 0.043434, acc.: 99.41%] [G loss: 8.704664]\n",
            "17650 [D loss: 0.022546, acc.: 99.80%] [G loss: 9.171796]\n",
            "17651 [D loss: 0.024012, acc.: 99.61%] [G loss: 9.873255]\n",
            "17652 [D loss: 0.021636, acc.: 98.63%] [G loss: 9.598918]\n",
            "17653 [D loss: 0.122230, acc.: 94.14%] [G loss: 15.396240]\n",
            "17654 [D loss: 1.342133, acc.: 73.05%] [G loss: 12.846440]\n",
            "17655 [D loss: 0.014127, acc.: 99.41%] [G loss: 19.535000]\n",
            "17656 [D loss: 0.181125, acc.: 95.12%] [G loss: 12.658467]\n",
            "17657 [D loss: 0.005575, acc.: 100.00%] [G loss: 9.161297]\n",
            "17658 [D loss: 0.047217, acc.: 98.24%] [G loss: 9.498552]\n",
            "17659 [D loss: 0.001332, acc.: 100.00%] [G loss: 9.698654]\n",
            "17660 [D loss: 0.038603, acc.: 99.61%] [G loss: 9.545325]\n",
            "17661 [D loss: 0.011103, acc.: 99.80%] [G loss: 9.598992]\n",
            "17662 [D loss: 0.016633, acc.: 99.80%] [G loss: 8.051342]\n",
            "17663 [D loss: 0.016437, acc.: 99.61%] [G loss: 9.274146]\n",
            "17664 [D loss: 0.021613, acc.: 99.41%] [G loss: 9.564022]\n",
            "17665 [D loss: 0.016529, acc.: 100.00%] [G loss: 8.073963]\n",
            "17666 [D loss: 0.040510, acc.: 98.44%] [G loss: 9.781265]\n",
            "17667 [D loss: 0.106284, acc.: 97.27%] [G loss: 8.835246]\n",
            "17668 [D loss: 0.076650, acc.: 97.85%] [G loss: 9.389456]\n",
            "17669 [D loss: 0.074160, acc.: 97.85%] [G loss: 9.934721]\n",
            "17670 [D loss: 0.149902, acc.: 93.95%] [G loss: 8.292217]\n",
            "17671 [D loss: 0.223469, acc.: 94.73%] [G loss: 11.432459]\n",
            "17672 [D loss: 0.309931, acc.: 90.04%] [G loss: 8.627243]\n",
            "17673 [D loss: 0.141059, acc.: 96.09%] [G loss: 11.488837]\n",
            "17674 [D loss: 0.181931, acc.: 96.68%] [G loss: 10.422804]\n",
            "17675 [D loss: 0.234510, acc.: 92.77%] [G loss: 13.020424]\n",
            "17676 [D loss: 0.024332, acc.: 98.44%] [G loss: 15.155142]\n",
            "17677 [D loss: 0.009129, acc.: 99.80%] [G loss: 13.502364]\n",
            "17678 [D loss: 0.028384, acc.: 99.41%] [G loss: 9.271304]\n",
            "17679 [D loss: 0.020546, acc.: 99.22%] [G loss: 9.765145]\n",
            "17680 [D loss: 0.046800, acc.: 98.05%] [G loss: 9.010288]\n",
            "17681 [D loss: 0.009848, acc.: 100.00%] [G loss: 9.492942]\n",
            "17682 [D loss: 0.014441, acc.: 99.80%] [G loss: 10.742755]\n",
            "17683 [D loss: 0.033419, acc.: 99.22%] [G loss: 8.846420]\n",
            "17684 [D loss: 0.040951, acc.: 98.44%] [G loss: 9.053417]\n",
            "17685 [D loss: 0.071091, acc.: 97.07%] [G loss: 8.462995]\n",
            "17686 [D loss: 0.049758, acc.: 98.63%] [G loss: 7.769999]\n",
            "17687 [D loss: 0.036164, acc.: 98.63%] [G loss: 8.129138]\n",
            "17688 [D loss: 0.034667, acc.: 99.22%] [G loss: 8.681443]\n",
            "17689 [D loss: 0.091278, acc.: 96.48%] [G loss: 10.084920]\n",
            "17690 [D loss: 0.811151, acc.: 76.17%] [G loss: 16.146885]\n",
            "17691 [D loss: 0.728692, acc.: 87.30%] [G loss: 6.321733]\n",
            "17692 [D loss: 0.381264, acc.: 88.67%] [G loss: 14.455574]\n",
            "17693 [D loss: 0.338292, acc.: 94.53%] [G loss: 12.586451]\n",
            "17694 [D loss: 0.164143, acc.: 94.73%] [G loss: 6.806174]\n",
            "17695 [D loss: 0.075516, acc.: 97.27%] [G loss: 9.513770]\n",
            "17696 [D loss: 0.016186, acc.: 99.41%] [G loss: 9.309830]\n",
            "17697 [D loss: 0.019466, acc.: 99.22%] [G loss: 9.522388]\n",
            "17698 [D loss: 0.086507, acc.: 97.85%] [G loss: 9.809959]\n",
            "17699 [D loss: 0.015411, acc.: 100.00%] [G loss: 9.299603]\n",
            "17700 [D loss: 0.203154, acc.: 94.92%] [G loss: 8.922785]\n",
            "17701 [D loss: 0.149822, acc.: 96.48%] [G loss: 9.596533]\n",
            "17702 [D loss: 0.115283, acc.: 96.09%] [G loss: 8.453136]\n",
            "17703 [D loss: 0.096674, acc.: 97.85%] [G loss: 10.088277]\n",
            "17704 [D loss: 0.123020, acc.: 95.31%] [G loss: 8.095731]\n",
            "17705 [D loss: 0.012801, acc.: 99.80%] [G loss: 8.318957]\n",
            "17706 [D loss: 0.036424, acc.: 98.63%] [G loss: 8.285973]\n",
            "17707 [D loss: 0.013232, acc.: 100.00%] [G loss: 8.558398]\n",
            "17708 [D loss: 0.018536, acc.: 100.00%] [G loss: 7.837381]\n",
            "17709 [D loss: 0.048452, acc.: 97.66%] [G loss: 10.819802]\n",
            "17710 [D loss: 0.164701, acc.: 92.97%] [G loss: 7.795706]\n",
            "17711 [D loss: 0.395144, acc.: 86.33%] [G loss: 22.085331]\n",
            "17712 [D loss: 1.582267, acc.: 80.27%] [G loss: 8.363785]\n",
            "17713 [D loss: 0.235506, acc.: 93.95%] [G loss: 11.541257]\n",
            "17714 [D loss: 0.203904, acc.: 93.36%] [G loss: 7.792744]\n",
            "17715 [D loss: 0.071673, acc.: 97.27%] [G loss: 8.806330]\n",
            "17716 [D loss: 0.016115, acc.: 99.80%] [G loss: 8.899644]\n",
            "17717 [D loss: 0.012269, acc.: 99.22%] [G loss: 10.652554]\n",
            "17718 [D loss: 0.027256, acc.: 99.22%] [G loss: 10.128108]\n",
            "17719 [D loss: 0.069019, acc.: 98.24%] [G loss: 8.899261]\n",
            "17720 [D loss: 0.007441, acc.: 99.80%] [G loss: 9.318789]\n",
            "17721 [D loss: 0.028389, acc.: 99.02%] [G loss: 8.437307]\n",
            "17722 [D loss: 0.021265, acc.: 99.61%] [G loss: 9.110285]\n",
            "17723 [D loss: 0.034453, acc.: 98.44%] [G loss: 7.497831]\n",
            "17724 [D loss: 0.009252, acc.: 99.80%] [G loss: 7.246209]\n",
            "17725 [D loss: 0.019023, acc.: 99.80%] [G loss: 7.632538]\n",
            "17726 [D loss: 0.023118, acc.: 99.80%] [G loss: 8.147535]\n",
            "17727 [D loss: 0.246063, acc.: 88.87%] [G loss: 13.737024]\n",
            "17728 [D loss: 0.381124, acc.: 91.99%] [G loss: 6.052444]\n",
            "17729 [D loss: 0.035596, acc.: 98.83%] [G loss: 8.760293]\n",
            "17730 [D loss: 0.023370, acc.: 100.00%] [G loss: 7.106146]\n",
            "17731 [D loss: 0.011191, acc.: 99.80%] [G loss: 7.655150]\n",
            "17732 [D loss: 0.016593, acc.: 99.61%] [G loss: 14.055851]\n",
            "17733 [D loss: 0.008593, acc.: 99.80%] [G loss: 11.151095]\n",
            "17734 [D loss: 0.019638, acc.: 99.02%] [G loss: 7.044819]\n",
            "17735 [D loss: 0.089094, acc.: 97.85%] [G loss: 9.179783]\n",
            "17736 [D loss: 0.289648, acc.: 90.62%] [G loss: 9.890348]\n",
            "17737 [D loss: 0.022748, acc.: 99.61%] [G loss: 9.541418]\n",
            "17738 [D loss: 0.045942, acc.: 99.02%] [G loss: 8.919593]\n",
            "17739 [D loss: 0.030737, acc.: 100.00%] [G loss: 8.030181]\n",
            "17740 [D loss: 0.019297, acc.: 99.61%] [G loss: 8.497300]\n",
            "17741 [D loss: 0.059247, acc.: 97.66%] [G loss: 7.465829]\n",
            "17742 [D loss: 0.029417, acc.: 99.22%] [G loss: 7.683381]\n",
            "17743 [D loss: 0.036704, acc.: 100.00%] [G loss: 8.720825]\n",
            "17744 [D loss: 0.123541, acc.: 94.53%] [G loss: 9.330322]\n",
            "17745 [D loss: 0.196888, acc.: 93.95%] [G loss: 6.824172]\n",
            "17746 [D loss: 0.017856, acc.: 99.61%] [G loss: 7.297756]\n",
            "17747 [D loss: 0.072575, acc.: 97.46%] [G loss: 8.053832]\n",
            "17748 [D loss: 0.044376, acc.: 98.24%] [G loss: 6.457794]\n",
            "17749 [D loss: 0.007888, acc.: 100.00%] [G loss: 8.157672]\n",
            "17750 [D loss: 0.018830, acc.: 100.00%] [G loss: 6.491494]\n",
            "17751 [D loss: 0.013520, acc.: 99.80%] [G loss: 6.946146]\n",
            "17752 [D loss: 0.061667, acc.: 97.85%] [G loss: 5.558801]\n",
            "17753 [D loss: 0.063956, acc.: 97.66%] [G loss: 13.269704]\n",
            "17754 [D loss: 0.529958, acc.: 86.33%] [G loss: 10.040409]\n",
            "17755 [D loss: 0.435972, acc.: 85.16%] [G loss: 18.774824]\n",
            "17756 [D loss: 0.986050, acc.: 83.79%] [G loss: 5.847471]\n",
            "17757 [D loss: 1.050068, acc.: 85.74%] [G loss: 15.024292]\n",
            "17758 [D loss: 0.231152, acc.: 96.68%] [G loss: 16.964310]\n",
            "17759 [D loss: 0.534803, acc.: 93.36%] [G loss: 7.080054]\n",
            "17760 [D loss: 0.130805, acc.: 94.53%] [G loss: 14.348920]\n",
            "17761 [D loss: 0.056411, acc.: 98.63%] [G loss: 14.743610]\n",
            "17762 [D loss: 0.090711, acc.: 97.85%] [G loss: 13.208452]\n",
            "17763 [D loss: 0.009486, acc.: 100.00%] [G loss: 8.844319]\n",
            "17764 [D loss: 0.008050, acc.: 100.00%] [G loss: 9.277937]\n",
            "17765 [D loss: 0.005595, acc.: 100.00%] [G loss: 9.966015]\n",
            "17766 [D loss: 0.004220, acc.: 100.00%] [G loss: 9.481483]\n",
            "17767 [D loss: 0.007698, acc.: 99.80%] [G loss: 8.720409]\n",
            "17768 [D loss: 0.004417, acc.: 100.00%] [G loss: 8.516203]\n",
            "17769 [D loss: 0.014609, acc.: 100.00%] [G loss: 8.497767]\n",
            "17770 [D loss: 0.012490, acc.: 100.00%] [G loss: 8.020623]\n",
            "17771 [D loss: 0.015780, acc.: 99.80%] [G loss: 8.959283]\n",
            "17772 [D loss: 0.027544, acc.: 99.61%] [G loss: 7.937387]\n",
            "17773 [D loss: 0.013595, acc.: 99.80%] [G loss: 7.777638]\n",
            "17774 [D loss: 0.018708, acc.: 100.00%] [G loss: 8.316231]\n",
            "17775 [D loss: 0.065221, acc.: 97.66%] [G loss: 9.668617]\n",
            "17776 [D loss: 0.262417, acc.: 89.26%] [G loss: 11.814771]\n",
            "17777 [D loss: 0.231378, acc.: 96.09%] [G loss: 7.934255]\n",
            "17778 [D loss: 0.061482, acc.: 98.05%] [G loss: 7.881125]\n",
            "17779 [D loss: 0.017463, acc.: 99.80%] [G loss: 8.550888]\n",
            "17780 [D loss: 0.107837, acc.: 95.31%] [G loss: 9.160567]\n",
            "17781 [D loss: 0.028003, acc.: 99.61%] [G loss: 10.185743]\n",
            "17782 [D loss: 0.189587, acc.: 95.90%] [G loss: 6.643435]\n",
            "17783 [D loss: 0.050482, acc.: 97.46%] [G loss: 7.641335]\n",
            "17784 [D loss: 0.091764, acc.: 96.68%] [G loss: 9.154550]\n",
            "17785 [D loss: 0.046450, acc.: 98.24%] [G loss: 8.761168]\n",
            "17786 [D loss: 0.020888, acc.: 99.02%] [G loss: 10.393361]\n",
            "17787 [D loss: 0.015151, acc.: 99.80%] [G loss: 8.317843]\n",
            "17788 [D loss: 0.020540, acc.: 99.80%] [G loss: 8.979054]\n",
            "17789 [D loss: 0.026922, acc.: 99.61%] [G loss: 9.372647]\n",
            "17790 [D loss: 0.053760, acc.: 97.66%] [G loss: 7.814274]\n",
            "17791 [D loss: 0.008312, acc.: 100.00%] [G loss: 8.105993]\n",
            "17792 [D loss: 0.054378, acc.: 99.41%] [G loss: 10.423673]\n",
            "17793 [D loss: 0.191653, acc.: 93.16%] [G loss: 10.406860]\n",
            "17794 [D loss: 0.057659, acc.: 99.02%] [G loss: 10.406094]\n",
            "17795 [D loss: 0.164991, acc.: 94.34%] [G loss: 10.656866]\n",
            "17796 [D loss: 0.270136, acc.: 93.55%] [G loss: 8.503581]\n",
            "17797 [D loss: 0.044677, acc.: 99.02%] [G loss: 7.393958]\n",
            "17798 [D loss: 0.100768, acc.: 98.24%] [G loss: 10.597835]\n",
            "17799 [D loss: 0.066938, acc.: 96.88%] [G loss: 7.485461]\n",
            "17800 [D loss: 0.093586, acc.: 96.88%] [G loss: 8.154613]\n",
            "17801 [D loss: 0.007121, acc.: 99.80%] [G loss: 7.197312]\n",
            "17802 [D loss: 0.021644, acc.: 99.41%] [G loss: 7.481258]\n",
            "17803 [D loss: 0.039915, acc.: 99.02%] [G loss: 7.581492]\n",
            "17804 [D loss: 0.079416, acc.: 97.07%] [G loss: 8.699516]\n",
            "17805 [D loss: 0.239206, acc.: 92.58%] [G loss: 11.237714]\n",
            "17806 [D loss: 0.293161, acc.: 94.14%] [G loss: 5.590268]\n",
            "17807 [D loss: 0.251179, acc.: 92.97%] [G loss: 14.037462]\n",
            "17808 [D loss: 0.488286, acc.: 92.77%] [G loss: 6.970144]\n",
            "17809 [D loss: 0.093117, acc.: 98.05%] [G loss: 10.728417]\n",
            "17810 [D loss: 0.093487, acc.: 98.05%] [G loss: 8.653608]\n",
            "17811 [D loss: 0.447737, acc.: 79.69%] [G loss: 18.080484]\n",
            "17812 [D loss: 1.166500, acc.: 88.28%] [G loss: 10.405428]\n",
            "17813 [D loss: 0.983397, acc.: 89.45%] [G loss: 8.771342]\n",
            "17814 [D loss: 0.094151, acc.: 96.88%] [G loss: 10.485601]\n",
            "17815 [D loss: 0.035975, acc.: 98.24%] [G loss: 7.045296]\n",
            "17816 [D loss: 0.054805, acc.: 98.24%] [G loss: 9.045121]\n",
            "17817 [D loss: 0.006286, acc.: 100.00%] [G loss: 10.203137]\n",
            "17818 [D loss: 0.016559, acc.: 99.41%] [G loss: 8.896409]\n",
            "17819 [D loss: 0.034608, acc.: 99.02%] [G loss: 7.503697]\n",
            "17820 [D loss: 0.013550, acc.: 100.00%] [G loss: 7.690742]\n",
            "17821 [D loss: 0.019231, acc.: 99.80%] [G loss: 7.707611]\n",
            "17822 [D loss: 0.044034, acc.: 98.05%] [G loss: 7.753196]\n",
            "17823 [D loss: 0.007845, acc.: 100.00%] [G loss: 8.906281]\n",
            "17824 [D loss: 0.064362, acc.: 99.22%] [G loss: 8.769670]\n",
            "17825 [D loss: 0.027977, acc.: 99.61%] [G loss: 7.464075]\n",
            "17826 [D loss: 0.031587, acc.: 99.02%] [G loss: 7.974148]\n",
            "17827 [D loss: 0.016858, acc.: 99.41%] [G loss: 7.558237]\n",
            "17828 [D loss: 0.005457, acc.: 99.80%] [G loss: 10.868526]\n",
            "17829 [D loss: 0.012700, acc.: 100.00%] [G loss: 7.653893]\n",
            "17830 [D loss: 0.028867, acc.: 99.80%] [G loss: 8.039635]\n",
            "17831 [D loss: 0.099182, acc.: 96.29%] [G loss: 13.044317]\n",
            "17832 [D loss: 0.062126, acc.: 97.66%] [G loss: 13.868567]\n",
            "17833 [D loss: 0.029549, acc.: 99.02%] [G loss: 9.802306]\n",
            "17834 [D loss: 0.026086, acc.: 99.41%] [G loss: 10.334332]\n",
            "17835 [D loss: 0.005965, acc.: 100.00%] [G loss: 14.710925]\n",
            "17836 [D loss: 0.022458, acc.: 99.02%] [G loss: 8.566761]\n",
            "17837 [D loss: 0.011444, acc.: 99.80%] [G loss: 7.966397]\n",
            "17838 [D loss: 0.015446, acc.: 100.00%] [G loss: 7.114646]\n",
            "17839 [D loss: 0.035833, acc.: 98.83%] [G loss: 7.745296]\n",
            "17840 [D loss: 0.091664, acc.: 96.88%] [G loss: 8.383003]\n",
            "17841 [D loss: 0.091101, acc.: 97.66%] [G loss: 6.661816]\n",
            "17842 [D loss: 0.018884, acc.: 100.00%] [G loss: 7.051681]\n",
            "17843 [D loss: 0.057198, acc.: 98.44%] [G loss: 8.187320]\n",
            "17844 [D loss: 0.089786, acc.: 97.07%] [G loss: 5.584157]\n",
            "17845 [D loss: 0.104354, acc.: 97.66%] [G loss: 11.096115]\n",
            "17846 [D loss: 0.227609, acc.: 95.12%] [G loss: 6.430010]\n",
            "17847 [D loss: 0.099307, acc.: 97.07%] [G loss: 7.178658]\n",
            "17848 [D loss: 0.041108, acc.: 99.22%] [G loss: 6.102371]\n",
            "17849 [D loss: 0.092398, acc.: 97.27%] [G loss: 9.760515]\n",
            "17850 [D loss: 0.183618, acc.: 97.27%] [G loss: 6.993986]\n",
            "17851 [D loss: 0.048540, acc.: 99.02%] [G loss: 8.100679]\n",
            "17852 [D loss: 0.023780, acc.: 99.80%] [G loss: 6.958734]\n",
            "17853 [D loss: 0.024530, acc.: 98.83%] [G loss: 9.808008]\n",
            "17854 [D loss: 0.012715, acc.: 100.00%] [G loss: 10.105240]\n",
            "17855 [D loss: 0.003298, acc.: 100.00%] [G loss: 8.363805]\n",
            "17856 [D loss: 0.057156, acc.: 98.24%] [G loss: 10.428796]\n",
            "17857 [D loss: 0.130717, acc.: 98.44%] [G loss: 8.531593]\n",
            "17858 [D loss: 0.191835, acc.: 95.90%] [G loss: 8.573908]\n",
            "17859 [D loss: 0.046358, acc.: 96.68%] [G loss: 7.935570]\n",
            "17860 [D loss: 0.099564, acc.: 99.22%] [G loss: 9.058861]\n",
            "17861 [D loss: 0.008771, acc.: 99.80%] [G loss: 9.403595]\n",
            "17862 [D loss: 0.206011, acc.: 91.80%] [G loss: 14.693807]\n",
            "17863 [D loss: 0.512886, acc.: 90.62%] [G loss: 8.569814]\n",
            "17864 [D loss: 0.410880, acc.: 92.58%] [G loss: 7.221009]\n",
            "17865 [D loss: 0.090929, acc.: 96.88%] [G loss: 6.073542]\n",
            "17866 [D loss: 0.008759, acc.: 99.80%] [G loss: 7.987437]\n",
            "17867 [D loss: 0.014279, acc.: 100.00%] [G loss: 8.722102]\n",
            "17868 [D loss: 0.007341, acc.: 100.00%] [G loss: 6.863805]\n",
            "17869 [D loss: 0.009582, acc.: 100.00%] [G loss: 8.731206]\n",
            "17870 [D loss: 0.014262, acc.: 100.00%] [G loss: 7.683663]\n",
            "17871 [D loss: 0.016185, acc.: 100.00%] [G loss: 8.285097]\n",
            "17872 [D loss: 0.011433, acc.: 99.80%] [G loss: 7.181125]\n",
            "17873 [D loss: 0.098781, acc.: 96.88%] [G loss: 11.308406]\n",
            "17874 [D loss: 0.175490, acc.: 97.46%] [G loss: 8.264739]\n",
            "17875 [D loss: 0.036827, acc.: 99.22%] [G loss: 6.774501]\n",
            "17876 [D loss: 0.021005, acc.: 99.80%] [G loss: 7.346673]\n",
            "17877 [D loss: 0.104399, acc.: 95.90%] [G loss: 9.964375]\n",
            "17878 [D loss: 0.055344, acc.: 98.05%] [G loss: 6.518354]\n",
            "17879 [D loss: 0.008174, acc.: 100.00%] [G loss: 7.582458]\n",
            "17880 [D loss: 0.016275, acc.: 100.00%] [G loss: 7.604078]\n",
            "17881 [D loss: 0.016606, acc.: 99.22%] [G loss: 7.128145]\n",
            "17882 [D loss: 0.014541, acc.: 99.80%] [G loss: 6.791690]\n",
            "17883 [D loss: 0.042760, acc.: 98.63%] [G loss: 7.571070]\n",
            "17884 [D loss: 0.037922, acc.: 98.83%] [G loss: 6.277638]\n",
            "17885 [D loss: 0.013358, acc.: 100.00%] [G loss: 6.528444]\n",
            "17886 [D loss: 0.086032, acc.: 97.27%] [G loss: 9.529347]\n",
            "17887 [D loss: 0.130998, acc.: 96.48%] [G loss: 3.235939]\n",
            "17888 [D loss: 0.098986, acc.: 96.48%] [G loss: 13.182758]\n",
            "17889 [D loss: 0.028036, acc.: 99.22%] [G loss: 9.125085]\n",
            "17890 [D loss: 0.089443, acc.: 95.90%] [G loss: 8.703177]\n",
            "17891 [D loss: 0.012776, acc.: 99.80%] [G loss: 8.215166]\n",
            "17892 [D loss: 0.294771, acc.: 91.02%] [G loss: 21.928467]\n",
            "17893 [D loss: 0.532085, acc.: 91.21%] [G loss: 4.322056]\n",
            "17894 [D loss: 0.294906, acc.: 91.99%] [G loss: 14.756002]\n",
            "17895 [D loss: 0.362339, acc.: 95.70%] [G loss: 13.114981]\n",
            "17896 [D loss: 0.361767, acc.: 94.92%] [G loss: 7.047114]\n",
            "17897 [D loss: 0.063335, acc.: 98.83%] [G loss: 7.423734]\n",
            "17898 [D loss: 0.022717, acc.: 99.22%] [G loss: 7.137996]\n",
            "17899 [D loss: 0.032814, acc.: 99.22%] [G loss: 6.304139]\n",
            "17900 [D loss: 0.012406, acc.: 100.00%] [G loss: 6.709432]\n",
            "17901 [D loss: 0.011223, acc.: 99.61%] [G loss: 7.361798]\n",
            "17902 [D loss: 0.019379, acc.: 100.00%] [G loss: 6.384117]\n",
            "17903 [D loss: 0.032570, acc.: 100.00%] [G loss: 6.434017]\n",
            "17904 [D loss: 0.033889, acc.: 98.83%] [G loss: 8.063656]\n",
            "17905 [D loss: 0.019512, acc.: 98.83%] [G loss: 6.435311]\n",
            "17906 [D loss: 0.014651, acc.: 99.41%] [G loss: 7.925509]\n",
            "17907 [D loss: 0.020084, acc.: 99.41%] [G loss: 10.760324]\n",
            "17908 [D loss: 0.025669, acc.: 98.24%] [G loss: 7.666819]\n",
            "17909 [D loss: 0.009553, acc.: 99.61%] [G loss: 9.236958]\n",
            "17910 [D loss: 0.018083, acc.: 99.80%] [G loss: 8.334335]\n",
            "17911 [D loss: 0.104598, acc.: 95.31%] [G loss: 11.223635]\n",
            "17912 [D loss: 0.076429, acc.: 98.24%] [G loss: 6.049960]\n",
            "17913 [D loss: 0.011885, acc.: 100.00%] [G loss: 6.402831]\n",
            "17914 [D loss: 0.029866, acc.: 99.80%] [G loss: 7.900309]\n",
            "17915 [D loss: 0.030940, acc.: 98.83%] [G loss: 7.171897]\n",
            "17916 [D loss: 0.009438, acc.: 100.00%] [G loss: 7.255380]\n",
            "17917 [D loss: 0.048420, acc.: 99.61%] [G loss: 8.952483]\n",
            "17918 [D loss: 0.068887, acc.: 97.27%] [G loss: 7.440276]\n",
            "17919 [D loss: 0.011757, acc.: 100.00%] [G loss: 8.180358]\n",
            "17920 [D loss: 0.100509, acc.: 96.88%] [G loss: 9.725520]\n",
            "17921 [D loss: 0.118212, acc.: 97.66%] [G loss: 9.255619]\n",
            "17922 [D loss: 0.114652, acc.: 95.70%] [G loss: 8.849906]\n",
            "17923 [D loss: 0.054173, acc.: 98.83%] [G loss: 6.512528]\n",
            "17924 [D loss: 0.111882, acc.: 96.09%] [G loss: 14.548122]\n",
            "17925 [D loss: 0.535125, acc.: 92.19%] [G loss: 7.902283]\n",
            "17926 [D loss: 0.161241, acc.: 96.68%] [G loss: 7.040059]\n",
            "17927 [D loss: 0.058310, acc.: 98.44%] [G loss: 7.218488]\n",
            "17928 [D loss: 0.065620, acc.: 98.24%] [G loss: 5.841769]\n",
            "17929 [D loss: 0.017644, acc.: 99.22%] [G loss: 9.137444]\n",
            "17930 [D loss: 0.026333, acc.: 99.61%] [G loss: 6.751783]\n",
            "17931 [D loss: 0.066688, acc.: 97.66%] [G loss: 6.183658]\n",
            "17932 [D loss: 0.025043, acc.: 99.41%] [G loss: 8.088736]\n",
            "17933 [D loss: 0.027931, acc.: 99.61%] [G loss: 6.373237]\n",
            "17934 [D loss: 0.012790, acc.: 100.00%] [G loss: 6.951226]\n",
            "17935 [D loss: 0.022618, acc.: 99.61%] [G loss: 6.756341]\n",
            "17936 [D loss: 0.165928, acc.: 94.92%] [G loss: 14.014589]\n",
            "17937 [D loss: 0.378486, acc.: 95.90%] [G loss: 10.250530]\n",
            "17938 [D loss: 0.331919, acc.: 94.73%] [G loss: 4.492175]\n",
            "17939 [D loss: 0.031102, acc.: 99.02%] [G loss: 6.961287]\n",
            "17940 [D loss: 0.269817, acc.: 88.87%] [G loss: 16.789644]\n",
            "17941 [D loss: 0.640540, acc.: 92.77%] [G loss: 8.095490]\n",
            "17942 [D loss: 0.283621, acc.: 96.48%] [G loss: 6.329100]\n",
            "17943 [D loss: 0.129709, acc.: 98.05%] [G loss: 10.005656]\n",
            "17944 [D loss: 0.232667, acc.: 97.46%] [G loss: 9.036619]\n",
            "17945 [D loss: 0.266316, acc.: 92.58%] [G loss: 7.843785]\n",
            "17946 [D loss: 0.062678, acc.: 97.46%] [G loss: 6.347199]\n",
            "17947 [D loss: 0.191460, acc.: 94.53%] [G loss: 11.781303]\n",
            "17948 [D loss: 0.460435, acc.: 95.70%] [G loss: 6.867176]\n",
            "17949 [D loss: 0.070955, acc.: 98.05%] [G loss: 6.320362]\n",
            "17950 [D loss: 0.159909, acc.: 93.55%] [G loss: 10.822140]\n",
            "17951 [D loss: 0.240341, acc.: 97.66%] [G loss: 9.779238]\n",
            "17952 [D loss: 0.109829, acc.: 97.66%] [G loss: 6.903126]\n",
            "17953 [D loss: 0.032622, acc.: 100.00%] [G loss: 6.694639]\n",
            "17954 [D loss: 0.165107, acc.: 93.16%] [G loss: 9.377487]\n",
            "17955 [D loss: 0.077384, acc.: 98.05%] [G loss: 6.822039]\n",
            "17956 [D loss: 0.049197, acc.: 97.85%] [G loss: 6.802677]\n",
            "17957 [D loss: 0.021426, acc.: 99.41%] [G loss: 7.322085]\n",
            "17958 [D loss: 0.039531, acc.: 99.22%] [G loss: 7.575354]\n",
            "17959 [D loss: 0.010302, acc.: 100.00%] [G loss: 6.889527]\n",
            "17960 [D loss: 0.016614, acc.: 100.00%] [G loss: 7.758871]\n",
            "17961 [D loss: 0.011147, acc.: 100.00%] [G loss: 6.218293]\n",
            "17962 [D loss: 0.015143, acc.: 99.80%] [G loss: 5.950611]\n",
            "17963 [D loss: 0.026943, acc.: 99.80%] [G loss: 5.408878]\n",
            "17964 [D loss: 0.025901, acc.: 100.00%] [G loss: 6.441908]\n",
            "17965 [D loss: 0.035884, acc.: 100.00%] [G loss: 6.472212]\n",
            "17966 [D loss: 0.012636, acc.: 100.00%] [G loss: 9.574541]\n",
            "17967 [D loss: 0.013051, acc.: 100.00%] [G loss: 8.341614]\n",
            "17968 [D loss: 0.011580, acc.: 100.00%] [G loss: 6.300103]\n",
            "17969 [D loss: 0.007952, acc.: 100.00%] [G loss: 7.105538]\n",
            "17970 [D loss: 0.021611, acc.: 100.00%] [G loss: 5.509017]\n",
            "17971 [D loss: 0.020313, acc.: 100.00%] [G loss: 6.529060]\n",
            "17972 [D loss: 0.012184, acc.: 99.22%] [G loss: 5.958785]\n",
            "17973 [D loss: 0.100587, acc.: 97.27%] [G loss: 8.712540]\n",
            "17974 [D loss: 0.173179, acc.: 95.51%] [G loss: 4.540483]\n",
            "17975 [D loss: 0.043945, acc.: 99.41%] [G loss: 7.927075]\n",
            "17976 [D loss: 1.149069, acc.: 57.81%] [G loss: 34.828388]\n",
            "17977 [D loss: 3.093286, acc.: 82.03%] [G loss: 23.292187]\n",
            "17978 [D loss: 2.223633, acc.: 90.43%] [G loss: 13.458344]\n",
            "17979 [D loss: 0.614815, acc.: 95.51%] [G loss: 9.211657]\n",
            "17980 [D loss: 0.401790, acc.: 93.75%] [G loss: 7.280725]\n",
            "17981 [D loss: 0.161866, acc.: 97.27%] [G loss: 5.868684]\n",
            "17982 [D loss: 0.235340, acc.: 96.68%] [G loss: 6.472265]\n",
            "17983 [D loss: 0.189407, acc.: 97.27%] [G loss: 4.859173]\n",
            "17984 [D loss: 0.618966, acc.: 67.58%] [G loss: 16.444681]\n",
            "17985 [D loss: 0.499977, acc.: 96.29%] [G loss: 20.256117]\n",
            "17986 [D loss: 0.639131, acc.: 96.48%] [G loss: 14.488644]\n",
            "17987 [D loss: 0.845412, acc.: 94.53%] [G loss: 8.766263]\n",
            "17988 [D loss: 0.273844, acc.: 97.07%] [G loss: 6.597524]\n",
            "17989 [D loss: 0.085959, acc.: 98.63%] [G loss: 6.835034]\n",
            "17990 [D loss: 0.104410, acc.: 98.05%] [G loss: 6.185842]\n",
            "17991 [D loss: 0.214001, acc.: 94.34%] [G loss: 7.336352]\n",
            "17992 [D loss: 0.102326, acc.: 97.66%] [G loss: 6.405647]\n",
            "17993 [D loss: 0.256588, acc.: 92.77%] [G loss: 7.183991]\n",
            "17994 [D loss: 0.133172, acc.: 97.66%] [G loss: 5.911461]\n",
            "17995 [D loss: 0.198383, acc.: 94.53%] [G loss: 7.609105]\n",
            "17996 [D loss: 0.088439, acc.: 98.24%] [G loss: 8.143445]\n",
            "17997 [D loss: 0.168501, acc.: 97.07%] [G loss: 4.435552]\n",
            "17998 [D loss: 0.069278, acc.: 99.61%] [G loss: 6.886508]\n",
            "17999 [D loss: 0.103047, acc.: 96.09%] [G loss: 5.569879]\n",
            "18000 [D loss: 0.029459, acc.: 99.80%] [G loss: 5.844364]\n",
            "18001 [D loss: 0.045297, acc.: 98.44%] [G loss: 5.861762]\n",
            "18002 [D loss: 0.042293, acc.: 99.02%] [G loss: 5.488763]\n",
            "18003 [D loss: 0.019135, acc.: 100.00%] [G loss: 6.556958]\n",
            "18004 [D loss: 0.032438, acc.: 98.24%] [G loss: 5.758628]\n",
            "18005 [D loss: 0.057004, acc.: 99.02%] [G loss: 6.384908]\n",
            "18006 [D loss: 0.084705, acc.: 96.68%] [G loss: 5.224140]\n",
            "18007 [D loss: 0.022139, acc.: 99.80%] [G loss: 5.711589]\n",
            "18008 [D loss: 0.044013, acc.: 98.24%] [G loss: 5.352302]\n",
            "18009 [D loss: 0.149090, acc.: 95.12%] [G loss: 8.701551]\n",
            "18010 [D loss: 0.111292, acc.: 98.05%] [G loss: 8.196931]\n",
            "18011 [D loss: 0.205407, acc.: 95.31%] [G loss: 6.424683]\n",
            "18012 [D loss: 0.011023, acc.: 99.80%] [G loss: 6.856978]\n",
            "18013 [D loss: 0.030416, acc.: 99.61%] [G loss: 6.440622]\n",
            "18014 [D loss: 0.031628, acc.: 100.00%] [G loss: 5.146191]\n",
            "18015 [D loss: 0.023153, acc.: 100.00%] [G loss: 5.684618]\n",
            "18016 [D loss: 0.029837, acc.: 100.00%] [G loss: 5.621108]\n",
            "18017 [D loss: 0.019037, acc.: 99.61%] [G loss: 6.107197]\n",
            "18018 [D loss: 0.056553, acc.: 98.44%] [G loss: 4.579689]\n",
            "18019 [D loss: 0.024410, acc.: 100.00%] [G loss: 5.002143]\n",
            "18020 [D loss: 0.071598, acc.: 97.46%] [G loss: 5.697763]\n",
            "18021 [D loss: 0.053222, acc.: 97.46%] [G loss: 4.583881]\n",
            "18022 [D loss: 0.039383, acc.: 99.41%] [G loss: 5.442657]\n",
            "18023 [D loss: 0.047356, acc.: 98.05%] [G loss: 5.749343]\n",
            "18024 [D loss: 0.023317, acc.: 100.00%] [G loss: 5.486456]\n",
            "18025 [D loss: 0.017706, acc.: 100.00%] [G loss: 5.321129]\n",
            "18026 [D loss: 0.058542, acc.: 99.22%] [G loss: 6.249604]\n",
            "18027 [D loss: 0.053471, acc.: 97.46%] [G loss: 5.103913]\n",
            "18028 [D loss: 0.023946, acc.: 99.41%] [G loss: 6.375042]\n",
            "18029 [D loss: 0.055821, acc.: 98.83%] [G loss: 4.867894]\n",
            "18030 [D loss: 0.014294, acc.: 100.00%] [G loss: 8.014365]\n",
            "18031 [D loss: 0.039719, acc.: 98.83%] [G loss: 5.072195]\n",
            "18032 [D loss: 0.019375, acc.: 100.00%] [G loss: 6.334049]\n",
            "18033 [D loss: 0.147265, acc.: 94.92%] [G loss: 8.792491]\n",
            "18034 [D loss: 0.133647, acc.: 97.27%] [G loss: 8.230191]\n",
            "18035 [D loss: 0.107758, acc.: 97.46%] [G loss: 5.582921]\n",
            "18036 [D loss: 0.022614, acc.: 99.22%] [G loss: 6.434913]\n",
            "18037 [D loss: 0.029248, acc.: 99.61%] [G loss: 5.685362]\n",
            "18038 [D loss: 0.046558, acc.: 98.44%] [G loss: 5.407440]\n",
            "18039 [D loss: 0.028087, acc.: 99.61%] [G loss: 7.115069]\n",
            "18040 [D loss: 0.085047, acc.: 95.90%] [G loss: 6.613400]\n",
            "18041 [D loss: 0.012819, acc.: 100.00%] [G loss: 6.687033]\n",
            "18042 [D loss: 0.047423, acc.: 97.46%] [G loss: 5.405402]\n",
            "18043 [D loss: 0.025120, acc.: 100.00%] [G loss: 5.369195]\n",
            "18044 [D loss: 0.038432, acc.: 99.80%] [G loss: 6.543146]\n",
            "18045 [D loss: 0.036849, acc.: 100.00%] [G loss: 4.715880]\n",
            "18046 [D loss: 0.032043, acc.: 100.00%] [G loss: 6.624196]\n",
            "18047 [D loss: 0.046583, acc.: 97.85%] [G loss: 6.117632]\n",
            "18048 [D loss: 0.025453, acc.: 99.22%] [G loss: 5.735824]\n",
            "18049 [D loss: 0.021411, acc.: 100.00%] [G loss: 5.580874]\n",
            "18050 [D loss: 0.027215, acc.: 100.00%] [G loss: 5.864416]\n",
            "18051 [D loss: 0.057570, acc.: 98.44%] [G loss: 6.857477]\n",
            "18052 [D loss: 0.042366, acc.: 98.24%] [G loss: 5.368458]\n",
            "18053 [D loss: 0.034040, acc.: 99.80%] [G loss: 5.618021]\n",
            "18054 [D loss: 0.056714, acc.: 98.63%] [G loss: 5.574525]\n",
            "18055 [D loss: 0.039135, acc.: 99.02%] [G loss: 6.136302]\n",
            "18056 [D loss: 0.019081, acc.: 100.00%] [G loss: 5.841949]\n",
            "18057 [D loss: 0.017047, acc.: 99.80%] [G loss: 5.331012]\n",
            "18058 [D loss: 0.010597, acc.: 100.00%] [G loss: 5.630493]\n",
            "18059 [D loss: 0.035149, acc.: 99.61%] [G loss: 5.593357]\n",
            "18060 [D loss: 0.015966, acc.: 100.00%] [G loss: 8.212316]\n",
            "18061 [D loss: 0.031812, acc.: 99.80%] [G loss: 7.063213]\n",
            "18062 [D loss: 0.041542, acc.: 96.29%] [G loss: 11.027683]\n",
            "18063 [D loss: 0.022460, acc.: 100.00%] [G loss: 6.971868]\n",
            "18064 [D loss: 0.041763, acc.: 99.41%] [G loss: 4.389221]\n",
            "18065 [D loss: 0.014006, acc.: 99.80%] [G loss: 7.949152]\n",
            "18066 [D loss: 0.016658, acc.: 100.00%] [G loss: 6.085207]\n",
            "18067 [D loss: 0.025170, acc.: 100.00%] [G loss: 6.761798]\n",
            "18068 [D loss: 0.032794, acc.: 99.80%] [G loss: 6.605362]\n",
            "18069 [D loss: 0.041380, acc.: 98.24%] [G loss: 6.112260]\n",
            "18070 [D loss: 0.066151, acc.: 98.63%] [G loss: 8.236406]\n",
            "18071 [D loss: 0.141392, acc.: 96.09%] [G loss: 6.031960]\n",
            "18072 [D loss: 0.012775, acc.: 99.41%] [G loss: 7.818471]\n",
            "18073 [D loss: 0.058703, acc.: 98.44%] [G loss: 6.330765]\n",
            "18074 [D loss: 0.035777, acc.: 100.00%] [G loss: 6.121837]\n",
            "18075 [D loss: 0.178942, acc.: 92.97%] [G loss: 12.623280]\n",
            "18076 [D loss: 0.367367, acc.: 94.92%] [G loss: 7.247575]\n",
            "18077 [D loss: 0.088772, acc.: 97.27%] [G loss: 6.147609]\n",
            "18078 [D loss: 0.121050, acc.: 97.66%] [G loss: 7.157531]\n",
            "18079 [D loss: 0.071944, acc.: 97.27%] [G loss: 5.861766]\n",
            "18080 [D loss: 0.032412, acc.: 98.83%] [G loss: 6.440034]\n",
            "18081 [D loss: 0.018632, acc.: 100.00%] [G loss: 7.730528]\n",
            "18082 [D loss: 0.055177, acc.: 98.83%] [G loss: 7.385660]\n",
            "18083 [D loss: 0.156649, acc.: 95.12%] [G loss: 8.243818]\n",
            "18084 [D loss: 0.007160, acc.: 100.00%] [G loss: 7.190491]\n",
            "18085 [D loss: 0.010123, acc.: 99.80%] [G loss: 9.279058]\n",
            "18086 [D loss: 0.038171, acc.: 100.00%] [G loss: 7.181954]\n",
            "18087 [D loss: 0.042364, acc.: 98.05%] [G loss: 6.298919]\n",
            "18088 [D loss: 0.022189, acc.: 100.00%] [G loss: 5.504148]\n",
            "18089 [D loss: 0.033131, acc.: 98.63%] [G loss: 7.498814]\n",
            "18090 [D loss: 0.012010, acc.: 99.02%] [G loss: 8.038328]\n",
            "18091 [D loss: 0.026500, acc.: 99.41%] [G loss: 6.956282]\n",
            "18092 [D loss: 0.011513, acc.: 99.61%] [G loss: 6.241068]\n",
            "18093 [D loss: 0.074328, acc.: 98.63%] [G loss: 10.186076]\n",
            "18094 [D loss: 0.115301, acc.: 97.66%] [G loss: 7.874520]\n",
            "18095 [D loss: 0.040441, acc.: 98.83%] [G loss: 5.659962]\n",
            "18096 [D loss: 0.021850, acc.: 100.00%] [G loss: 6.040780]\n",
            "18097 [D loss: 0.076441, acc.: 96.88%] [G loss: 8.590563]\n",
            "18098 [D loss: 0.091522, acc.: 95.90%] [G loss: 6.164428]\n",
            "18099 [D loss: 0.019411, acc.: 99.61%] [G loss: 6.493356]\n",
            "18100 [D loss: 0.093329, acc.: 98.83%] [G loss: 6.072098]\n",
            "18101 [D loss: 0.064703, acc.: 98.44%] [G loss: 8.192540]\n",
            "18102 [D loss: 0.088637, acc.: 96.68%] [G loss: 9.867003]\n",
            "18103 [D loss: 0.275039, acc.: 94.73%] [G loss: 7.868934]\n",
            "18104 [D loss: 0.024949, acc.: 99.02%] [G loss: 7.971104]\n",
            "18105 [D loss: 0.095121, acc.: 97.66%] [G loss: 5.883467]\n",
            "18106 [D loss: 0.078768, acc.: 98.05%] [G loss: 8.609770]\n",
            "18107 [D loss: 0.094139, acc.: 96.88%] [G loss: 6.280849]\n",
            "18108 [D loss: 0.174682, acc.: 93.75%] [G loss: 12.764227]\n",
            "18109 [D loss: 0.279589, acc.: 96.29%] [G loss: 10.280371]\n",
            "18110 [D loss: 0.167238, acc.: 97.07%] [G loss: 8.786676]\n",
            "18111 [D loss: 0.028743, acc.: 98.83%] [G loss: 5.170185]\n",
            "18112 [D loss: 0.053361, acc.: 99.02%] [G loss: 7.238698]\n",
            "18113 [D loss: 0.039237, acc.: 98.24%] [G loss: 8.871030]\n",
            "18114 [D loss: 0.115319, acc.: 97.46%] [G loss: 12.459211]\n",
            "18115 [D loss: 0.147180, acc.: 97.46%] [G loss: 9.383758]\n",
            "18116 [D loss: 0.113433, acc.: 96.88%] [G loss: 6.808730]\n",
            "18117 [D loss: 0.036997, acc.: 99.22%] [G loss: 6.654406]\n",
            "18118 [D loss: 0.050747, acc.: 99.02%] [G loss: 7.567058]\n",
            "18119 [D loss: 0.101584, acc.: 96.48%] [G loss: 11.336874]\n",
            "18120 [D loss: 0.124341, acc.: 97.66%] [G loss: 7.342691]\n",
            "18121 [D loss: 0.032193, acc.: 99.41%] [G loss: 9.301981]\n",
            "18122 [D loss: 0.020533, acc.: 99.61%] [G loss: 6.702819]\n",
            "18123 [D loss: 0.136905, acc.: 96.48%] [G loss: 12.656010]\n",
            "18124 [D loss: 0.083310, acc.: 97.27%] [G loss: 6.676651]\n",
            "18125 [D loss: 0.014402, acc.: 99.61%] [G loss: 6.608693]\n",
            "18126 [D loss: 0.017036, acc.: 99.22%] [G loss: 11.461266]\n",
            "18127 [D loss: 0.020265, acc.: 100.00%] [G loss: 8.108747]\n",
            "18128 [D loss: 0.012462, acc.: 100.00%] [G loss: 7.730051]\n",
            "18129 [D loss: 0.050236, acc.: 98.63%] [G loss: 8.160325]\n",
            "18130 [D loss: 0.078342, acc.: 97.85%] [G loss: 6.299779]\n",
            "18131 [D loss: 0.036875, acc.: 99.41%] [G loss: 7.389711]\n",
            "18132 [D loss: 0.230742, acc.: 90.82%] [G loss: 14.144276]\n",
            "18133 [D loss: 0.275491, acc.: 96.68%] [G loss: 9.166200]\n",
            "18134 [D loss: 0.174865, acc.: 97.27%] [G loss: 5.232358]\n",
            "18135 [D loss: 0.096032, acc.: 97.46%] [G loss: 8.897560]\n",
            "18136 [D loss: 0.146440, acc.: 98.05%] [G loss: 7.905036]\n",
            "18137 [D loss: 0.140423, acc.: 96.29%] [G loss: 6.069954]\n",
            "18138 [D loss: 0.025613, acc.: 99.41%] [G loss: 7.493834]\n",
            "18139 [D loss: 0.166247, acc.: 96.09%] [G loss: 10.541353]\n",
            "18140 [D loss: 0.101732, acc.: 98.83%] [G loss: 11.635482]\n",
            "18141 [D loss: 0.278673, acc.: 96.88%] [G loss: 4.377310]\n",
            "18142 [D loss: 0.145196, acc.: 93.95%] [G loss: 11.342896]\n",
            "18143 [D loss: 0.255793, acc.: 97.46%] [G loss: 12.309122]\n",
            "18144 [D loss: 0.406275, acc.: 97.07%] [G loss: 5.426110]\n",
            "18145 [D loss: 0.342278, acc.: 84.57%] [G loss: 18.841385]\n",
            "18146 [D loss: 0.481680, acc.: 95.51%] [G loss: 17.898314]\n",
            "18147 [D loss: 0.680762, acc.: 95.31%] [G loss: 11.553433]\n",
            "18148 [D loss: 0.210877, acc.: 95.90%] [G loss: 6.866177]\n",
            "18149 [D loss: 0.162371, acc.: 95.31%] [G loss: 10.198603]\n",
            "18150 [D loss: 0.178495, acc.: 96.68%] [G loss: 6.778777]\n",
            "18151 [D loss: 0.214023, acc.: 93.16%] [G loss: 10.372189]\n",
            "18152 [D loss: 0.104673, acc.: 98.63%] [G loss: 11.699074]\n",
            "18153 [D loss: 0.159725, acc.: 98.44%] [G loss: 9.214199]\n",
            "18154 [D loss: 0.183151, acc.: 96.68%] [G loss: 5.659374]\n",
            "18155 [D loss: 0.069579, acc.: 98.83%] [G loss: 6.995043]\n",
            "18156 [D loss: 0.047087, acc.: 98.05%] [G loss: 6.312058]\n",
            "18157 [D loss: 0.089930, acc.: 97.07%] [G loss: 6.527456]\n",
            "18158 [D loss: 0.132908, acc.: 95.31%] [G loss: 7.403534]\n",
            "18159 [D loss: 0.052548, acc.: 97.66%] [G loss: 6.528251]\n",
            "18160 [D loss: 0.051880, acc.: 99.61%] [G loss: 6.242939]\n",
            "18161 [D loss: 0.026390, acc.: 98.83%] [G loss: 6.431851]\n",
            "18162 [D loss: 0.091732, acc.: 96.68%] [G loss: 9.400521]\n",
            "18163 [D loss: 0.180426, acc.: 96.48%] [G loss: 5.089051]\n",
            "18164 [D loss: 0.023503, acc.: 100.00%] [G loss: 5.836545]\n",
            "18165 [D loss: 0.096957, acc.: 97.85%] [G loss: 8.198699]\n",
            "18166 [D loss: 0.129847, acc.: 97.27%] [G loss: 6.204188]\n",
            "18167 [D loss: 0.208039, acc.: 90.23%] [G loss: 10.534468]\n",
            "18168 [D loss: 0.202849, acc.: 96.68%] [G loss: 10.346489]\n",
            "18169 [D loss: 0.196936, acc.: 96.88%] [G loss: 6.564305]\n",
            "18170 [D loss: 0.091046, acc.: 97.85%] [G loss: 5.685698]\n",
            "18171 [D loss: 0.052617, acc.: 97.27%] [G loss: 11.258525]\n",
            "18172 [D loss: 0.045727, acc.: 98.24%] [G loss: 6.013895]\n",
            "18173 [D loss: 0.033988, acc.: 100.00%] [G loss: 6.966903]\n",
            "18174 [D loss: 0.091304, acc.: 96.48%] [G loss: 5.757207]\n",
            "18175 [D loss: 0.052199, acc.: 100.00%] [G loss: 6.568040]\n",
            "18176 [D loss: 0.021627, acc.: 99.02%] [G loss: 6.169305]\n",
            "18177 [D loss: 0.080508, acc.: 98.05%] [G loss: 6.651556]\n",
            "18178 [D loss: 0.056066, acc.: 97.85%] [G loss: 5.413133]\n",
            "18179 [D loss: 0.054044, acc.: 98.05%] [G loss: 5.831702]\n",
            "18180 [D loss: 0.023897, acc.: 98.83%] [G loss: 6.111817]\n",
            "18181 [D loss: 0.039569, acc.: 98.63%] [G loss: 5.429885]\n",
            "18182 [D loss: 0.068630, acc.: 97.66%] [G loss: 6.291288]\n",
            "18183 [D loss: 0.052649, acc.: 97.85%] [G loss: 5.217088]\n",
            "18184 [D loss: 0.038604, acc.: 100.00%] [G loss: 5.729829]\n",
            "18185 [D loss: 0.044938, acc.: 98.05%] [G loss: 6.255637]\n",
            "18186 [D loss: 0.026597, acc.: 98.83%] [G loss: 7.107415]\n",
            "18187 [D loss: 0.013009, acc.: 100.00%] [G loss: 5.554535]\n",
            "18188 [D loss: 0.009962, acc.: 100.00%] [G loss: 6.931581]\n",
            "18189 [D loss: 0.031364, acc.: 100.00%] [G loss: 5.821555]\n",
            "18190 [D loss: 0.027525, acc.: 98.24%] [G loss: 5.649147]\n",
            "18191 [D loss: 0.049245, acc.: 100.00%] [G loss: 8.301793]\n",
            "18192 [D loss: 0.024163, acc.: 98.83%] [G loss: 8.786392]\n",
            "18193 [D loss: 0.029082, acc.: 98.24%] [G loss: 5.665006]\n",
            "18194 [D loss: 0.042437, acc.: 99.80%] [G loss: 7.069214]\n",
            "18195 [D loss: 0.146150, acc.: 95.70%] [G loss: 5.973484]\n",
            "18196 [D loss: 0.058040, acc.: 97.27%] [G loss: 8.055063]\n",
            "18197 [D loss: 0.125799, acc.: 96.88%] [G loss: 6.731508]\n",
            "18198 [D loss: 0.043964, acc.: 97.46%] [G loss: 4.403937]\n",
            "18199 [D loss: 0.024358, acc.: 100.00%] [G loss: 5.820091]\n",
            "18200 [D loss: 0.034845, acc.: 99.02%] [G loss: 5.608723]\n",
            "18201 [D loss: 0.084528, acc.: 97.85%] [G loss: 7.539563]\n",
            "18202 [D loss: 0.091035, acc.: 96.48%] [G loss: 5.716881]\n",
            "18203 [D loss: 0.084099, acc.: 98.63%] [G loss: 7.327693]\n",
            "18204 [D loss: 0.147940, acc.: 96.88%] [G loss: 4.552845]\n",
            "18205 [D loss: 0.127999, acc.: 97.85%] [G loss: 10.083160]\n",
            "18206 [D loss: 0.140773, acc.: 98.05%] [G loss: 12.077927]\n",
            "18207 [D loss: 0.209922, acc.: 96.48%] [G loss: 6.034142]\n",
            "18208 [D loss: 0.222051, acc.: 94.53%] [G loss: 10.080758]\n",
            "18209 [D loss: 0.143284, acc.: 98.24%] [G loss: 11.122396]\n",
            "18210 [D loss: 0.380325, acc.: 93.95%] [G loss: 4.326102]\n",
            "18211 [D loss: 0.206231, acc.: 94.53%] [G loss: 12.479779]\n",
            "18212 [D loss: 0.283572, acc.: 97.46%] [G loss: 12.119764]\n",
            "18213 [D loss: 0.266246, acc.: 97.46%] [G loss: 7.365035]\n",
            "18214 [D loss: 0.316253, acc.: 91.21%] [G loss: 10.285745]\n",
            "18215 [D loss: 0.273445, acc.: 96.68%] [G loss: 7.167868]\n",
            "18216 [D loss: 0.076808, acc.: 98.44%] [G loss: 4.846847]\n",
            "18217 [D loss: 0.071626, acc.: 99.61%] [G loss: 6.903232]\n",
            "18218 [D loss: 0.058732, acc.: 98.44%] [G loss: 6.820249]\n",
            "18219 [D loss: 0.061498, acc.: 98.44%] [G loss: 4.475689]\n",
            "18220 [D loss: 0.026481, acc.: 99.80%] [G loss: 5.728046]\n",
            "18221 [D loss: 0.024529, acc.: 98.83%] [G loss: 5.502031]\n",
            "18222 [D loss: 0.110300, acc.: 97.07%] [G loss: 7.191304]\n",
            "18223 [D loss: 0.053754, acc.: 98.24%] [G loss: 7.312635]\n",
            "18224 [D loss: 0.081310, acc.: 98.24%] [G loss: 4.656709]\n",
            "18225 [D loss: 0.058327, acc.: 100.00%] [G loss: 6.302031]\n",
            "18226 [D loss: 0.060947, acc.: 97.66%] [G loss: 5.714365]\n",
            "18227 [D loss: 0.128421, acc.: 95.31%] [G loss: 8.488819]\n",
            "18228 [D loss: 0.209288, acc.: 96.68%] [G loss: 5.846292]\n",
            "18229 [D loss: 0.072393, acc.: 98.63%] [G loss: 6.799749]\n",
            "18230 [D loss: 0.048625, acc.: 98.05%] [G loss: 5.524368]\n",
            "18231 [D loss: 0.060292, acc.: 98.05%] [G loss: 6.050996]\n",
            "18232 [D loss: 0.026913, acc.: 98.63%] [G loss: 6.684558]\n",
            "18233 [D loss: 0.050095, acc.: 98.05%] [G loss: 4.890511]\n",
            "18234 [D loss: 0.027193, acc.: 99.02%] [G loss: 5.362602]\n",
            "18235 [D loss: 0.039302, acc.: 98.24%] [G loss: 5.281717]\n",
            "18236 [D loss: 0.037974, acc.: 98.24%] [G loss: 5.931744]\n",
            "18237 [D loss: 0.035306, acc.: 97.46%] [G loss: 5.352314]\n",
            "18238 [D loss: 0.011237, acc.: 100.00%] [G loss: 6.947984]\n",
            "18239 [D loss: 0.039510, acc.: 100.00%] [G loss: 6.520726]\n",
            "18240 [D loss: 0.065053, acc.: 97.85%] [G loss: 4.682993]\n",
            "18241 [D loss: 0.029595, acc.: 99.61%] [G loss: 6.375792]\n",
            "18242 [D loss: 0.072444, acc.: 97.66%] [G loss: 5.417939]\n",
            "18243 [D loss: 0.027873, acc.: 98.83%] [G loss: 5.484694]\n",
            "18244 [D loss: 0.062840, acc.: 98.05%] [G loss: 7.121777]\n",
            "18245 [D loss: 0.171739, acc.: 94.92%] [G loss: 8.368606]\n",
            "18246 [D loss: 0.124728, acc.: 97.46%] [G loss: 5.541548]\n",
            "18247 [D loss: 0.091631, acc.: 96.88%] [G loss: 7.640685]\n",
            "18248 [D loss: 0.084146, acc.: 96.88%] [G loss: 5.052938]\n",
            "18249 [D loss: 0.005909, acc.: 100.00%] [G loss: 9.176518]\n",
            "18250 [D loss: 0.076256, acc.: 98.83%] [G loss: 8.059223]\n",
            "18251 [D loss: 0.093200, acc.: 97.46%] [G loss: 6.850145]\n",
            "18252 [D loss: 0.071152, acc.: 98.63%] [G loss: 8.084327]\n",
            "18253 [D loss: 0.074177, acc.: 96.88%] [G loss: 6.376449]\n",
            "18254 [D loss: 0.057337, acc.: 98.63%] [G loss: 6.602348]\n",
            "18255 [D loss: 0.052862, acc.: 98.44%] [G loss: 5.209579]\n",
            "18256 [D loss: 0.072440, acc.: 98.83%] [G loss: 7.614234]\n",
            "18257 [D loss: 0.061289, acc.: 98.44%] [G loss: 6.562171]\n",
            "18258 [D loss: 0.058060, acc.: 98.24%] [G loss: 5.534483]\n",
            "18259 [D loss: 0.021626, acc.: 100.00%] [G loss: 7.121700]\n",
            "18260 [D loss: 0.104827, acc.: 97.46%] [G loss: 9.667014]\n",
            "18261 [D loss: 0.243270, acc.: 95.90%] [G loss: 7.190045]\n",
            "18262 [D loss: 0.080106, acc.: 97.85%] [G loss: 5.614421]\n",
            "18263 [D loss: 0.043974, acc.: 98.24%] [G loss: 5.555117]\n",
            "18264 [D loss: 0.063323, acc.: 98.24%] [G loss: 6.346376]\n",
            "18265 [D loss: 0.056931, acc.: 98.44%] [G loss: 4.851923]\n",
            "18266 [D loss: 0.085691, acc.: 97.85%] [G loss: 7.917839]\n",
            "18267 [D loss: 0.108985, acc.: 96.48%] [G loss: 4.364296]\n",
            "18268 [D loss: 0.045063, acc.: 99.22%] [G loss: 8.153688]\n",
            "18269 [D loss: 0.227856, acc.: 94.34%] [G loss: 10.134003]\n",
            "18270 [D loss: 0.369908, acc.: 94.92%] [G loss: 5.926500]\n",
            "18271 [D loss: 0.281126, acc.: 87.89%] [G loss: 11.437619]\n",
            "18272 [D loss: 0.385309, acc.: 95.31%] [G loss: 6.571115]\n",
            "18273 [D loss: 0.099700, acc.: 97.27%] [G loss: 6.432570]\n",
            "18274 [D loss: 0.197049, acc.: 92.77%] [G loss: 8.437947]\n",
            "18275 [D loss: 0.176034, acc.: 96.29%] [G loss: 4.833566]\n",
            "18276 [D loss: 0.158919, acc.: 95.70%] [G loss: 8.723824]\n",
            "18277 [D loss: 0.238684, acc.: 96.09%] [G loss: 7.578342]\n",
            "18278 [D loss: 0.148004, acc.: 97.07%] [G loss: 5.623855]\n",
            "18279 [D loss: 0.040904, acc.: 98.63%] [G loss: 6.527463]\n",
            "18280 [D loss: 0.124196, acc.: 96.68%] [G loss: 6.245113]\n",
            "18281 [D loss: 0.057641, acc.: 97.66%] [G loss: 5.118941]\n",
            "18282 [D loss: 0.103984, acc.: 97.46%] [G loss: 6.106218]\n",
            "18283 [D loss: 0.095160, acc.: 97.85%] [G loss: 4.971357]\n",
            "18284 [D loss: 0.049780, acc.: 98.63%] [G loss: 6.044924]\n",
            "18285 [D loss: 0.108172, acc.: 97.66%] [G loss: 5.742127]\n",
            "18286 [D loss: 0.063909, acc.: 98.44%] [G loss: 5.200715]\n",
            "18287 [D loss: 0.074571, acc.: 98.05%] [G loss: 5.517538]\n",
            "18288 [D loss: 0.041389, acc.: 98.63%] [G loss: 5.070356]\n",
            "18289 [D loss: 0.135499, acc.: 97.27%] [G loss: 8.124189]\n",
            "18290 [D loss: 0.202526, acc.: 95.70%] [G loss: 5.487419]\n",
            "18291 [D loss: 0.080351, acc.: 97.85%] [G loss: 5.682518]\n",
            "18292 [D loss: 0.131077, acc.: 97.07%] [G loss: 5.418004]\n",
            "18293 [D loss: 0.131244, acc.: 96.88%] [G loss: 5.852689]\n",
            "18294 [D loss: 0.089571, acc.: 97.85%] [G loss: 4.924486]\n",
            "18295 [D loss: 0.091143, acc.: 97.66%] [G loss: 6.238346]\n",
            "18296 [D loss: 0.050861, acc.: 98.44%] [G loss: 6.156515]\n",
            "18297 [D loss: 0.139075, acc.: 96.88%] [G loss: 11.537753]\n",
            "18298 [D loss: 0.194978, acc.: 96.29%] [G loss: 5.067157]\n",
            "18299 [D loss: 0.039094, acc.: 99.41%] [G loss: 6.392815]\n",
            "18300 [D loss: 0.033212, acc.: 100.00%] [G loss: 7.682370]\n",
            "18301 [D loss: 0.337427, acc.: 86.72%] [G loss: 14.275799]\n",
            "18302 [D loss: 0.337737, acc.: 96.29%] [G loss: 16.225264]\n",
            "18303 [D loss: 0.296653, acc.: 97.85%] [G loss: 10.882817]\n",
            "18304 [D loss: 0.147499, acc.: 98.44%] [G loss: 6.387957]\n",
            "18305 [D loss: 0.279658, acc.: 95.51%] [G loss: 6.985822]\n",
            "18306 [D loss: 0.063475, acc.: 98.63%] [G loss: 7.420305]\n",
            "18307 [D loss: 0.070835, acc.: 98.44%] [G loss: 6.265881]\n",
            "18308 [D loss: 0.065049, acc.: 98.44%] [G loss: 5.354511]\n",
            "18309 [D loss: 0.061610, acc.: 96.88%] [G loss: 6.022810]\n",
            "18310 [D loss: 0.284533, acc.: 95.12%] [G loss: 7.526887]\n",
            "18311 [D loss: 0.096511, acc.: 97.46%] [G loss: 7.087937]\n",
            "18312 [D loss: 0.091119, acc.: 97.27%] [G loss: 6.948271]\n",
            "18313 [D loss: 0.060110, acc.: 98.05%] [G loss: 7.804987]\n",
            "18314 [D loss: 0.080956, acc.: 97.27%] [G loss: 8.477564]\n",
            "18315 [D loss: 0.256487, acc.: 93.75%] [G loss: 9.902334]\n",
            "18316 [D loss: 0.129041, acc.: 98.05%] [G loss: 9.171724]\n",
            "18317 [D loss: 0.218268, acc.: 97.27%] [G loss: 4.635124]\n",
            "18318 [D loss: 0.067715, acc.: 99.61%] [G loss: 7.525784]\n",
            "18319 [D loss: 0.077314, acc.: 98.05%] [G loss: 6.682492]\n",
            "18320 [D loss: 0.108748, acc.: 97.27%] [G loss: 6.678991]\n",
            "18321 [D loss: 0.083783, acc.: 97.07%] [G loss: 5.266675]\n",
            "18322 [D loss: 0.073620, acc.: 98.05%] [G loss: 5.800879]\n",
            "18323 [D loss: 0.043927, acc.: 98.44%] [G loss: 5.695945]\n",
            "18324 [D loss: 0.119048, acc.: 97.66%] [G loss: 7.698394]\n",
            "18325 [D loss: 0.104182, acc.: 96.88%] [G loss: 5.430298]\n",
            "18326 [D loss: 0.038834, acc.: 98.63%] [G loss: 5.623407]\n",
            "18327 [D loss: 0.023902, acc.: 99.02%] [G loss: 7.012974]\n",
            "18328 [D loss: 0.070207, acc.: 98.05%] [G loss: 5.577316]\n",
            "18329 [D loss: 0.105897, acc.: 96.29%] [G loss: 6.741652]\n",
            "18330 [D loss: 0.070662, acc.: 98.05%] [G loss: 5.857591]\n",
            "18331 [D loss: 0.111483, acc.: 97.46%] [G loss: 7.042256]\n",
            "18332 [D loss: 0.043075, acc.: 98.44%] [G loss: 7.027056]\n",
            "18333 [D loss: 0.155336, acc.: 96.68%] [G loss: 5.937440]\n",
            "18334 [D loss: 0.071802, acc.: 97.07%] [G loss: 5.323990]\n",
            "18335 [D loss: 0.027701, acc.: 100.00%] [G loss: 5.534255]\n",
            "18336 [D loss: 0.041766, acc.: 98.24%] [G loss: 5.528188]\n",
            "18337 [D loss: 0.040905, acc.: 98.44%] [G loss: 5.512333]\n",
            "18338 [D loss: 0.029910, acc.: 98.44%] [G loss: 5.445935]\n",
            "18339 [D loss: 0.039165, acc.: 100.00%] [G loss: 5.450935]\n",
            "18340 [D loss: 0.073780, acc.: 97.27%] [G loss: 6.426253]\n",
            "18341 [D loss: 0.090778, acc.: 97.27%] [G loss: 5.844442]\n",
            "18342 [D loss: 0.074739, acc.: 96.88%] [G loss: 5.855988]\n",
            "18343 [D loss: 0.028049, acc.: 99.41%] [G loss: 5.291044]\n",
            "18344 [D loss: 0.110135, acc.: 97.27%] [G loss: 8.689119]\n",
            "18345 [D loss: 0.153550, acc.: 97.07%] [G loss: 6.771846]\n",
            "18346 [D loss: 0.124783, acc.: 96.48%] [G loss: 8.657200]\n",
            "18347 [D loss: 0.117701, acc.: 97.07%] [G loss: 6.171595]\n",
            "18348 [D loss: 0.081282, acc.: 97.46%] [G loss: 3.842949]\n",
            "18349 [D loss: 0.046859, acc.: 99.61%] [G loss: 6.449691]\n",
            "18350 [D loss: 0.113414, acc.: 97.07%] [G loss: 6.560493]\n",
            "18351 [D loss: 0.060476, acc.: 98.44%] [G loss: 7.637744]\n",
            "18352 [D loss: 0.099289, acc.: 96.68%] [G loss: 7.030595]\n",
            "18353 [D loss: 0.039809, acc.: 97.66%] [G loss: 6.067461]\n",
            "18354 [D loss: 0.085874, acc.: 98.05%] [G loss: 8.522318]\n",
            "18355 [D loss: 0.185801, acc.: 96.88%] [G loss: 5.867949]\n",
            "18356 [D loss: 0.190171, acc.: 91.02%] [G loss: 10.150999]\n",
            "18357 [D loss: 0.284083, acc.: 96.48%] [G loss: 8.665148]\n",
            "18358 [D loss: 0.187928, acc.: 97.07%] [G loss: 4.537250]\n",
            "18359 [D loss: 0.173124, acc.: 95.70%] [G loss: 9.545448]\n",
            "18360 [D loss: 0.128719, acc.: 97.27%] [G loss: 11.183716]\n",
            "18361 [D loss: 0.194692, acc.: 97.66%] [G loss: 8.139507]\n",
            "18362 [D loss: 0.118555, acc.: 97.27%] [G loss: 5.796320]\n",
            "18363 [D loss: 0.072270, acc.: 97.66%] [G loss: 8.534012]\n",
            "18364 [D loss: 0.106405, acc.: 96.68%] [G loss: 4.589477]\n",
            "18365 [D loss: 0.048281, acc.: 100.00%] [G loss: 6.344169]\n",
            "18366 [D loss: 0.078819, acc.: 97.66%] [G loss: 6.242762]\n",
            "18367 [D loss: 0.087052, acc.: 96.88%] [G loss: 5.815339]\n",
            "18368 [D loss: 0.033974, acc.: 98.44%] [G loss: 5.933256]\n",
            "18369 [D loss: 0.060150, acc.: 98.24%] [G loss: 6.022899]\n",
            "18370 [D loss: 0.054439, acc.: 98.24%] [G loss: 6.156902]\n",
            "18371 [D loss: 0.061785, acc.: 98.05%] [G loss: 6.386827]\n",
            "18372 [D loss: 0.044517, acc.: 98.83%] [G loss: 5.870546]\n",
            "18373 [D loss: 0.094082, acc.: 96.48%] [G loss: 6.715930]\n",
            "18374 [D loss: 0.229957, acc.: 94.14%] [G loss: 7.421699]\n",
            "18375 [D loss: 0.162281, acc.: 96.48%] [G loss: 5.056004]\n",
            "18376 [D loss: 0.154420, acc.: 96.68%] [G loss: 8.732794]\n",
            "18377 [D loss: 0.188102, acc.: 97.27%] [G loss: 6.927886]\n",
            "18378 [D loss: 0.186815, acc.: 96.29%] [G loss: 4.959280]\n",
            "18379 [D loss: 0.058883, acc.: 98.05%] [G loss: 5.520556]\n",
            "18380 [D loss: 0.062590, acc.: 97.85%] [G loss: 5.809590]\n",
            "18381 [D loss: 0.052135, acc.: 98.24%] [G loss: 4.861142]\n",
            "18382 [D loss: 0.029200, acc.: 98.83%] [G loss: 5.772150]\n",
            "18383 [D loss: 0.054187, acc.: 98.44%] [G loss: 6.029149]\n",
            "18384 [D loss: 0.046242, acc.: 98.24%] [G loss: 6.173081]\n",
            "18385 [D loss: 0.048547, acc.: 98.24%] [G loss: 5.858668]\n",
            "18386 [D loss: 0.045481, acc.: 100.00%] [G loss: 6.198844]\n",
            "18387 [D loss: 0.032695, acc.: 98.44%] [G loss: 6.323668]\n",
            "18388 [D loss: 0.082126, acc.: 97.85%] [G loss: 6.856849]\n",
            "18389 [D loss: 0.066742, acc.: 98.05%] [G loss: 5.735618]\n",
            "18390 [D loss: 0.030104, acc.: 98.83%] [G loss: 6.455235]\n",
            "18391 [D loss: 0.029436, acc.: 99.22%] [G loss: 6.081428]\n",
            "18392 [D loss: 0.023189, acc.: 99.02%] [G loss: 6.106722]\n",
            "18393 [D loss: 0.150252, acc.: 95.90%] [G loss: 10.357847]\n",
            "18394 [D loss: 0.283002, acc.: 95.70%] [G loss: 5.888340]\n",
            "18395 [D loss: 0.116798, acc.: 97.27%] [G loss: 5.888129]\n",
            "18396 [D loss: 0.069338, acc.: 97.66%] [G loss: 6.369093]\n",
            "18397 [D loss: 0.063446, acc.: 97.07%] [G loss: 6.189088]\n",
            "18398 [D loss: 0.090426, acc.: 96.68%] [G loss: 7.985418]\n",
            "18399 [D loss: 0.146676, acc.: 97.07%] [G loss: 5.049995]\n",
            "18400 [D loss: 0.096327, acc.: 98.24%] [G loss: 9.672434]\n",
            "18401 [D loss: 0.114862, acc.: 97.46%] [G loss: 6.175565]\n",
            "18402 [D loss: 0.214020, acc.: 94.53%] [G loss: 11.336037]\n",
            "18403 [D loss: 0.405445, acc.: 93.55%] [G loss: 7.200340]\n",
            "18404 [D loss: 0.152870, acc.: 97.07%] [G loss: 8.362103]\n",
            "18405 [D loss: 0.430506, acc.: 93.55%] [G loss: 7.002222]\n",
            "18406 [D loss: 0.165699, acc.: 96.88%] [G loss: 7.042644]\n",
            "18407 [D loss: 0.119554, acc.: 97.66%] [G loss: 6.589221]\n",
            "18408 [D loss: 0.114965, acc.: 97.85%] [G loss: 4.927377]\n",
            "18409 [D loss: 0.076097, acc.: 98.24%] [G loss: 7.192550]\n",
            "18410 [D loss: 0.068948, acc.: 98.44%] [G loss: 6.621156]\n",
            "18411 [D loss: 0.238032, acc.: 94.14%] [G loss: 8.488572]\n",
            "18412 [D loss: 0.246108, acc.: 95.51%] [G loss: 7.345480]\n",
            "18413 [D loss: 0.100047, acc.: 98.05%] [G loss: 6.028823]\n",
            "18414 [D loss: 0.134433, acc.: 96.88%] [G loss: 7.017972]\n",
            "18415 [D loss: 0.086238, acc.: 97.66%] [G loss: 4.986001]\n",
            "18416 [D loss: 0.066969, acc.: 98.44%] [G loss: 5.804576]\n",
            "18417 [D loss: 0.091525, acc.: 98.05%] [G loss: 6.351754]\n",
            "18418 [D loss: 0.078865, acc.: 97.27%] [G loss: 5.912344]\n",
            "18419 [D loss: 0.082743, acc.: 97.85%] [G loss: 5.420490]\n",
            "18420 [D loss: 0.122202, acc.: 96.88%] [G loss: 7.538126]\n",
            "18421 [D loss: 0.120452, acc.: 98.05%] [G loss: 5.837752]\n",
            "18422 [D loss: 0.131277, acc.: 96.68%] [G loss: 6.323085]\n",
            "18423 [D loss: 0.061094, acc.: 98.05%] [G loss: 5.154890]\n",
            "18424 [D loss: 0.108454, acc.: 97.85%] [G loss: 6.727358]\n",
            "18425 [D loss: 0.071696, acc.: 98.24%] [G loss: 5.608036]\n",
            "18426 [D loss: 0.121669, acc.: 97.46%] [G loss: 7.830394]\n",
            "18427 [D loss: 0.125791, acc.: 96.48%] [G loss: 4.989978]\n",
            "18428 [D loss: 0.035551, acc.: 98.44%] [G loss: 6.171715]\n",
            "18429 [D loss: 0.049590, acc.: 98.63%] [G loss: 5.896692]\n",
            "18430 [D loss: 0.091476, acc.: 97.27%] [G loss: 6.232702]\n",
            "18431 [D loss: 0.029314, acc.: 99.02%] [G loss: 6.722427]\n",
            "18432 [D loss: 0.399675, acc.: 75.98%] [G loss: 15.965178]\n",
            "18433 [D loss: 0.551960, acc.: 96.29%] [G loss: 17.511877]\n",
            "18434 [D loss: 0.696379, acc.: 96.09%] [G loss: 9.632442]\n",
            "18435 [D loss: 0.355792, acc.: 96.09%] [G loss: 6.019503]\n",
            "18436 [D loss: 0.195517, acc.: 96.48%] [G loss: 7.212653]\n",
            "18437 [D loss: 0.065815, acc.: 98.44%] [G loss: 6.698437]\n",
            "18438 [D loss: 0.202182, acc.: 96.88%] [G loss: 5.199226]\n",
            "18439 [D loss: 0.120744, acc.: 97.27%] [G loss: 4.575124]\n",
            "18440 [D loss: 0.140215, acc.: 96.29%] [G loss: 6.439066]\n",
            "18441 [D loss: 0.172427, acc.: 96.29%] [G loss: 7.672982]\n",
            "18442 [D loss: 0.085779, acc.: 98.24%] [G loss: 5.378068]\n",
            "18443 [D loss: 0.043508, acc.: 97.66%] [G loss: 4.492459]\n",
            "18444 [D loss: 0.039998, acc.: 99.41%] [G loss: 5.254577]\n",
            "18445 [D loss: 0.040030, acc.: 98.63%] [G loss: 5.115442]\n",
            "18446 [D loss: 0.039756, acc.: 98.63%] [G loss: 5.548779]\n",
            "18447 [D loss: 0.093083, acc.: 97.66%] [G loss: 5.667749]\n",
            "18448 [D loss: 0.093685, acc.: 97.46%] [G loss: 5.049071]\n",
            "18449 [D loss: 0.095111, acc.: 97.27%] [G loss: 5.597740]\n",
            "18450 [D loss: 0.067533, acc.: 97.66%] [G loss: 5.031346]\n",
            "18451 [D loss: 0.192998, acc.: 96.48%] [G loss: 7.072291]\n",
            "18452 [D loss: 0.136310, acc.: 95.90%] [G loss: 6.569288]\n",
            "18453 [D loss: 0.109683, acc.: 97.85%] [G loss: 4.646743]\n",
            "18454 [D loss: 0.066557, acc.: 97.66%] [G loss: 5.213581]\n",
            "18455 [D loss: 0.068039, acc.: 98.05%] [G loss: 4.848834]\n",
            "18456 [D loss: 0.090535, acc.: 98.05%] [G loss: 5.561650]\n",
            "18457 [D loss: 0.081521, acc.: 97.66%] [G loss: 4.733963]\n",
            "18458 [D loss: 0.076497, acc.: 98.24%] [G loss: 5.008615]\n",
            "18459 [D loss: 0.033023, acc.: 99.02%] [G loss: 6.026094]\n",
            "18460 [D loss: 0.066620, acc.: 98.44%] [G loss: 4.899701]\n",
            "18461 [D loss: 0.079869, acc.: 97.85%] [G loss: 5.063809]\n",
            "18462 [D loss: 0.061675, acc.: 98.24%] [G loss: 4.954098]\n",
            "18463 [D loss: 0.072170, acc.: 98.24%] [G loss: 5.336965]\n",
            "18464 [D loss: 0.081816, acc.: 97.66%] [G loss: 5.318553]\n",
            "18465 [D loss: 0.069181, acc.: 98.24%] [G loss: 5.150094]\n",
            "18466 [D loss: 0.061956, acc.: 98.63%] [G loss: 5.442235]\n",
            "18467 [D loss: 0.025531, acc.: 99.41%] [G loss: 5.476351]\n",
            "18468 [D loss: 0.222514, acc.: 95.70%] [G loss: 6.273418]\n",
            "18469 [D loss: 0.129348, acc.: 96.88%] [G loss: 4.876892]\n",
            "18470 [D loss: 0.127132, acc.: 96.88%] [G loss: 6.409657]\n",
            "18471 [D loss: 0.121085, acc.: 96.68%] [G loss: 6.843558]\n",
            "18472 [D loss: 0.115951, acc.: 95.90%] [G loss: 6.402258]\n",
            "18473 [D loss: 0.019443, acc.: 99.22%] [G loss: 10.000292]\n",
            "18474 [D loss: 0.181190, acc.: 96.68%] [G loss: 5.454419]\n",
            "18475 [D loss: 0.066011, acc.: 96.68%] [G loss: 5.292693]\n",
            "18476 [D loss: 0.069578, acc.: 97.07%] [G loss: 5.199237]\n",
            "18477 [D loss: 0.066528, acc.: 98.05%] [G loss: 5.607341]\n",
            "18478 [D loss: 0.175479, acc.: 92.38%] [G loss: 8.251415]\n",
            "18479 [D loss: 0.231208, acc.: 95.90%] [G loss: 8.514901]\n",
            "18480 [D loss: 0.167183, acc.: 97.46%] [G loss: 4.718579]\n",
            "18481 [D loss: 0.106991, acc.: 97.27%] [G loss: 7.260714]\n",
            "18482 [D loss: 0.119019, acc.: 97.27%] [G loss: 5.600091]\n",
            "18483 [D loss: 0.146503, acc.: 95.51%] [G loss: 6.925623]\n",
            "18484 [D loss: 0.058771, acc.: 97.07%] [G loss: 5.570919]\n",
            "18485 [D loss: 0.080938, acc.: 99.41%] [G loss: 6.709270]\n",
            "18486 [D loss: 0.060373, acc.: 98.24%] [G loss: 5.546331]\n",
            "18487 [D loss: 0.051189, acc.: 98.83%] [G loss: 6.631932]\n",
            "18488 [D loss: 0.080959, acc.: 98.44%] [G loss: 5.657071]\n",
            "18489 [D loss: 0.069086, acc.: 97.27%] [G loss: 5.343524]\n",
            "18490 [D loss: 0.037479, acc.: 99.61%] [G loss: 6.676970]\n",
            "18491 [D loss: 0.054560, acc.: 98.05%] [G loss: 5.237320]\n",
            "18492 [D loss: 0.056709, acc.: 98.44%] [G loss: 4.968651]\n",
            "18493 [D loss: 0.042133, acc.: 98.05%] [G loss: 6.678227]\n",
            "18494 [D loss: 0.068207, acc.: 97.46%] [G loss: 5.924811]\n",
            "18495 [D loss: 0.056662, acc.: 98.44%] [G loss: 5.878783]\n",
            "18496 [D loss: 0.173535, acc.: 94.14%] [G loss: 9.007862]\n",
            "18497 [D loss: 0.185585, acc.: 96.88%] [G loss: 8.028173]\n",
            "18498 [D loss: 0.166897, acc.: 96.88%] [G loss: 4.985166]\n",
            "18499 [D loss: 0.174435, acc.: 92.38%] [G loss: 10.069777]\n",
            "18500 [D loss: 0.207532, acc.: 96.68%] [G loss: 6.856037]\n",
            "18501 [D loss: 0.085557, acc.: 98.24%] [G loss: 4.715316]\n",
            "18502 [D loss: 0.081265, acc.: 96.88%] [G loss: 5.539742]\n",
            "18503 [D loss: 0.096587, acc.: 97.46%] [G loss: 4.814778]\n",
            "18504 [D loss: 0.042500, acc.: 98.24%] [G loss: 4.600518]\n",
            "18505 [D loss: 0.047711, acc.: 98.05%] [G loss: 5.113782]\n",
            "18506 [D loss: 0.044113, acc.: 98.44%] [G loss: 5.773791]\n",
            "18507 [D loss: 0.096370, acc.: 97.46%] [G loss: 7.041790]\n",
            "18508 [D loss: 0.090402, acc.: 97.07%] [G loss: 4.778230]\n",
            "18509 [D loss: 0.034140, acc.: 98.83%] [G loss: 6.036728]\n",
            "18510 [D loss: 0.048304, acc.: 98.44%] [G loss: 5.005563]\n",
            "18511 [D loss: 0.049954, acc.: 98.44%] [G loss: 6.458676]\n",
            "18512 [D loss: 0.033635, acc.: 99.02%] [G loss: 6.066439]\n",
            "18513 [D loss: 0.084450, acc.: 96.68%] [G loss: 6.591382]\n",
            "18514 [D loss: 0.170770, acc.: 94.73%] [G loss: 7.309446]\n",
            "18515 [D loss: 0.154254, acc.: 96.88%] [G loss: 5.563878]\n",
            "18516 [D loss: 0.082887, acc.: 97.46%] [G loss: 6.024128]\n",
            "18517 [D loss: 0.047378, acc.: 97.46%] [G loss: 6.767677]\n",
            "18518 [D loss: 0.072014, acc.: 97.85%] [G loss: 6.243508]\n",
            "18519 [D loss: 0.037389, acc.: 99.22%] [G loss: 7.399026]\n",
            "18520 [D loss: 0.081332, acc.: 98.24%] [G loss: 6.497306]\n",
            "18521 [D loss: 0.146831, acc.: 96.88%] [G loss: 7.240894]\n",
            "18522 [D loss: 0.057583, acc.: 98.63%] [G loss: 7.716850]\n",
            "18523 [D loss: 0.085772, acc.: 98.05%] [G loss: 8.774775]\n",
            "18524 [D loss: 0.119663, acc.: 97.27%] [G loss: 6.087781]\n",
            "18525 [D loss: 0.044661, acc.: 98.44%] [G loss: 6.146855]\n",
            "18526 [D loss: 0.082421, acc.: 97.85%] [G loss: 6.168523]\n",
            "18527 [D loss: 0.070667, acc.: 98.05%] [G loss: 6.154393]\n",
            "18528 [D loss: 0.177274, acc.: 95.31%] [G loss: 10.117887]\n",
            "18529 [D loss: 0.251035, acc.: 95.70%] [G loss: 5.759189]\n",
            "18530 [D loss: 0.088593, acc.: 97.07%] [G loss: 5.705785]\n",
            "18531 [D loss: 0.028379, acc.: 98.63%] [G loss: 6.898318]\n",
            "18532 [D loss: 0.184121, acc.: 96.09%] [G loss: 7.180784]\n",
            "18533 [D loss: 0.146075, acc.: 97.07%] [G loss: 6.379790]\n",
            "18534 [D loss: 0.059873, acc.: 97.27%] [G loss: 5.049422]\n",
            "18535 [D loss: 0.090198, acc.: 96.68%] [G loss: 5.760193]\n",
            "18536 [D loss: 0.043829, acc.: 98.44%] [G loss: 5.793858]\n",
            "18537 [D loss: 0.085630, acc.: 97.85%] [G loss: 4.672136]\n",
            "18538 [D loss: 0.114611, acc.: 96.88%] [G loss: 6.196534]\n",
            "18539 [D loss: 0.064008, acc.: 98.24%] [G loss: 6.126413]\n",
            "18540 [D loss: 0.068751, acc.: 98.44%] [G loss: 4.690553]\n",
            "18541 [D loss: 0.160411, acc.: 96.48%] [G loss: 8.165133]\n",
            "18542 [D loss: 0.201043, acc.: 96.48%] [G loss: 5.752419]\n",
            "18543 [D loss: 0.084033, acc.: 97.85%] [G loss: 4.481616]\n",
            "18544 [D loss: 0.164038, acc.: 95.90%] [G loss: 7.509787]\n",
            "18545 [D loss: 0.158022, acc.: 96.09%] [G loss: 6.932967]\n",
            "18546 [D loss: 0.153287, acc.: 96.88%] [G loss: 5.716542]\n",
            "18547 [D loss: 0.034178, acc.: 98.24%] [G loss: 5.327443]\n",
            "18548 [D loss: 0.060122, acc.: 97.85%] [G loss: 5.968963]\n",
            "18549 [D loss: 0.178944, acc.: 96.48%] [G loss: 6.878000]\n",
            "18550 [D loss: 0.082643, acc.: 97.85%] [G loss: 5.920661]\n",
            "18551 [D loss: 0.041553, acc.: 98.83%] [G loss: 5.287790]\n",
            "18552 [D loss: 0.065016, acc.: 98.05%] [G loss: 6.465717]\n",
            "18553 [D loss: 0.060005, acc.: 97.66%] [G loss: 5.081791]\n",
            "18554 [D loss: 0.048822, acc.: 98.24%] [G loss: 4.821496]\n",
            "18555 [D loss: 0.026729, acc.: 99.22%] [G loss: 5.083599]\n",
            "18556 [D loss: 0.249026, acc.: 93.55%] [G loss: 8.962704]\n",
            "18557 [D loss: 0.226654, acc.: 96.88%] [G loss: 7.848674]\n",
            "18558 [D loss: 0.221883, acc.: 96.29%] [G loss: 5.698845]\n",
            "18559 [D loss: 0.098965, acc.: 97.46%] [G loss: 4.620399]\n",
            "18560 [D loss: 0.061313, acc.: 98.83%] [G loss: 7.153366]\n",
            "18561 [D loss: 0.109881, acc.: 97.27%] [G loss: 5.451578]\n",
            "18562 [D loss: 0.054529, acc.: 98.05%] [G loss: 7.556854]\n",
            "18563 [D loss: 0.089151, acc.: 97.27%] [G loss: 6.330997]\n",
            "18564 [D loss: 0.031405, acc.: 98.44%] [G loss: 4.719594]\n",
            "18565 [D loss: 0.148486, acc.: 98.24%] [G loss: 8.876066]\n",
            "18566 [D loss: 0.110506, acc.: 98.24%] [G loss: 8.012557]\n",
            "18567 [D loss: 0.151283, acc.: 97.66%] [G loss: 5.357114]\n",
            "18568 [D loss: 0.118465, acc.: 97.46%] [G loss: 7.733918]\n",
            "18569 [D loss: 0.099297, acc.: 96.68%] [G loss: 5.945265]\n",
            "18570 [D loss: 0.048217, acc.: 98.44%] [G loss: 5.996016]\n",
            "18571 [D loss: 0.064080, acc.: 97.66%] [G loss: 7.595863]\n",
            "18572 [D loss: 0.045703, acc.: 98.24%] [G loss: 5.616505]\n",
            "18573 [D loss: 0.055947, acc.: 99.02%] [G loss: 5.891651]\n",
            "18574 [D loss: 0.227384, acc.: 95.70%] [G loss: 10.632139]\n",
            "18575 [D loss: 0.052152, acc.: 97.85%] [G loss: 16.078215]\n",
            "18576 [D loss: 0.326659, acc.: 94.73%] [G loss: 11.625343]\n",
            "18577 [D loss: 0.254202, acc.: 96.68%] [G loss: 10.169369]\n",
            "18578 [D loss: 0.190714, acc.: 96.88%] [G loss: 4.182111]\n",
            "18579 [D loss: 0.082330, acc.: 99.80%] [G loss: 5.666509]\n",
            "18580 [D loss: 0.063614, acc.: 97.46%] [G loss: 5.102331]\n",
            "18581 [D loss: 0.052623, acc.: 98.24%] [G loss: 4.195320]\n",
            "18582 [D loss: 0.051473, acc.: 99.22%] [G loss: 5.785240]\n",
            "18583 [D loss: 0.086815, acc.: 97.85%] [G loss: 4.617537]\n",
            "18584 [D loss: 0.079140, acc.: 98.05%] [G loss: 5.617970]\n",
            "18585 [D loss: 0.069643, acc.: 98.05%] [G loss: 5.651926]\n",
            "18586 [D loss: 0.117876, acc.: 97.27%] [G loss: 6.803281]\n",
            "18587 [D loss: 0.072192, acc.: 97.07%] [G loss: 5.566228]\n",
            "18588 [D loss: 0.153408, acc.: 97.27%] [G loss: 5.369724]\n",
            "18589 [D loss: 0.137481, acc.: 96.68%] [G loss: 5.146609]\n",
            "18590 [D loss: 0.058509, acc.: 98.05%] [G loss: 6.246473]\n",
            "18591 [D loss: 0.118312, acc.: 97.46%] [G loss: 5.159789]\n",
            "18592 [D loss: 0.083156, acc.: 97.85%] [G loss: 5.093410]\n",
            "18593 [D loss: 0.161470, acc.: 96.29%] [G loss: 6.370936]\n",
            "18594 [D loss: 0.068816, acc.: 98.44%] [G loss: 5.982783]\n",
            "18595 [D loss: 0.078066, acc.: 98.24%] [G loss: 5.040259]\n",
            "18596 [D loss: 0.200329, acc.: 96.09%] [G loss: 7.782798]\n",
            "18597 [D loss: 0.169829, acc.: 96.68%] [G loss: 6.382328]\n",
            "18598 [D loss: 0.106202, acc.: 97.85%] [G loss: 5.083652]\n",
            "18599 [D loss: 0.082659, acc.: 96.48%] [G loss: 5.872373]\n",
            "18600 [D loss: 0.094333, acc.: 97.85%] [G loss: 5.146407]\n",
            "18601 [D loss: 0.111591, acc.: 97.27%] [G loss: 5.483912]\n",
            "18602 [D loss: 0.090738, acc.: 97.46%] [G loss: 4.826158]\n",
            "18603 [D loss: 0.077081, acc.: 97.85%] [G loss: 5.256375]\n",
            "18604 [D loss: 0.119613, acc.: 96.88%] [G loss: 5.244819]\n",
            "18605 [D loss: 0.034054, acc.: 99.02%] [G loss: 5.496484]\n",
            "18606 [D loss: 0.118874, acc.: 97.66%] [G loss: 4.260829]\n",
            "18607 [D loss: 0.074200, acc.: 97.85%] [G loss: 5.082514]\n",
            "18608 [D loss: 0.079134, acc.: 97.46%] [G loss: 4.783837]\n",
            "18609 [D loss: 0.078092, acc.: 97.66%] [G loss: 4.770188]\n",
            "18610 [D loss: 0.167696, acc.: 96.29%] [G loss: 6.221416]\n",
            "18611 [D loss: 0.092366, acc.: 98.05%] [G loss: 5.628610]\n",
            "18612 [D loss: 0.067632, acc.: 98.63%] [G loss: 5.563665]\n",
            "18613 [D loss: 0.058180, acc.: 98.24%] [G loss: 5.499856]\n",
            "18614 [D loss: 0.083862, acc.: 97.07%] [G loss: 5.548338]\n",
            "18615 [D loss: 0.198254, acc.: 95.51%] [G loss: 7.892219]\n",
            "18616 [D loss: 0.102367, acc.: 98.24%] [G loss: 8.352180]\n",
            "18617 [D loss: 0.079189, acc.: 99.02%] [G loss: 6.161836]\n",
            "18618 [D loss: 0.118756, acc.: 97.27%] [G loss: 4.346015]\n",
            "18619 [D loss: 0.068845, acc.: 98.05%] [G loss: 5.813625]\n",
            "18620 [D loss: 0.043503, acc.: 98.05%] [G loss: 5.516398]\n",
            "18621 [D loss: 0.047433, acc.: 98.63%] [G loss: 5.035865]\n",
            "18622 [D loss: 0.066146, acc.: 97.85%] [G loss: 4.836054]\n",
            "18623 [D loss: 0.059492, acc.: 98.05%] [G loss: 5.103419]\n",
            "18624 [D loss: 0.038595, acc.: 98.63%] [G loss: 5.384688]\n",
            "18625 [D loss: 0.071317, acc.: 97.66%] [G loss: 6.401722]\n",
            "18626 [D loss: 0.097973, acc.: 97.46%] [G loss: 5.390888]\n",
            "18627 [D loss: 0.056859, acc.: 98.24%] [G loss: 5.385754]\n",
            "18628 [D loss: 0.060753, acc.: 98.44%] [G loss: 6.644984]\n",
            "18629 [D loss: 0.065880, acc.: 97.46%] [G loss: 6.708657]\n",
            "18630 [D loss: 0.022868, acc.: 100.00%] [G loss: 6.121118]\n",
            "18631 [D loss: 0.032169, acc.: 99.61%] [G loss: 6.558613]\n",
            "18632 [D loss: 0.053364, acc.: 99.02%] [G loss: 6.248296]\n",
            "18633 [D loss: 0.079043, acc.: 97.07%] [G loss: 6.940730]\n",
            "18634 [D loss: 0.107495, acc.: 99.22%] [G loss: 7.504874]\n",
            "18635 [D loss: 0.120542, acc.: 97.46%] [G loss: 7.449985]\n",
            "18636 [D loss: 0.180873, acc.: 96.88%] [G loss: 5.474031]\n",
            "18637 [D loss: 0.060579, acc.: 99.41%] [G loss: 7.279381]\n",
            "18638 [D loss: 0.244245, acc.: 94.73%] [G loss: 6.504071]\n",
            "18639 [D loss: 0.129065, acc.: 96.88%] [G loss: 9.943172]\n",
            "18640 [D loss: 0.165484, acc.: 96.88%] [G loss: 7.521438]\n",
            "18641 [D loss: 0.063719, acc.: 97.66%] [G loss: 6.324387]\n",
            "18642 [D loss: 0.217264, acc.: 91.60%] [G loss: 12.743271]\n",
            "18643 [D loss: 0.323581, acc.: 95.70%] [G loss: 9.793212]\n",
            "18644 [D loss: 0.323222, acc.: 95.51%] [G loss: 6.957342]\n",
            "18645 [D loss: 0.117729, acc.: 97.85%] [G loss: 4.812014]\n",
            "18646 [D loss: 0.170581, acc.: 93.75%] [G loss: 9.394904]\n",
            "18647 [D loss: 0.166363, acc.: 97.85%] [G loss: 10.030902]\n",
            "18648 [D loss: 0.176958, acc.: 96.88%] [G loss: 6.506525]\n",
            "18649 [D loss: 0.258221, acc.: 95.31%] [G loss: 5.744871]\n",
            "18650 [D loss: 0.084143, acc.: 97.46%] [G loss: 5.230929]\n",
            "18651 [D loss: 0.057882, acc.: 98.63%] [G loss: 4.958457]\n",
            "18652 [D loss: 0.079132, acc.: 97.66%] [G loss: 4.611184]\n",
            "18653 [D loss: 0.069722, acc.: 97.85%] [G loss: 5.290775]\n",
            "18654 [D loss: 0.055544, acc.: 98.63%] [G loss: 5.437093]\n",
            "18655 [D loss: 0.142398, acc.: 96.88%] [G loss: 5.856753]\n",
            "18656 [D loss: 0.082184, acc.: 98.05%] [G loss: 5.674190]\n",
            "18657 [D loss: 0.192528, acc.: 96.09%] [G loss: 6.609781]\n",
            "18658 [D loss: 0.053748, acc.: 98.44%] [G loss: 6.459152]\n",
            "18659 [D loss: 0.123518, acc.: 97.46%] [G loss: 4.700510]\n",
            "18660 [D loss: 0.127662, acc.: 97.07%] [G loss: 5.806441]\n",
            "18661 [D loss: 0.086788, acc.: 97.85%] [G loss: 5.295646]\n",
            "18662 [D loss: 0.083179, acc.: 98.44%] [G loss: 4.743991]\n",
            "18663 [D loss: 0.053467, acc.: 98.63%] [G loss: 4.905842]\n",
            "18664 [D loss: 0.072231, acc.: 98.24%] [G loss: 4.922943]\n",
            "18665 [D loss: 0.073289, acc.: 98.05%] [G loss: 4.596228]\n",
            "18666 [D loss: 0.100125, acc.: 97.46%] [G loss: 4.970504]\n",
            "18667 [D loss: 0.052650, acc.: 98.44%] [G loss: 5.068534]\n",
            "18668 [D loss: 0.104474, acc.: 97.46%] [G loss: 4.697535]\n",
            "18669 [D loss: 0.112748, acc.: 96.29%] [G loss: 5.185964]\n",
            "18670 [D loss: 0.080116, acc.: 98.24%] [G loss: 4.580111]\n",
            "18671 [D loss: 0.157686, acc.: 96.68%] [G loss: 5.132347]\n",
            "18672 [D loss: 0.112238, acc.: 97.27%] [G loss: 4.775949]\n",
            "18673 [D loss: 0.152196, acc.: 96.48%] [G loss: 6.016312]\n",
            "18674 [D loss: 0.094465, acc.: 98.05%] [G loss: 6.431396]\n",
            "18675 [D loss: 0.105952, acc.: 97.46%] [G loss: 4.423942]\n",
            "18676 [D loss: 0.119160, acc.: 96.68%] [G loss: 7.020878]\n",
            "18677 [D loss: 0.070497, acc.: 98.05%] [G loss: 5.926483]\n",
            "18678 [D loss: 0.089301, acc.: 98.24%] [G loss: 4.510507]\n",
            "18679 [D loss: 0.104978, acc.: 97.27%] [G loss: 5.128181]\n",
            "18680 [D loss: 0.063141, acc.: 98.05%] [G loss: 5.053279]\n",
            "18681 [D loss: 0.058144, acc.: 98.63%] [G loss: 5.507880]\n",
            "18682 [D loss: 0.053131, acc.: 98.83%] [G loss: 4.866254]\n",
            "18683 [D loss: 0.123994, acc.: 97.27%] [G loss: 5.286731]\n",
            "18684 [D loss: 0.097780, acc.: 97.46%] [G loss: 4.607405]\n",
            "18685 [D loss: 0.087649, acc.: 98.24%] [G loss: 5.620826]\n",
            "18686 [D loss: 0.137355, acc.: 97.07%] [G loss: 4.481378]\n",
            "18687 [D loss: 0.058329, acc.: 97.85%] [G loss: 5.062577]\n",
            "18688 [D loss: 0.126055, acc.: 97.27%] [G loss: 4.950090]\n",
            "18689 [D loss: 0.070630, acc.: 98.24%] [G loss: 4.598987]\n",
            "18690 [D loss: 0.108816, acc.: 97.85%] [G loss: 4.623950]\n",
            "18691 [D loss: 0.037388, acc.: 99.02%] [G loss: 5.161272]\n",
            "18692 [D loss: 0.093946, acc.: 98.05%] [G loss: 4.498173]\n",
            "18693 [D loss: 0.079972, acc.: 98.05%] [G loss: 4.921611]\n",
            "18694 [D loss: 0.077916, acc.: 98.05%] [G loss: 4.959321]\n",
            "18695 [D loss: 0.136877, acc.: 97.27%] [G loss: 5.046747]\n",
            "18696 [D loss: 0.094656, acc.: 97.85%] [G loss: 4.389552]\n",
            "18697 [D loss: 0.125958, acc.: 96.88%] [G loss: 5.193614]\n",
            "18698 [D loss: 0.112222, acc.: 97.07%] [G loss: 4.206493]\n",
            "18699 [D loss: 0.070876, acc.: 98.83%] [G loss: 4.828460]\n",
            "18700 [D loss: 0.066920, acc.: 98.24%] [G loss: 4.510055]\n",
            "18701 [D loss: 0.096349, acc.: 97.46%] [G loss: 4.724583]\n",
            "18702 [D loss: 0.120998, acc.: 96.88%] [G loss: 5.101196]\n",
            "18703 [D loss: 0.034298, acc.: 99.22%] [G loss: 5.572871]\n",
            "18704 [D loss: 0.099636, acc.: 97.85%] [G loss: 4.353991]\n",
            "18705 [D loss: 0.129253, acc.: 96.88%] [G loss: 6.297555]\n",
            "18706 [D loss: 0.090517, acc.: 98.05%] [G loss: 5.751403]\n",
            "18707 [D loss: 0.073990, acc.: 98.24%] [G loss: 4.432408]\n",
            "18708 [D loss: 0.067263, acc.: 98.44%] [G loss: 5.309884]\n",
            "18709 [D loss: 0.069362, acc.: 97.85%] [G loss: 5.082576]\n",
            "18710 [D loss: 0.074746, acc.: 97.07%] [G loss: 5.102542]\n",
            "18711 [D loss: 0.037787, acc.: 98.44%] [G loss: 6.430403]\n",
            "18712 [D loss: 0.115103, acc.: 98.05%] [G loss: 7.154368]\n",
            "18713 [D loss: 0.115478, acc.: 97.46%] [G loss: 6.304286]\n",
            "18714 [D loss: 0.191470, acc.: 96.29%] [G loss: 4.134798]\n",
            "18715 [D loss: 0.097093, acc.: 97.07%] [G loss: 6.313244]\n",
            "18716 [D loss: 0.090350, acc.: 97.85%] [G loss: 5.112054]\n",
            "18717 [D loss: 0.129098, acc.: 97.07%] [G loss: 6.039473]\n",
            "18718 [D loss: 0.141371, acc.: 96.29%] [G loss: 4.225184]\n",
            "18719 [D loss: 0.037366, acc.: 98.83%] [G loss: 4.993027]\n",
            "18720 [D loss: 0.095210, acc.: 97.85%] [G loss: 5.174210]\n",
            "18721 [D loss: 0.056728, acc.: 98.44%] [G loss: 6.646410]\n",
            "18722 [D loss: 0.037720, acc.: 99.02%] [G loss: 5.372205]\n",
            "18723 [D loss: 0.068532, acc.: 97.46%] [G loss: 5.204307]\n",
            "18724 [D loss: 0.016025, acc.: 99.61%] [G loss: 5.614954]\n",
            "18725 [D loss: 0.113933, acc.: 97.66%] [G loss: 8.839922]\n",
            "18726 [D loss: 0.066939, acc.: 97.66%] [G loss: 4.675357]\n",
            "18727 [D loss: 0.049959, acc.: 98.05%] [G loss: 5.782094]\n",
            "18728 [D loss: 0.022156, acc.: 99.80%] [G loss: 6.692680]\n",
            "18729 [D loss: 0.065296, acc.: 98.24%] [G loss: 6.833777]\n",
            "18730 [D loss: 0.187817, acc.: 96.88%] [G loss: 8.140647]\n",
            "18731 [D loss: 0.139329, acc.: 97.27%] [G loss: 7.857122]\n",
            "18732 [D loss: 0.121944, acc.: 97.85%] [G loss: 4.932287]\n",
            "18733 [D loss: 0.044524, acc.: 99.22%] [G loss: 5.624182]\n",
            "18734 [D loss: 0.091099, acc.: 97.46%] [G loss: 5.130829]\n",
            "18735 [D loss: 0.056849, acc.: 98.63%] [G loss: 4.941343]\n",
            "18736 [D loss: 0.113133, acc.: 97.46%] [G loss: 4.612460]\n",
            "18737 [D loss: 0.058877, acc.: 98.24%] [G loss: 4.896722]\n",
            "18738 [D loss: 0.096431, acc.: 97.85%] [G loss: 4.811581]\n",
            "18739 [D loss: 0.107028, acc.: 97.46%] [G loss: 4.959850]\n",
            "18740 [D loss: 0.056517, acc.: 98.44%] [G loss: 4.889442]\n",
            "18741 [D loss: 0.114976, acc.: 97.85%] [G loss: 5.422565]\n",
            "18742 [D loss: 0.058314, acc.: 98.44%] [G loss: 4.895361]\n",
            "18743 [D loss: 0.080985, acc.: 98.05%] [G loss: 4.572285]\n",
            "18744 [D loss: 0.054875, acc.: 98.63%] [G loss: 4.589741]\n",
            "18745 [D loss: 0.089828, acc.: 97.66%] [G loss: 4.254230]\n",
            "18746 [D loss: 0.055386, acc.: 98.63%] [G loss: 4.766510]\n",
            "18747 [D loss: 0.113176, acc.: 97.46%] [G loss: 4.739219]\n",
            "18748 [D loss: 0.158033, acc.: 96.48%] [G loss: 5.036644]\n",
            "18749 [D loss: 0.112019, acc.: 96.88%] [G loss: 4.506267]\n",
            "18750 [D loss: 0.161983, acc.: 96.29%] [G loss: 5.651911]\n",
            "18751 [D loss: 0.139232, acc.: 97.07%] [G loss: 4.580323]\n",
            "18752 [D loss: 0.102207, acc.: 97.66%] [G loss: 5.226994]\n",
            "18753 [D loss: 0.117566, acc.: 96.68%] [G loss: 4.312197]\n",
            "18754 [D loss: 0.052723, acc.: 98.83%] [G loss: 4.762534]\n",
            "18755 [D loss: 0.083380, acc.: 97.85%] [G loss: 4.889234]\n",
            "18756 [D loss: 0.115282, acc.: 97.46%] [G loss: 5.002385]\n",
            "18757 [D loss: 0.051300, acc.: 98.44%] [G loss: 5.140002]\n",
            "18758 [D loss: 0.161072, acc.: 97.07%] [G loss: 5.421589]\n",
            "18759 [D loss: 0.147501, acc.: 96.68%] [G loss: 5.836893]\n",
            "18760 [D loss: 0.056228, acc.: 98.24%] [G loss: 5.368510]\n",
            "18761 [D loss: 0.083053, acc.: 97.85%] [G loss: 13.026461]\n",
            "18762 [D loss: 0.104454, acc.: 97.46%] [G loss: 6.563922]\n",
            "18763 [D loss: 0.071707, acc.: 97.85%] [G loss: 8.542325]\n",
            "18764 [D loss: 0.050675, acc.: 98.24%] [G loss: 6.187823]\n",
            "18765 [D loss: 0.078589, acc.: 97.46%] [G loss: 5.577593]\n",
            "18766 [D loss: 0.076855, acc.: 96.29%] [G loss: 7.418612]\n",
            "18767 [D loss: 0.079151, acc.: 97.46%] [G loss: 5.364701]\n",
            "18768 [D loss: 0.073185, acc.: 97.85%] [G loss: 7.541173]\n",
            "18769 [D loss: 0.045653, acc.: 97.85%] [G loss: 5.620120]\n",
            "18770 [D loss: 0.028598, acc.: 99.02%] [G loss: 6.190925]\n",
            "18771 [D loss: 0.064094, acc.: 97.66%] [G loss: 5.542431]\n",
            "18772 [D loss: 0.054285, acc.: 98.24%] [G loss: 5.951304]\n",
            "18773 [D loss: 0.088392, acc.: 97.66%] [G loss: 7.846735]\n",
            "18774 [D loss: 0.044700, acc.: 99.02%] [G loss: 5.528907]\n",
            "18775 [D loss: 0.096297, acc.: 97.07%] [G loss: 4.554897]\n",
            "18776 [D loss: 0.101274, acc.: 96.88%] [G loss: 8.260222]\n",
            "18777 [D loss: 0.102147, acc.: 97.85%] [G loss: 5.712184]\n",
            "18778 [D loss: 0.078561, acc.: 98.24%] [G loss: 7.472938]\n",
            "18779 [D loss: 0.067740, acc.: 98.05%] [G loss: 5.392051]\n",
            "18780 [D loss: 0.077179, acc.: 97.27%] [G loss: 4.954655]\n",
            "18781 [D loss: 0.053457, acc.: 98.44%] [G loss: 5.879919]\n",
            "18782 [D loss: 0.109874, acc.: 97.46%] [G loss: 5.231076]\n",
            "18783 [D loss: 0.182393, acc.: 95.12%] [G loss: 10.485253]\n",
            "18784 [D loss: 0.266105, acc.: 94.34%] [G loss: 6.031234]\n",
            "18785 [D loss: 0.207004, acc.: 95.90%] [G loss: 5.048699]\n",
            "18786 [D loss: 0.055762, acc.: 98.44%] [G loss: 5.299853]\n",
            "18787 [D loss: 0.089455, acc.: 97.66%] [G loss: 5.232626]\n",
            "18788 [D loss: 0.075370, acc.: 97.27%] [G loss: 5.063080]\n",
            "18789 [D loss: 0.063006, acc.: 98.44%] [G loss: 5.128216]\n",
            "18790 [D loss: 0.161390, acc.: 97.07%] [G loss: 6.915435]\n",
            "18791 [D loss: 0.044213, acc.: 99.22%] [G loss: 6.881991]\n",
            "18792 [D loss: 0.055287, acc.: 98.63%] [G loss: 5.224068]\n",
            "18793 [D loss: 0.115860, acc.: 97.27%] [G loss: 4.735860]\n",
            "18794 [D loss: 0.101352, acc.: 97.66%] [G loss: 4.938873]\n",
            "18795 [D loss: 0.169162, acc.: 96.29%] [G loss: 5.999760]\n",
            "18796 [D loss: 0.126962, acc.: 97.07%] [G loss: 4.872972]\n",
            "18797 [D loss: 0.099875, acc.: 98.05%] [G loss: 5.027810]\n",
            "18798 [D loss: 0.042529, acc.: 99.22%] [G loss: 5.521530]\n",
            "18799 [D loss: 0.320120, acc.: 95.12%] [G loss: 8.333149]\n",
            "18800 [D loss: 0.118574, acc.: 98.24%] [G loss: 9.482645]\n",
            "18801 [D loss: 0.173139, acc.: 97.85%] [G loss: 5.694922]\n",
            "18802 [D loss: 0.102028, acc.: 98.05%] [G loss: 4.133543]\n",
            "18803 [D loss: 0.059806, acc.: 98.63%] [G loss: 4.992226]\n",
            "18804 [D loss: 0.045664, acc.: 98.63%] [G loss: 4.833325]\n",
            "18805 [D loss: 0.055180, acc.: 98.83%] [G loss: 4.493459]\n",
            "18806 [D loss: 0.084157, acc.: 98.24%] [G loss: 4.569952]\n",
            "18807 [D loss: 0.097951, acc.: 97.66%] [G loss: 4.367897]\n",
            "18808 [D loss: 0.098989, acc.: 97.66%] [G loss: 4.384912]\n",
            "18809 [D loss: 0.089491, acc.: 97.85%] [G loss: 4.232301]\n",
            "18810 [D loss: 0.074007, acc.: 98.05%] [G loss: 4.607684]\n",
            "18811 [D loss: 0.049045, acc.: 98.63%] [G loss: 4.752244]\n",
            "18812 [D loss: 0.074397, acc.: 98.24%] [G loss: 4.489917]\n",
            "18813 [D loss: 0.036194, acc.: 99.02%] [G loss: 4.764496]\n",
            "18814 [D loss: 0.044159, acc.: 99.02%] [G loss: 4.889112]\n",
            "18815 [D loss: 0.051526, acc.: 98.83%] [G loss: 4.611672]\n",
            "18816 [D loss: 0.134042, acc.: 97.07%] [G loss: 4.742331]\n",
            "18817 [D loss: 0.094734, acc.: 97.66%] [G loss: 4.470769]\n",
            "18818 [D loss: 0.074460, acc.: 98.24%] [G loss: 4.478631]\n",
            "18819 [D loss: 0.145374, acc.: 96.68%] [G loss: 4.953289]\n",
            "18820 [D loss: 0.110504, acc.: 96.88%] [G loss: 4.600973]\n",
            "18821 [D loss: 0.116915, acc.: 97.46%] [G loss: 4.799186]\n",
            "18822 [D loss: 0.040561, acc.: 99.02%] [G loss: 5.213097]\n",
            "18823 [D loss: 0.111707, acc.: 97.46%] [G loss: 4.174579]\n",
            "18824 [D loss: 0.097437, acc.: 97.66%] [G loss: 4.531080]\n",
            "18825 [D loss: 0.063871, acc.: 98.44%] [G loss: 4.492838]\n",
            "18826 [D loss: 0.107067, acc.: 97.66%] [G loss: 4.590415]\n",
            "18827 [D loss: 0.122023, acc.: 97.07%] [G loss: 4.459642]\n",
            "18828 [D loss: 0.072493, acc.: 98.24%] [G loss: 4.415087]\n",
            "18829 [D loss: 0.127632, acc.: 97.27%] [G loss: 4.804576]\n",
            "18830 [D loss: 0.096791, acc.: 97.46%] [G loss: 4.825866]\n",
            "18831 [D loss: 0.091144, acc.: 97.85%] [G loss: 4.480863]\n",
            "18832 [D loss: 0.136028, acc.: 97.07%] [G loss: 4.880526]\n",
            "18833 [D loss: 0.125552, acc.: 96.88%] [G loss: 4.120416]\n",
            "18834 [D loss: 0.052504, acc.: 98.63%] [G loss: 4.684657]\n",
            "18835 [D loss: 0.097134, acc.: 97.66%] [G loss: 4.369920]\n",
            "18836 [D loss: 0.092538, acc.: 97.66%] [G loss: 4.708655]\n",
            "18837 [D loss: 0.086812, acc.: 97.85%] [G loss: 4.891962]\n",
            "18838 [D loss: 0.056434, acc.: 97.85%] [G loss: 4.604092]\n",
            "18839 [D loss: 0.058598, acc.: 98.44%] [G loss: 5.055767]\n",
            "18840 [D loss: 0.076769, acc.: 98.24%] [G loss: 4.529345]\n",
            "18841 [D loss: 0.125010, acc.: 97.07%] [G loss: 4.676509]\n",
            "18842 [D loss: 0.068032, acc.: 98.24%] [G loss: 4.790152]\n",
            "18843 [D loss: 0.099351, acc.: 98.24%] [G loss: 5.063158]\n",
            "18844 [D loss: 0.070892, acc.: 98.05%] [G loss: 4.549689]\n",
            "18845 [D loss: 0.127791, acc.: 96.88%] [G loss: 4.501507]\n",
            "18846 [D loss: 0.101973, acc.: 97.27%] [G loss: 4.857815]\n",
            "18847 [D loss: 0.096038, acc.: 97.85%] [G loss: 4.987553]\n",
            "18848 [D loss: 0.079295, acc.: 98.05%] [G loss: 4.525151]\n",
            "18849 [D loss: 0.062225, acc.: 98.44%] [G loss: 4.685576]\n",
            "18850 [D loss: 0.060127, acc.: 98.63%] [G loss: 4.997736]\n",
            "18851 [D loss: 0.132226, acc.: 97.27%] [G loss: 4.554372]\n",
            "18852 [D loss: 0.048106, acc.: 98.83%] [G loss: 4.975373]\n",
            "18853 [D loss: 0.044747, acc.: 98.83%] [G loss: 4.789366]\n",
            "18854 [D loss: 0.070593, acc.: 98.44%] [G loss: 4.615587]\n",
            "18855 [D loss: 0.080090, acc.: 98.24%] [G loss: 4.647227]\n",
            "18856 [D loss: 0.102852, acc.: 97.66%] [G loss: 4.560815]\n",
            "18857 [D loss: 0.047546, acc.: 98.83%] [G loss: 4.766503]\n",
            "18858 [D loss: 0.069135, acc.: 98.44%] [G loss: 4.408508]\n",
            "18859 [D loss: 0.114818, acc.: 97.27%] [G loss: 4.728034]\n",
            "18860 [D loss: 0.034534, acc.: 99.22%] [G loss: 5.294210]\n",
            "18861 [D loss: 0.113722, acc.: 97.85%] [G loss: 4.433853]\n",
            "18862 [D loss: 0.077891, acc.: 97.66%] [G loss: 5.940491]\n",
            "18863 [D loss: 0.129560, acc.: 97.66%] [G loss: 4.758390]\n",
            "18864 [D loss: 0.099276, acc.: 97.66%] [G loss: 5.073583]\n",
            "18865 [D loss: 0.067720, acc.: 98.63%] [G loss: 4.706288]\n",
            "18866 [D loss: 0.136119, acc.: 97.27%] [G loss: 5.020803]\n",
            "18867 [D loss: 0.063357, acc.: 98.44%] [G loss: 4.869861]\n",
            "18868 [D loss: 0.139161, acc.: 97.27%] [G loss: 4.743499]\n",
            "18869 [D loss: 0.047030, acc.: 98.83%] [G loss: 5.037339]\n",
            "18870 [D loss: 0.170008, acc.: 96.68%] [G loss: 5.694264]\n",
            "18871 [D loss: 0.100816, acc.: 97.85%] [G loss: 4.750520]\n",
            "18872 [D loss: 0.107934, acc.: 98.05%] [G loss: 5.067940]\n",
            "18873 [D loss: 0.093548, acc.: 97.85%] [G loss: 4.442991]\n",
            "18874 [D loss: 0.082189, acc.: 98.24%] [G loss: 4.650939]\n",
            "18875 [D loss: 0.083630, acc.: 97.85%] [G loss: 4.722755]\n",
            "18876 [D loss: 0.073504, acc.: 98.24%] [G loss: 4.722376]\n",
            "18877 [D loss: 0.103520, acc.: 97.66%] [G loss: 4.474979]\n",
            "18878 [D loss: 0.053445, acc.: 98.63%] [G loss: 5.461916]\n",
            "18879 [D loss: 0.151799, acc.: 96.88%] [G loss: 5.239980]\n",
            "18880 [D loss: 0.078255, acc.: 97.85%] [G loss: 4.948165]\n",
            "18881 [D loss: 0.064232, acc.: 98.44%] [G loss: 4.749749]\n",
            "18882 [D loss: 0.046516, acc.: 98.63%] [G loss: 4.691784]\n",
            "18883 [D loss: 0.110785, acc.: 97.46%] [G loss: 5.373435]\n",
            "18884 [D loss: 0.064515, acc.: 98.44%] [G loss: 5.126739]\n",
            "18885 [D loss: 0.148488, acc.: 97.07%] [G loss: 6.114386]\n",
            "18886 [D loss: 0.171215, acc.: 96.68%] [G loss: 4.569771]\n",
            "18887 [D loss: 0.110808, acc.: 97.46%] [G loss: 4.775787]\n",
            "18888 [D loss: 0.040313, acc.: 98.83%] [G loss: 5.334903]\n",
            "18889 [D loss: 0.179910, acc.: 95.90%] [G loss: 4.856310]\n",
            "18890 [D loss: 0.122944, acc.: 96.88%] [G loss: 4.208396]\n",
            "18891 [D loss: 0.059100, acc.: 98.83%] [G loss: 4.823232]\n",
            "18892 [D loss: 0.109337, acc.: 97.66%] [G loss: 4.232891]\n",
            "18893 [D loss: 0.119213, acc.: 97.07%] [G loss: 4.745101]\n",
            "18894 [D loss: 0.086221, acc.: 97.66%] [G loss: 4.556114]\n",
            "18895 [D loss: 0.175395, acc.: 96.48%] [G loss: 5.194532]\n",
            "18896 [D loss: 0.083342, acc.: 97.46%] [G loss: 5.521957]\n",
            "18897 [D loss: 0.160374, acc.: 97.07%] [G loss: 6.286737]\n",
            "18898 [D loss: 0.045724, acc.: 99.02%] [G loss: 6.248087]\n",
            "18899 [D loss: 0.104365, acc.: 98.24%] [G loss: 4.485665]\n",
            "18900 [D loss: 0.077734, acc.: 98.44%] [G loss: 4.535146]\n",
            "18901 [D loss: 0.086848, acc.: 97.66%] [G loss: 4.809540]\n",
            "18902 [D loss: 0.116882, acc.: 97.27%] [G loss: 4.335303]\n",
            "18903 [D loss: 0.124403, acc.: 97.07%] [G loss: 4.588367]\n",
            "18904 [D loss: 0.080527, acc.: 98.05%] [G loss: 4.607460]\n",
            "18905 [D loss: 0.152285, acc.: 97.07%] [G loss: 5.084995]\n",
            "18906 [D loss: 0.153479, acc.: 96.68%] [G loss: 4.632462]\n",
            "18907 [D loss: 0.081484, acc.: 98.05%] [G loss: 4.700133]\n",
            "18908 [D loss: 0.072445, acc.: 98.24%] [G loss: 4.306686]\n",
            "18909 [D loss: 0.085093, acc.: 97.85%] [G loss: 4.603549]\n",
            "18910 [D loss: 0.066879, acc.: 98.05%] [G loss: 4.711658]\n",
            "18911 [D loss: 0.137701, acc.: 97.27%] [G loss: 4.809184]\n",
            "18912 [D loss: 0.050612, acc.: 98.63%] [G loss: 5.050098]\n",
            "18913 [D loss: 0.124185, acc.: 97.46%] [G loss: 4.471584]\n",
            "18914 [D loss: 0.063519, acc.: 98.05%] [G loss: 4.787548]\n",
            "18915 [D loss: 0.066296, acc.: 98.24%] [G loss: 4.741533]\n",
            "18916 [D loss: 0.103285, acc.: 97.66%] [G loss: 4.365714]\n",
            "18917 [D loss: 0.079658, acc.: 98.05%] [G loss: 4.228545]\n",
            "18918 [D loss: 0.031560, acc.: 99.22%] [G loss: 5.325778]\n",
            "18919 [D loss: 0.116426, acc.: 97.85%] [G loss: 4.324915]\n",
            "18920 [D loss: 0.089740, acc.: 97.85%] [G loss: 5.014410]\n",
            "18921 [D loss: 0.093030, acc.: 97.85%] [G loss: 4.375823]\n",
            "18922 [D loss: 0.070669, acc.: 98.44%] [G loss: 4.628927]\n",
            "18923 [D loss: 0.129078, acc.: 97.07%] [G loss: 4.866980]\n",
            "18924 [D loss: 0.078995, acc.: 97.46%] [G loss: 4.585368]\n",
            "18925 [D loss: 0.091399, acc.: 97.85%] [G loss: 4.616864]\n",
            "18926 [D loss: 0.129853, acc.: 97.27%] [G loss: 5.084180]\n",
            "18927 [D loss: 0.083488, acc.: 97.85%] [G loss: 4.741505]\n",
            "18928 [D loss: 0.091723, acc.: 98.24%] [G loss: 4.731120]\n",
            "18929 [D loss: 0.040753, acc.: 99.02%] [G loss: 5.252560]\n",
            "18930 [D loss: 0.078307, acc.: 98.44%] [G loss: 4.468383]\n",
            "18931 [D loss: 0.123911, acc.: 97.27%] [G loss: 4.647512]\n",
            "18932 [D loss: 0.087314, acc.: 97.85%] [G loss: 4.596526]\n",
            "18933 [D loss: 0.087698, acc.: 98.05%] [G loss: 4.585432]\n",
            "18934 [D loss: 0.128435, acc.: 96.68%] [G loss: 5.161039]\n",
            "18935 [D loss: 0.072409, acc.: 98.24%] [G loss: 5.129557]\n",
            "18936 [D loss: 0.131196, acc.: 97.46%] [G loss: 4.087524]\n",
            "18937 [D loss: 0.153374, acc.: 96.88%] [G loss: 5.685016]\n",
            "18938 [D loss: 0.094133, acc.: 98.05%] [G loss: 4.957794]\n",
            "18939 [D loss: 0.083580, acc.: 98.05%] [G loss: 4.614961]\n",
            "18940 [D loss: 0.164092, acc.: 96.88%] [G loss: 5.683273]\n",
            "18941 [D loss: 0.081748, acc.: 97.85%] [G loss: 5.318523]\n",
            "18942 [D loss: 0.091677, acc.: 98.24%] [G loss: 4.230052]\n",
            "18943 [D loss: 0.086257, acc.: 98.24%] [G loss: 4.412093]\n",
            "18944 [D loss: 0.104791, acc.: 97.66%] [G loss: 4.332470]\n",
            "18945 [D loss: 0.087814, acc.: 98.05%] [G loss: 4.437727]\n",
            "18946 [D loss: 0.082725, acc.: 98.05%] [G loss: 4.810383]\n",
            "18947 [D loss: 0.079011, acc.: 98.24%] [G loss: 4.474024]\n",
            "18948 [D loss: 0.113606, acc.: 97.66%] [G loss: 4.953723]\n",
            "18949 [D loss: 0.083074, acc.: 98.05%] [G loss: 4.540139]\n",
            "18950 [D loss: 0.086056, acc.: 98.24%] [G loss: 4.411371]\n",
            "18951 [D loss: 0.099339, acc.: 97.85%] [G loss: 4.738384]\n",
            "18952 [D loss: 0.060069, acc.: 98.63%] [G loss: 4.943500]\n",
            "18953 [D loss: 0.090018, acc.: 97.85%] [G loss: 4.560268]\n",
            "18954 [D loss: 0.134821, acc.: 97.07%] [G loss: 5.119991]\n",
            "18955 [D loss: 0.042245, acc.: 99.22%] [G loss: 5.417313]\n",
            "18956 [D loss: 0.085837, acc.: 98.05%] [G loss: 4.892819]\n",
            "18957 [D loss: 0.074066, acc.: 98.44%] [G loss: 4.335252]\n",
            "18958 [D loss: 0.053836, acc.: 98.44%] [G loss: 5.062450]\n",
            "18959 [D loss: 0.118316, acc.: 97.46%] [G loss: 4.461465]\n",
            "18960 [D loss: 0.135301, acc.: 97.27%] [G loss: 5.014882]\n",
            "18961 [D loss: 0.081639, acc.: 98.05%] [G loss: 4.768692]\n",
            "18962 [D loss: 0.093538, acc.: 98.05%] [G loss: 4.556323]\n",
            "18963 [D loss: 0.073922, acc.: 98.24%] [G loss: 4.674698]\n",
            "18964 [D loss: 0.077160, acc.: 98.24%] [G loss: 4.363808]\n",
            "18965 [D loss: 0.031522, acc.: 99.41%] [G loss: 5.119645]\n",
            "18966 [D loss: 0.140926, acc.: 97.07%] [G loss: 4.544755]\n",
            "18967 [D loss: 0.095952, acc.: 97.66%] [G loss: 4.666789]\n",
            "18968 [D loss: 0.105916, acc.: 97.66%] [G loss: 4.767901]\n",
            "18969 [D loss: 0.154772, acc.: 96.68%] [G loss: 4.921529]\n",
            "18970 [D loss: 0.118438, acc.: 97.46%] [G loss: 4.468359]\n",
            "18971 [D loss: 0.098455, acc.: 97.85%] [G loss: 4.913538]\n",
            "18972 [D loss: 0.092465, acc.: 97.85%] [G loss: 4.417922]\n",
            "18973 [D loss: 0.082542, acc.: 98.24%] [G loss: 4.788306]\n",
            "18974 [D loss: 0.075014, acc.: 98.24%] [G loss: 4.702003]\n",
            "18975 [D loss: 0.057955, acc.: 98.63%] [G loss: 4.632742]\n",
            "18976 [D loss: 0.104955, acc.: 97.85%] [G loss: 4.820075]\n",
            "18977 [D loss: 0.061273, acc.: 98.44%] [G loss: 4.794622]\n",
            "18978 [D loss: 0.094590, acc.: 97.85%] [G loss: 4.451836]\n",
            "18979 [D loss: 0.053677, acc.: 98.83%] [G loss: 4.900166]\n",
            "18980 [D loss: 0.075047, acc.: 98.44%] [G loss: 4.514862]\n",
            "18981 [D loss: 0.072499, acc.: 98.24%] [G loss: 4.603653]\n",
            "18982 [D loss: 0.078362, acc.: 98.24%] [G loss: 4.817775]\n",
            "18983 [D loss: 0.061551, acc.: 98.63%] [G loss: 4.721844]\n",
            "18984 [D loss: 0.100114, acc.: 97.85%] [G loss: 4.641189]\n",
            "18985 [D loss: 0.148109, acc.: 96.29%] [G loss: 5.079540]\n",
            "18986 [D loss: 0.071271, acc.: 97.66%] [G loss: 4.677092]\n",
            "18987 [D loss: 0.080798, acc.: 98.44%] [G loss: 5.644606]\n",
            "18988 [D loss: 0.152042, acc.: 96.88%] [G loss: 5.329653]\n",
            "18989 [D loss: 0.117784, acc.: 97.66%] [G loss: 4.344768]\n",
            "18990 [D loss: 0.134894, acc.: 97.46%] [G loss: 5.573377]\n",
            "18991 [D loss: 0.112253, acc.: 97.85%] [G loss: 4.822535]\n",
            "18992 [D loss: 0.148802, acc.: 97.27%] [G loss: 4.969165]\n",
            "18993 [D loss: 0.085080, acc.: 98.05%] [G loss: 4.744146]\n",
            "18994 [D loss: 0.095844, acc.: 98.05%] [G loss: 4.558000]\n",
            "18995 [D loss: 0.060688, acc.: 98.63%] [G loss: 4.787316]\n",
            "18996 [D loss: 0.064271, acc.: 98.63%] [G loss: 4.596020]\n",
            "18997 [D loss: 0.199026, acc.: 96.29%] [G loss: 5.682085]\n",
            "18998 [D loss: 0.103709, acc.: 97.85%] [G loss: 5.615002]\n",
            "18999 [D loss: 0.137597, acc.: 97.66%] [G loss: 4.118878]\n",
            "19000 [D loss: 0.099785, acc.: 97.85%] [G loss: 5.184367]\n",
            "19001 [D loss: 0.100501, acc.: 97.85%] [G loss: 4.840511]\n",
            "19002 [D loss: 0.218088, acc.: 96.29%] [G loss: 7.050612]\n",
            "19003 [D loss: 0.077636, acc.: 98.63%] [G loss: 7.621220]\n",
            "19004 [D loss: 0.109621, acc.: 98.24%] [G loss: 5.555299]\n",
            "19005 [D loss: 0.109708, acc.: 97.85%] [G loss: 4.234280]\n",
            "19006 [D loss: 0.105285, acc.: 97.85%] [G loss: 4.506454]\n",
            "19007 [D loss: 0.095089, acc.: 97.66%] [G loss: 4.443479]\n",
            "19008 [D loss: 0.089663, acc.: 98.05%] [G loss: 4.832336]\n",
            "19009 [D loss: 0.072652, acc.: 98.44%] [G loss: 4.624564]\n",
            "19010 [D loss: 0.152950, acc.: 97.07%] [G loss: 6.352441]\n",
            "19011 [D loss: 0.053006, acc.: 98.83%] [G loss: 6.695431]\n",
            "19012 [D loss: 0.149307, acc.: 97.27%] [G loss: 13.000282]\n",
            "19013 [D loss: 0.046471, acc.: 98.44%] [G loss: 7.294910]\n",
            "19014 [D loss: 0.041780, acc.: 98.44%] [G loss: 6.279577]\n",
            "19015 [D loss: 0.175605, acc.: 97.07%] [G loss: 12.192213]\n",
            "19016 [D loss: 0.261378, acc.: 96.48%] [G loss: 10.380499]\n",
            "19017 [D loss: 0.161007, acc.: 96.88%] [G loss: 6.299945]\n",
            "19018 [D loss: 0.090769, acc.: 98.05%] [G loss: 5.188545]\n",
            "19019 [D loss: 0.088567, acc.: 96.68%] [G loss: 5.485962]\n",
            "19020 [D loss: 0.035029, acc.: 99.02%] [G loss: 6.094216]\n",
            "19021 [D loss: 0.040747, acc.: 99.02%] [G loss: 5.662554]\n",
            "19022 [D loss: 0.099407, acc.: 97.85%] [G loss: 5.641553]\n",
            "19023 [D loss: 0.106472, acc.: 97.27%] [G loss: 5.117673]\n",
            "19024 [D loss: 0.090945, acc.: 98.05%] [G loss: 6.174400]\n",
            "19025 [D loss: 0.112129, acc.: 97.66%] [G loss: 5.438396]\n",
            "19026 [D loss: 0.085589, acc.: 98.05%] [G loss: 5.110305]\n",
            "19027 [D loss: 0.080149, acc.: 98.05%] [G loss: 5.357353]\n",
            "19028 [D loss: 0.053791, acc.: 98.83%] [G loss: 5.380437]\n",
            "19029 [D loss: 0.130736, acc.: 97.66%] [G loss: 5.754456]\n",
            "19030 [D loss: 0.033499, acc.: 99.22%] [G loss: 6.086582]\n",
            "19031 [D loss: 0.073583, acc.: 98.63%] [G loss: 5.143177]\n",
            "19032 [D loss: 0.114837, acc.: 97.66%] [G loss: 5.586287]\n",
            "19033 [D loss: 0.086001, acc.: 98.24%] [G loss: 4.989325]\n",
            "19034 [D loss: 0.066564, acc.: 98.44%] [G loss: 6.063523]\n",
            "19035 [D loss: 0.095395, acc.: 97.85%] [G loss: 5.449228]\n",
            "19036 [D loss: 0.136084, acc.: 97.27%] [G loss: 5.659502]\n",
            "19037 [D loss: 0.084645, acc.: 97.85%] [G loss: 5.141091]\n",
            "19038 [D loss: 0.130033, acc.: 97.46%] [G loss: 5.734743]\n",
            "19039 [D loss: 0.080603, acc.: 98.05%] [G loss: 5.934573]\n",
            "19040 [D loss: 0.117203, acc.: 97.27%] [G loss: 4.481289]\n",
            "19041 [D loss: 0.122374, acc.: 97.66%] [G loss: 6.219039]\n",
            "19042 [D loss: 0.120411, acc.: 97.66%] [G loss: 5.515061]\n",
            "19043 [D loss: 0.093748, acc.: 98.24%] [G loss: 4.512114]\n",
            "19044 [D loss: 0.094290, acc.: 98.05%] [G loss: 5.184852]\n",
            "19045 [D loss: 0.077103, acc.: 98.24%] [G loss: 4.973891]\n",
            "19046 [D loss: 0.118958, acc.: 97.46%] [G loss: 5.003845]\n",
            "19047 [D loss: 0.061377, acc.: 98.63%] [G loss: 5.117821]\n",
            "19048 [D loss: 0.099952, acc.: 98.05%] [G loss: 4.730558]\n",
            "19049 [D loss: 0.165923, acc.: 96.68%] [G loss: 5.909873]\n",
            "19050 [D loss: 0.103513, acc.: 98.05%] [G loss: 5.104610]\n",
            "19051 [D loss: 0.092861, acc.: 98.24%] [G loss: 4.827908]\n",
            "19052 [D loss: 0.063669, acc.: 98.44%] [G loss: 4.855031]\n",
            "19053 [D loss: 0.213321, acc.: 96.68%] [G loss: 6.868880]\n",
            "19054 [D loss: 0.159078, acc.: 96.88%] [G loss: 6.213470]\n",
            "19055 [D loss: 0.161068, acc.: 96.88%] [G loss: 4.644234]\n",
            "19056 [D loss: 0.130276, acc.: 97.27%] [G loss: 5.163905]\n",
            "19057 [D loss: 0.168276, acc.: 96.48%] [G loss: 4.926640]\n",
            "19058 [D loss: 0.150726, acc.: 96.68%] [G loss: 4.879328]\n",
            "19059 [D loss: 0.135380, acc.: 96.88%] [G loss: 4.848240]\n",
            "19060 [D loss: 0.102325, acc.: 97.27%] [G loss: 5.063358]\n",
            "19061 [D loss: 0.082420, acc.: 97.66%] [G loss: 4.682692]\n",
            "19062 [D loss: 0.119563, acc.: 97.66%] [G loss: 4.494115]\n",
            "19063 [D loss: 0.093820, acc.: 97.46%] [G loss: 5.386961]\n",
            "19064 [D loss: 0.141482, acc.: 97.07%] [G loss: 4.301207]\n",
            "19065 [D loss: 0.060669, acc.: 98.63%] [G loss: 4.671314]\n",
            "19066 [D loss: 0.081378, acc.: 98.05%] [G loss: 4.719419]\n",
            "19067 [D loss: 0.053937, acc.: 98.83%] [G loss: 4.580892]\n",
            "19068 [D loss: 0.080240, acc.: 98.24%] [G loss: 4.827894]\n",
            "19069 [D loss: 0.141615, acc.: 96.88%] [G loss: 4.767937]\n",
            "19070 [D loss: 0.080627, acc.: 98.05%] [G loss: 4.604513]\n",
            "19071 [D loss: 0.099209, acc.: 97.85%] [G loss: 4.532100]\n",
            "19072 [D loss: 0.112207, acc.: 97.46%] [G loss: 4.641486]\n",
            "19073 [D loss: 0.119906, acc.: 97.27%] [G loss: 4.524851]\n",
            "19074 [D loss: 0.092281, acc.: 97.85%] [G loss: 4.709791]\n",
            "19075 [D loss: 0.124929, acc.: 97.46%] [G loss: 4.447552]\n",
            "19076 [D loss: 0.106098, acc.: 97.46%] [G loss: 4.498614]\n",
            "19077 [D loss: 0.193100, acc.: 95.90%] [G loss: 5.994885]\n",
            "19078 [D loss: 0.114938, acc.: 97.07%] [G loss: 4.691813]\n",
            "19079 [D loss: 0.153695, acc.: 97.07%] [G loss: 4.998728]\n",
            "19080 [D loss: 0.172317, acc.: 96.68%] [G loss: 4.646731]\n",
            "19081 [D loss: 0.088890, acc.: 97.85%] [G loss: 5.153188]\n",
            "19082 [D loss: 0.154304, acc.: 96.48%] [G loss: 6.506121]\n",
            "19083 [D loss: 0.108645, acc.: 97.27%] [G loss: 5.768829]\n",
            "19084 [D loss: 0.114281, acc.: 96.88%] [G loss: 7.903409]\n",
            "19085 [D loss: 0.129572, acc.: 96.48%] [G loss: 6.156154]\n",
            "19086 [D loss: 0.057098, acc.: 97.85%] [G loss: 5.905807]\n",
            "19087 [D loss: 0.074354, acc.: 98.05%] [G loss: 7.242810]\n",
            "19088 [D loss: 0.161887, acc.: 97.07%] [G loss: 8.561052]\n",
            "19089 [D loss: 0.117164, acc.: 98.24%] [G loss: 5.599705]\n",
            "19090 [D loss: 0.142001, acc.: 96.68%] [G loss: 4.838454]\n",
            "19091 [D loss: 0.033754, acc.: 98.83%] [G loss: 5.505040]\n",
            "19092 [D loss: 0.157908, acc.: 96.68%] [G loss: 6.344061]\n",
            "19093 [D loss: 0.055752, acc.: 98.83%] [G loss: 6.863969]\n",
            "19094 [D loss: 0.142571, acc.: 97.85%] [G loss: 4.252669]\n",
            "19095 [D loss: 0.124862, acc.: 97.66%] [G loss: 6.212689]\n",
            "19096 [D loss: 0.161238, acc.: 96.68%] [G loss: 5.004536]\n",
            "19097 [D loss: 0.096125, acc.: 98.24%] [G loss: 4.732173]\n",
            "19098 [D loss: 0.082739, acc.: 97.85%] [G loss: 4.614179]\n",
            "19099 [D loss: 0.077601, acc.: 98.24%] [G loss: 4.588115]\n",
            "19100 [D loss: 0.069316, acc.: 98.24%] [G loss: 4.244663]\n",
            "19101 [D loss: 0.113417, acc.: 97.07%] [G loss: 4.500782]\n",
            "19102 [D loss: 0.080719, acc.: 98.05%] [G loss: 4.537581]\n",
            "19103 [D loss: 0.105366, acc.: 97.66%] [G loss: 4.982935]\n",
            "19104 [D loss: 0.080876, acc.: 98.05%] [G loss: 4.855305]\n",
            "19105 [D loss: 0.102896, acc.: 97.85%] [G loss: 4.563867]\n",
            "19106 [D loss: 0.123632, acc.: 97.27%] [G loss: 4.896731]\n",
            "19107 [D loss: 0.095722, acc.: 97.66%] [G loss: 4.323049]\n",
            "19108 [D loss: 0.100385, acc.: 97.85%] [G loss: 4.571711]\n",
            "19109 [D loss: 0.093505, acc.: 97.85%] [G loss: 4.340351]\n",
            "19110 [D loss: 0.090072, acc.: 98.05%] [G loss: 4.356035]\n",
            "19111 [D loss: 0.081926, acc.: 97.85%] [G loss: 4.371463]\n",
            "19112 [D loss: 0.147237, acc.: 96.88%] [G loss: 5.210775]\n",
            "19113 [D loss: 0.121927, acc.: 97.66%] [G loss: 5.221822]\n",
            "19114 [D loss: 0.073606, acc.: 97.85%] [G loss: 5.198028]\n",
            "19115 [D loss: 0.100928, acc.: 97.46%] [G loss: 4.543227]\n",
            "19116 [D loss: 0.082160, acc.: 98.24%] [G loss: 4.455507]\n",
            "19117 [D loss: 0.069385, acc.: 98.24%] [G loss: 4.930081]\n",
            "19118 [D loss: 0.077119, acc.: 98.24%] [G loss: 4.504164]\n",
            "19119 [D loss: 0.108830, acc.: 97.66%] [G loss: 4.955486]\n",
            "19120 [D loss: 0.091087, acc.: 97.85%] [G loss: 4.721879]\n",
            "19121 [D loss: 0.128632, acc.: 96.88%] [G loss: 5.243453]\n",
            "19122 [D loss: 0.098769, acc.: 97.85%] [G loss: 4.756205]\n",
            "19123 [D loss: 0.082474, acc.: 98.05%] [G loss: 5.089211]\n",
            "19124 [D loss: 0.107069, acc.: 96.88%] [G loss: 4.798761]\n",
            "19125 [D loss: 0.134278, acc.: 97.27%] [G loss: 6.242305]\n",
            "19126 [D loss: 0.112490, acc.: 97.46%] [G loss: 6.110228]\n",
            "19127 [D loss: 0.135076, acc.: 97.27%] [G loss: 8.844143]\n",
            "19128 [D loss: 0.066673, acc.: 98.44%] [G loss: 5.730847]\n",
            "19129 [D loss: 0.089749, acc.: 97.46%] [G loss: 5.716749]\n",
            "19130 [D loss: 0.148074, acc.: 96.88%] [G loss: 9.025915]\n",
            "19131 [D loss: 0.126358, acc.: 97.66%] [G loss: 9.137337]\n",
            "19132 [D loss: 0.237503, acc.: 96.29%] [G loss: 6.237534]\n",
            "19133 [D loss: 0.127113, acc.: 97.27%] [G loss: 4.321897]\n",
            "19134 [D loss: 0.047603, acc.: 98.63%] [G loss: 5.902204]\n",
            "19135 [D loss: 0.094761, acc.: 97.66%] [G loss: 5.734015]\n",
            "19136 [D loss: 0.105078, acc.: 96.88%] [G loss: 5.364549]\n",
            "19137 [D loss: 0.150287, acc.: 97.27%] [G loss: 5.533820]\n",
            "19138 [D loss: 0.092331, acc.: 97.66%] [G loss: 5.910959]\n",
            "19139 [D loss: 0.078545, acc.: 98.24%] [G loss: 4.564556]\n",
            "19140 [D loss: 0.073653, acc.: 98.24%] [G loss: 4.782928]\n",
            "19141 [D loss: 0.049815, acc.: 98.83%] [G loss: 5.344882]\n",
            "19142 [D loss: 0.085990, acc.: 98.24%] [G loss: 4.503350]\n",
            "19143 [D loss: 0.102439, acc.: 97.46%] [G loss: 5.635539]\n",
            "19144 [D loss: 0.159439, acc.: 96.09%] [G loss: 4.908713]\n",
            "19145 [D loss: 0.182679, acc.: 96.48%] [G loss: 6.465772]\n",
            "19146 [D loss: 0.159959, acc.: 95.51%] [G loss: 5.833477]\n",
            "19147 [D loss: 0.110844, acc.: 96.68%] [G loss: 5.063208]\n",
            "19148 [D loss: 0.085442, acc.: 98.05%] [G loss: 4.923926]\n",
            "19149 [D loss: 0.103789, acc.: 97.46%] [G loss: 5.645309]\n",
            "19150 [D loss: 0.111446, acc.: 95.90%] [G loss: 5.573768]\n",
            "19151 [D loss: 0.043574, acc.: 98.83%] [G loss: 6.521503]\n",
            "19152 [D loss: 0.186482, acc.: 97.07%] [G loss: 6.059029]\n",
            "19153 [D loss: 0.114628, acc.: 97.27%] [G loss: 7.828000]\n",
            "19154 [D loss: 0.109573, acc.: 97.07%] [G loss: 4.825772]\n",
            "19155 [D loss: 0.088353, acc.: 97.07%] [G loss: 6.400928]\n",
            "19156 [D loss: 0.123815, acc.: 97.46%] [G loss: 9.096784]\n",
            "19157 [D loss: 0.159259, acc.: 96.29%] [G loss: 5.602991]\n",
            "19158 [D loss: 0.104940, acc.: 97.85%] [G loss: 8.608486]\n",
            "19159 [D loss: 0.195947, acc.: 96.09%] [G loss: 7.066370]\n",
            "19160 [D loss: 0.190598, acc.: 95.51%] [G loss: 6.357689]\n",
            "19161 [D loss: 0.092942, acc.: 97.07%] [G loss: 6.051574]\n",
            "19162 [D loss: 0.120750, acc.: 97.27%] [G loss: 7.014519]\n",
            "19163 [D loss: 0.153995, acc.: 97.27%] [G loss: 4.907041]\n",
            "19164 [D loss: 0.091995, acc.: 97.85%] [G loss: 9.216858]\n",
            "19165 [D loss: 0.054440, acc.: 98.83%] [G loss: 4.773516]\n",
            "19166 [D loss: 0.065869, acc.: 98.05%] [G loss: 9.132720]\n",
            "19167 [D loss: 0.050301, acc.: 98.63%] [G loss: 4.286812]\n",
            "19168 [D loss: 0.088451, acc.: 98.24%] [G loss: 5.892290]\n",
            "19169 [D loss: 0.126526, acc.: 97.46%] [G loss: 4.579988]\n",
            "19170 [D loss: 0.128763, acc.: 97.66%] [G loss: 6.375025]\n",
            "19171 [D loss: 0.099978, acc.: 97.85%] [G loss: 5.641936]\n",
            "19172 [D loss: 0.114057, acc.: 97.66%] [G loss: 4.671817]\n",
            "19173 [D loss: 0.068665, acc.: 98.05%] [G loss: 5.585198]\n",
            "19174 [D loss: 0.087326, acc.: 97.66%] [G loss: 4.958622]\n",
            "19175 [D loss: 0.105161, acc.: 97.46%] [G loss: 6.424057]\n",
            "19176 [D loss: 0.072575, acc.: 98.05%] [G loss: 5.540953]\n",
            "19177 [D loss: 0.113579, acc.: 97.46%] [G loss: 6.881393]\n",
            "19178 [D loss: 0.082938, acc.: 97.66%] [G loss: 5.018167]\n",
            "19179 [D loss: 0.087755, acc.: 97.46%] [G loss: 6.714343]\n",
            "19180 [D loss: 0.062895, acc.: 98.05%] [G loss: 5.583683]\n",
            "19181 [D loss: 0.130542, acc.: 97.46%] [G loss: 6.876643]\n",
            "19182 [D loss: 0.135509, acc.: 97.07%] [G loss: 5.862197]\n",
            "19183 [D loss: 0.196100, acc.: 96.48%] [G loss: 5.929334]\n",
            "19184 [D loss: 0.271977, acc.: 85.94%] [G loss: 11.869872]\n",
            "19185 [D loss: 0.328631, acc.: 95.12%] [G loss: 13.993155]\n",
            "19186 [D loss: 0.326549, acc.: 97.27%] [G loss: 10.959373]\n",
            "19187 [D loss: 0.269005, acc.: 97.07%] [G loss: 6.337331]\n",
            "19188 [D loss: 0.145901, acc.: 97.07%] [G loss: 4.675994]\n",
            "19189 [D loss: 0.080889, acc.: 98.05%] [G loss: 4.967602]\n",
            "19190 [D loss: 0.119777, acc.: 97.07%] [G loss: 4.631689]\n",
            "19191 [D loss: 0.093144, acc.: 97.66%] [G loss: 4.544601]\n",
            "19192 [D loss: 0.098825, acc.: 96.88%] [G loss: 4.783165]\n",
            "19193 [D loss: 0.109447, acc.: 97.66%] [G loss: 5.174693]\n",
            "19194 [D loss: 0.090062, acc.: 97.66%] [G loss: 4.724813]\n",
            "19195 [D loss: 0.040866, acc.: 99.22%] [G loss: 4.994658]\n",
            "19196 [D loss: 0.120762, acc.: 97.85%] [G loss: 4.899950]\n",
            "19197 [D loss: 0.072559, acc.: 98.63%] [G loss: 4.552439]\n",
            "19198 [D loss: 0.097666, acc.: 98.05%] [G loss: 4.836984]\n",
            "19199 [D loss: 0.182533, acc.: 94.73%] [G loss: 4.748631]\n",
            "19200 [D loss: 0.068647, acc.: 98.24%] [G loss: 4.837469]\n",
            "19201 [D loss: 0.196451, acc.: 96.09%] [G loss: 4.368442]\n",
            "19202 [D loss: 0.102839, acc.: 96.68%] [G loss: 4.768410]\n",
            "19203 [D loss: 0.081660, acc.: 97.85%] [G loss: 4.329744]\n",
            "19204 [D loss: 0.067022, acc.: 98.24%] [G loss: 4.226834]\n",
            "19205 [D loss: 0.081793, acc.: 98.05%] [G loss: 4.644457]\n",
            "19206 [D loss: 0.069158, acc.: 98.63%] [G loss: 4.519340]\n",
            "19207 [D loss: 0.054961, acc.: 98.83%] [G loss: 4.494328]\n",
            "19208 [D loss: 0.054708, acc.: 98.83%] [G loss: 4.308833]\n",
            "19209 [D loss: 0.054959, acc.: 98.83%] [G loss: 4.253789]\n",
            "19210 [D loss: 0.073921, acc.: 98.44%] [G loss: 4.454112]\n",
            "19211 [D loss: 0.085549, acc.: 98.05%] [G loss: 3.995117]\n",
            "19212 [D loss: 0.056103, acc.: 98.83%] [G loss: 4.471633]\n",
            "19213 [D loss: 0.083143, acc.: 98.24%] [G loss: 4.615968]\n",
            "19214 [D loss: 0.074072, acc.: 98.24%] [G loss: 4.446423]\n",
            "19215 [D loss: 0.094850, acc.: 98.05%] [G loss: 4.294531]\n",
            "19216 [D loss: 0.126084, acc.: 97.27%] [G loss: 4.298655]\n",
            "19217 [D loss: 0.158901, acc.: 96.48%] [G loss: 4.453654]\n",
            "19218 [D loss: 0.030409, acc.: 99.41%] [G loss: 5.425951]\n",
            "19219 [D loss: 0.107540, acc.: 96.88%] [G loss: 4.830694]\n",
            "19220 [D loss: 0.048730, acc.: 98.83%] [G loss: 4.743834]\n",
            "19221 [D loss: 0.026567, acc.: 99.61%] [G loss: 4.964650]\n",
            "19222 [D loss: 0.140977, acc.: 96.68%] [G loss: 3.889855]\n",
            "19223 [D loss: 0.111812, acc.: 97.66%] [G loss: 4.633433]\n",
            "19224 [D loss: 0.159754, acc.: 95.51%] [G loss: 4.582159]\n",
            "19225 [D loss: 0.063996, acc.: 98.24%] [G loss: 4.554025]\n",
            "19226 [D loss: 0.117286, acc.: 97.46%] [G loss: 4.132967]\n",
            "19227 [D loss: 0.104690, acc.: 97.66%] [G loss: 4.859814]\n",
            "19228 [D loss: 0.124870, acc.: 97.46%] [G loss: 4.266943]\n",
            "19229 [D loss: 0.140646, acc.: 97.27%] [G loss: 4.673030]\n",
            "19230 [D loss: 0.036509, acc.: 99.22%] [G loss: 5.365020]\n",
            "19231 [D loss: 0.074867, acc.: 98.24%] [G loss: 4.529099]\n",
            "19232 [D loss: 0.168574, acc.: 96.68%] [G loss: 4.676645]\n",
            "19233 [D loss: 0.073475, acc.: 98.05%] [G loss: 4.982429]\n",
            "19234 [D loss: 0.081535, acc.: 98.44%] [G loss: 4.459636]\n",
            "19235 [D loss: 0.143711, acc.: 96.88%] [G loss: 4.645922]\n",
            "19236 [D loss: 0.197364, acc.: 96.29%] [G loss: 5.379902]\n",
            "19237 [D loss: 0.152266, acc.: 96.29%] [G loss: 4.636569]\n",
            "19238 [D loss: 0.174882, acc.: 95.51%] [G loss: 4.792427]\n",
            "19239 [D loss: 0.171179, acc.: 95.90%] [G loss: 4.835184]\n",
            "19240 [D loss: 0.158298, acc.: 96.88%] [G loss: 4.247118]\n",
            "19241 [D loss: 0.103781, acc.: 97.66%] [G loss: 4.429362]\n",
            "19242 [D loss: 0.109430, acc.: 97.66%] [G loss: 4.577141]\n",
            "19243 [D loss: 0.184355, acc.: 95.70%] [G loss: 4.635968]\n",
            "19244 [D loss: 0.062953, acc.: 98.44%] [G loss: 5.509979]\n",
            "19245 [D loss: 0.092414, acc.: 97.46%] [G loss: 4.597158]\n",
            "19246 [D loss: 0.112647, acc.: 97.66%] [G loss: 4.709640]\n",
            "19247 [D loss: 0.098018, acc.: 97.85%] [G loss: 4.319689]\n",
            "19248 [D loss: 0.117478, acc.: 97.46%] [G loss: 4.353354]\n",
            "19249 [D loss: 0.046550, acc.: 98.83%] [G loss: 4.898410]\n",
            "19250 [D loss: 0.081568, acc.: 98.24%] [G loss: 4.571235]\n",
            "19251 [D loss: 0.088025, acc.: 97.85%] [G loss: 5.126463]\n",
            "19252 [D loss: 0.080965, acc.: 97.66%] [G loss: 4.297543]\n",
            "19253 [D loss: 0.168801, acc.: 96.29%] [G loss: 5.855699]\n",
            "19254 [D loss: 0.072429, acc.: 98.44%] [G loss: 6.545642]\n",
            "19255 [D loss: 0.082921, acc.: 98.24%] [G loss: 5.353015]\n",
            "19256 [D loss: 0.113103, acc.: 97.66%] [G loss: 6.137793]\n",
            "19257 [D loss: 0.089546, acc.: 97.85%] [G loss: 5.428666]\n",
            "19258 [D loss: 0.100057, acc.: 97.85%] [G loss: 5.432653]\n",
            "19259 [D loss: 0.149093, acc.: 95.90%] [G loss: 5.989211]\n",
            "19260 [D loss: 0.135872, acc.: 96.88%] [G loss: 5.457496]\n",
            "19261 [D loss: 0.155747, acc.: 96.48%] [G loss: 6.172654]\n",
            "19262 [D loss: 0.054731, acc.: 98.63%] [G loss: 7.116482]\n",
            "19263 [D loss: 0.131399, acc.: 97.66%] [G loss: 5.980703]\n",
            "19264 [D loss: 0.090551, acc.: 97.27%] [G loss: 7.333813]\n",
            "19265 [D loss: 0.082053, acc.: 97.85%] [G loss: 6.301039]\n",
            "19266 [D loss: 0.117386, acc.: 96.88%] [G loss: 5.301960]\n",
            "19267 [D loss: 0.064608, acc.: 98.05%] [G loss: 7.104474]\n",
            "19268 [D loss: 0.089586, acc.: 97.85%] [G loss: 5.480803]\n",
            "19269 [D loss: 0.058754, acc.: 98.05%] [G loss: 6.267437]\n",
            "19270 [D loss: 0.079296, acc.: 98.05%] [G loss: 5.776830]\n",
            "19271 [D loss: 0.056839, acc.: 98.83%] [G loss: 6.066726]\n",
            "19272 [D loss: 0.077719, acc.: 98.24%] [G loss: 5.538199]\n",
            "19273 [D loss: 0.111255, acc.: 97.66%] [G loss: 5.482271]\n",
            "19274 [D loss: 0.026262, acc.: 99.41%] [G loss: 6.565921]\n",
            "19275 [D loss: 0.166845, acc.: 97.07%] [G loss: 4.672677]\n",
            "19276 [D loss: 0.146756, acc.: 96.48%] [G loss: 5.450540]\n",
            "19277 [D loss: 0.047214, acc.: 98.83%] [G loss: 6.122776]\n",
            "19278 [D loss: 0.090841, acc.: 97.85%] [G loss: 5.355004]\n",
            "19279 [D loss: 0.115126, acc.: 97.66%] [G loss: 5.658223]\n",
            "19280 [D loss: 0.099871, acc.: 97.66%] [G loss: 5.828336]\n",
            "19281 [D loss: 0.092427, acc.: 97.66%] [G loss: 4.791111]\n",
            "19282 [D loss: 0.138947, acc.: 97.07%] [G loss: 5.515060]\n",
            "19283 [D loss: 0.102654, acc.: 97.46%] [G loss: 4.632890]\n",
            "19284 [D loss: 0.078373, acc.: 98.63%] [G loss: 4.784734]\n",
            "19285 [D loss: 0.079419, acc.: 98.05%] [G loss: 5.491287]\n",
            "19286 [D loss: 0.083827, acc.: 98.05%] [G loss: 5.178762]\n",
            "19287 [D loss: 0.063171, acc.: 98.44%] [G loss: 5.073316]\n",
            "19288 [D loss: 0.083406, acc.: 97.85%] [G loss: 5.057429]\n",
            "19289 [D loss: 0.068124, acc.: 98.24%] [G loss: 6.134050]\n",
            "19290 [D loss: 0.125286, acc.: 97.66%] [G loss: 6.316405]\n",
            "19291 [D loss: 0.072892, acc.: 98.44%] [G loss: 6.331929]\n",
            "19292 [D loss: 0.062097, acc.: 98.63%] [G loss: 5.973577]\n",
            "19293 [D loss: 0.065463, acc.: 98.24%] [G loss: 5.186685]\n",
            "19294 [D loss: 0.090372, acc.: 97.85%] [G loss: 4.998834]\n",
            "19295 [D loss: 0.043609, acc.: 98.83%] [G loss: 5.688300]\n",
            "19296 [D loss: 0.066419, acc.: 98.63%] [G loss: 5.771660]\n",
            "19297 [D loss: 0.111135, acc.: 97.46%] [G loss: 5.349742]\n",
            "19298 [D loss: 0.110350, acc.: 97.27%] [G loss: 5.467970]\n",
            "19299 [D loss: 0.057225, acc.: 98.24%] [G loss: 5.595479]\n",
            "19300 [D loss: 0.089573, acc.: 98.24%] [G loss: 5.430850]\n",
            "19301 [D loss: 0.059296, acc.: 98.24%] [G loss: 5.425721]\n",
            "19302 [D loss: 0.097082, acc.: 97.66%] [G loss: 7.455682]\n",
            "19303 [D loss: 0.106649, acc.: 97.66%] [G loss: 6.220598]\n",
            "19304 [D loss: 0.072049, acc.: 98.05%] [G loss: 4.293077]\n",
            "19305 [D loss: 0.134157, acc.: 97.66%] [G loss: 8.043354]\n",
            "19306 [D loss: 0.126190, acc.: 96.68%] [G loss: 6.704030]\n",
            "19307 [D loss: 0.335776, acc.: 96.09%] [G loss: 8.369459]\n",
            "19308 [D loss: 0.362951, acc.: 92.58%] [G loss: 8.897058]\n",
            "19309 [D loss: 0.418901, acc.: 93.55%] [G loss: 10.776213]\n",
            "19310 [D loss: 0.113056, acc.: 97.27%] [G loss: 12.578108]\n",
            "19311 [D loss: 0.117552, acc.: 98.63%] [G loss: 11.955341]\n",
            "19312 [D loss: 0.728522, acc.: 74.02%] [G loss: 24.446564]\n",
            "19313 [D loss: 0.887449, acc.: 95.31%] [G loss: 20.853195]\n",
            "19314 [D loss: 0.515132, acc.: 96.48%] [G loss: 17.742943]\n",
            "19315 [D loss: 2.680974, acc.: 50.00%] [G loss: 62.691360]\n",
            "19316 [D loss: 100.858563, acc.: 38.48%] [G loss: 241.920227]\n",
            "19317 [D loss: 371.490284, acc.: 13.09%] [G loss: 87.008896]\n",
            "19318 [D loss: 96.539798, acc.: 34.96%] [G loss: 189.300751]\n",
            "19319 [D loss: 66.315022, acc.: 17.77%] [G loss: 64.408775]\n",
            "19320 [D loss: 10.997396, acc.: 69.14%] [G loss: 76.535912]\n",
            "19321 [D loss: 37.928045, acc.: 44.14%] [G loss: 212.711807]\n",
            "19322 [D loss: 52.495354, acc.: 65.23%] [G loss: 39.332115]\n",
            "19323 [D loss: 36.396081, acc.: 46.09%] [G loss: 112.736618]\n",
            "19324 [D loss: 26.593601, acc.: 72.46%] [G loss: 28.078926]\n",
            "19325 [D loss: 28.438625, acc.: 46.88%] [G loss: 63.197964]\n",
            "19326 [D loss: 8.940780, acc.: 84.18%] [G loss: 53.727005]\n",
            "19327 [D loss: 8.981521, acc.: 74.02%] [G loss: 74.586258]\n",
            "19328 [D loss: 1.656091, acc.: 76.37%] [G loss: 60.993343]\n",
            "19329 [D loss: 1.201176, acc.: 93.55%] [G loss: 38.732079]\n",
            "19330 [D loss: 4.753545, acc.: 58.79%] [G loss: 60.330082]\n",
            "19331 [D loss: 6.302434, acc.: 86.13%] [G loss: 54.573906]\n",
            "19332 [D loss: 4.009238, acc.: 86.72%] [G loss: 36.772884]\n",
            "19333 [D loss: 3.268718, acc.: 81.25%] [G loss: 39.476242]\n",
            "19334 [D loss: 2.460995, acc.: 84.38%] [G loss: 24.588852]\n",
            "19335 [D loss: 2.997000, acc.: 75.98%] [G loss: 53.639153]\n",
            "19336 [D loss: 3.895880, acc.: 73.63%] [G loss: 44.094261]\n",
            "19337 [D loss: 2.534476, acc.: 81.64%] [G loss: 32.819332]\n",
            "19338 [D loss: 0.156696, acc.: 97.07%] [G loss: 27.060719]\n",
            "19339 [D loss: 1.799001, acc.: 68.36%] [G loss: 54.025009]\n",
            "19340 [D loss: 7.325029, acc.: 79.10%] [G loss: 38.337337]\n",
            "19341 [D loss: 2.548786, acc.: 87.50%] [G loss: 7.792017]\n",
            "19342 [D loss: 2.068314, acc.: 62.50%] [G loss: 45.337914]\n",
            "19343 [D loss: 4.497336, acc.: 85.16%] [G loss: 39.969139]\n",
            "19344 [D loss: 2.576696, acc.: 88.48%] [G loss: 26.972218]\n",
            "19345 [D loss: 3.339730, acc.: 65.04%] [G loss: 42.295662]\n",
            "19346 [D loss: 2.048566, acc.: 87.11%] [G loss: 26.069559]\n",
            "19347 [D loss: 0.600223, acc.: 94.73%] [G loss: 59.414597]\n",
            "19348 [D loss: 0.768836, acc.: 88.48%] [G loss: 29.703470]\n",
            "19349 [D loss: 0.246162, acc.: 94.34%] [G loss: 26.455719]\n",
            "19350 [D loss: 0.218913, acc.: 94.14%] [G loss: 18.741196]\n",
            "19351 [D loss: 0.211596, acc.: 95.12%] [G loss: 11.919486]\n",
            "19352 [D loss: 0.623499, acc.: 95.31%] [G loss: 13.215755]\n",
            "19353 [D loss: 0.696443, acc.: 88.87%] [G loss: 20.426655]\n",
            "19354 [D loss: 0.928952, acc.: 82.81%] [G loss: 14.841352]\n",
            "19355 [D loss: 0.897053, acc.: 87.70%] [G loss: 15.397614]\n",
            "19356 [D loss: 1.177959, acc.: 83.98%] [G loss: 20.037374]\n",
            "19357 [D loss: 1.877690, acc.: 75.00%] [G loss: 22.666044]\n",
            "19358 [D loss: 1.225923, acc.: 89.84%] [G loss: 15.989843]\n",
            "19359 [D loss: 0.327793, acc.: 96.09%] [G loss: 16.061525]\n",
            "19360 [D loss: 1.383960, acc.: 75.39%] [G loss: 39.026138]\n",
            "19361 [D loss: 3.110952, acc.: 73.83%] [G loss: 4.084316]\n",
            "19362 [D loss: 0.368665, acc.: 91.80%] [G loss: 25.401487]\n",
            "19363 [D loss: 0.496680, acc.: 93.55%] [G loss: 23.612535]\n",
            "19364 [D loss: 0.078918, acc.: 98.44%] [G loss: 30.119392]\n",
            "19365 [D loss: 0.097114, acc.: 98.05%] [G loss: 14.065331]\n",
            "19366 [D loss: 0.280677, acc.: 96.68%] [G loss: 14.405813]\n",
            "19367 [D loss: 0.405037, acc.: 93.75%] [G loss: 17.210094]\n",
            "19368 [D loss: 0.361809, acc.: 91.80%] [G loss: 11.573513]\n",
            "19369 [D loss: 0.260896, acc.: 91.60%] [G loss: 17.754200]\n",
            "19370 [D loss: 0.485509, acc.: 92.77%] [G loss: 10.537394]\n",
            "19371 [D loss: 0.418834, acc.: 94.34%] [G loss: 14.085764]\n",
            "19372 [D loss: 1.348988, acc.: 87.11%] [G loss: 20.390444]\n",
            "19373 [D loss: 1.038356, acc.: 87.89%] [G loss: 10.318217]\n",
            "19374 [D loss: 0.665687, acc.: 80.66%] [G loss: 25.828119]\n",
            "19375 [D loss: 1.058380, acc.: 85.35%] [G loss: 6.907949]\n",
            "19376 [D loss: 0.361936, acc.: 89.45%] [G loss: 19.145945]\n",
            "19377 [D loss: 0.108112, acc.: 97.66%] [G loss: 28.756662]\n",
            "19378 [D loss: 0.503751, acc.: 91.41%] [G loss: 13.442086]\n",
            "19379 [D loss: 0.174327, acc.: 96.88%] [G loss: 16.381819]\n",
            "19380 [D loss: 0.040504, acc.: 99.41%] [G loss: 16.298235]\n",
            "19381 [D loss: 0.068958, acc.: 98.24%] [G loss: 12.285112]\n",
            "19382 [D loss: 0.353026, acc.: 95.51%] [G loss: 15.483342]\n",
            "19383 [D loss: 0.667220, acc.: 97.07%] [G loss: 13.741742]\n",
            "19384 [D loss: 0.049277, acc.: 99.02%] [G loss: 11.141212]\n",
            "19385 [D loss: 0.354362, acc.: 94.34%] [G loss: 13.976335]\n",
            "19386 [D loss: 0.971279, acc.: 90.23%] [G loss: 9.337701]\n",
            "19387 [D loss: 0.165609, acc.: 97.46%] [G loss: 11.753700]\n",
            "19388 [D loss: 0.172838, acc.: 95.70%] [G loss: 13.327002]\n",
            "19389 [D loss: 0.803353, acc.: 75.00%] [G loss: 26.771484]\n",
            "19390 [D loss: 2.337516, acc.: 84.77%] [G loss: 14.066242]\n",
            "19391 [D loss: 0.769827, acc.: 91.21%] [G loss: 11.281358]\n",
            "19392 [D loss: 0.112790, acc.: 97.85%] [G loss: 14.941915]\n",
            "19393 [D loss: 0.149039, acc.: 97.07%] [G loss: 11.994546]\n",
            "19394 [D loss: 0.092797, acc.: 96.29%] [G loss: 17.373674]\n",
            "19395 [D loss: 0.043894, acc.: 99.02%] [G loss: 13.067583]\n",
            "19396 [D loss: 0.051957, acc.: 98.83%] [G loss: 8.700171]\n",
            "19397 [D loss: 0.150254, acc.: 95.70%] [G loss: 12.027812]\n",
            "19398 [D loss: 0.891549, acc.: 92.77%] [G loss: 10.064919]\n",
            "19399 [D loss: 0.477436, acc.: 92.38%] [G loss: 16.518238]\n",
            "19400 [D loss: 0.936342, acc.: 81.64%] [G loss: 22.699005]\n",
            "19401 [D loss: 0.290559, acc.: 94.73%] [G loss: 21.294819]\n",
            "19402 [D loss: 0.116680, acc.: 96.29%] [G loss: 12.845907]\n",
            "19403 [D loss: 0.065262, acc.: 98.05%] [G loss: 11.370220]\n",
            "19404 [D loss: 0.074730, acc.: 98.83%] [G loss: 16.056482]\n",
            "19405 [D loss: 0.036480, acc.: 96.88%] [G loss: 8.802173]\n",
            "19406 [D loss: 0.144809, acc.: 95.51%] [G loss: 15.304204]\n",
            "19407 [D loss: 0.606993, acc.: 83.20%] [G loss: 16.490032]\n",
            "19408 [D loss: 0.411345, acc.: 96.29%] [G loss: 11.848081]\n",
            "19409 [D loss: 0.130554, acc.: 97.46%] [G loss: 9.740651]\n",
            "19410 [D loss: 0.218937, acc.: 95.51%] [G loss: 9.635347]\n",
            "19411 [D loss: 0.285445, acc.: 95.70%] [G loss: 10.388481]\n",
            "19412 [D loss: 0.087528, acc.: 96.88%] [G loss: 8.530051]\n",
            "19413 [D loss: 0.446370, acc.: 87.89%] [G loss: 20.244642]\n",
            "19414 [D loss: 1.303837, acc.: 91.02%] [G loss: 14.825386]\n",
            "19415 [D loss: 0.246698, acc.: 95.12%] [G loss: 9.126195]\n",
            "19416 [D loss: 0.035342, acc.: 98.44%] [G loss: 10.569675]\n",
            "19417 [D loss: 0.032322, acc.: 99.22%] [G loss: 11.229646]\n",
            "19418 [D loss: 0.053590, acc.: 98.24%] [G loss: 12.044620]\n",
            "19419 [D loss: 0.028091, acc.: 99.02%] [G loss: 9.634865]\n",
            "19420 [D loss: 0.125594, acc.: 94.14%] [G loss: 13.445443]\n",
            "19421 [D loss: 0.230820, acc.: 96.88%] [G loss: 8.389656]\n",
            "19422 [D loss: 0.156167, acc.: 93.95%] [G loss: 17.512367]\n",
            "19423 [D loss: 0.736317, acc.: 91.99%] [G loss: 7.588801]\n",
            "19424 [D loss: 0.118218, acc.: 96.68%] [G loss: 18.144426]\n",
            "19425 [D loss: 0.410819, acc.: 90.62%] [G loss: 11.856055]\n",
            "19426 [D loss: 0.060932, acc.: 99.41%] [G loss: 15.965653]\n",
            "19427 [D loss: 0.102091, acc.: 96.48%] [G loss: 22.258760]\n",
            "19428 [D loss: 0.359632, acc.: 95.31%] [G loss: 14.278205]\n",
            "19429 [D loss: 0.243925, acc.: 89.26%] [G loss: 20.037815]\n",
            "19430 [D loss: 0.561330, acc.: 94.14%] [G loss: 16.393913]\n",
            "19431 [D loss: 0.373866, acc.: 95.31%] [G loss: 5.339259]\n",
            "19432 [D loss: 0.643918, acc.: 83.20%] [G loss: 29.276497]\n",
            "19433 [D loss: 2.279691, acc.: 85.55%] [G loss: 17.902542]\n",
            "19434 [D loss: 0.980774, acc.: 92.38%] [G loss: 6.750565]\n",
            "19435 [D loss: 0.452063, acc.: 93.55%] [G loss: 11.980242]\n",
            "19436 [D loss: 0.161848, acc.: 94.34%] [G loss: 12.618389]\n",
            "19437 [D loss: 0.029276, acc.: 97.85%] [G loss: 10.270553]\n",
            "19438 [D loss: 0.027015, acc.: 98.63%] [G loss: 7.737543]\n",
            "19439 [D loss: 0.142161, acc.: 96.88%] [G loss: 9.417743]\n",
            "19440 [D loss: 0.039789, acc.: 99.02%] [G loss: 8.179901]\n",
            "19441 [D loss: 0.159201, acc.: 93.75%] [G loss: 9.691464]\n",
            "19442 [D loss: 0.374842, acc.: 92.77%] [G loss: 9.646723]\n",
            "19443 [D loss: 0.467895, acc.: 88.67%] [G loss: 11.222607]\n",
            "19444 [D loss: 0.124215, acc.: 97.27%] [G loss: 8.958071]\n",
            "19445 [D loss: 0.183753, acc.: 95.31%] [G loss: 11.508002]\n",
            "19446 [D loss: 0.140258, acc.: 96.48%] [G loss: 8.776541]\n",
            "19447 [D loss: 0.123022, acc.: 94.14%] [G loss: 10.137962]\n",
            "19448 [D loss: 0.181434, acc.: 94.92%] [G loss: 8.123079]\n",
            "19449 [D loss: 0.108517, acc.: 95.51%] [G loss: 7.897171]\n",
            "19450 [D loss: 0.012276, acc.: 99.41%] [G loss: 9.510324]\n",
            "19451 [D loss: 0.066758, acc.: 97.46%] [G loss: 11.156311]\n",
            "19452 [D loss: 0.177558, acc.: 94.34%] [G loss: 6.933427]\n",
            "19453 [D loss: 0.114946, acc.: 97.46%] [G loss: 11.267879]\n",
            "19454 [D loss: 0.227685, acc.: 92.38%] [G loss: 6.758434]\n",
            "19455 [D loss: 0.219637, acc.: 96.09%] [G loss: 10.255988]\n",
            "19456 [D loss: 0.164521, acc.: 95.70%] [G loss: 12.888062]\n",
            "19457 [D loss: 0.259493, acc.: 90.43%] [G loss: 11.611718]\n",
            "19458 [D loss: 0.157209, acc.: 93.95%] [G loss: 6.451238]\n",
            "19459 [D loss: 0.033952, acc.: 99.41%] [G loss: 7.091857]\n",
            "19460 [D loss: 0.027410, acc.: 99.02%] [G loss: 10.722131]\n",
            "19461 [D loss: 0.048769, acc.: 97.85%] [G loss: 6.733436]\n",
            "19462 [D loss: 0.039993, acc.: 98.44%] [G loss: 8.643687]\n",
            "19463 [D loss: 0.188479, acc.: 95.12%] [G loss: 9.331230]\n",
            "19464 [D loss: 0.137257, acc.: 93.36%] [G loss: 9.254539]\n",
            "19465 [D loss: 0.431041, acc.: 90.82%] [G loss: 12.980211]\n",
            "19466 [D loss: 1.177050, acc.: 76.37%] [G loss: 15.361543]\n",
            "19467 [D loss: 0.928145, acc.: 80.66%] [G loss: 14.329467]\n",
            "19468 [D loss: 0.123420, acc.: 96.48%] [G loss: 14.698413]\n",
            "19469 [D loss: 0.698995, acc.: 88.87%] [G loss: 7.259886]\n",
            "19470 [D loss: 0.077417, acc.: 98.05%] [G loss: 8.248732]\n",
            "19471 [D loss: 0.031764, acc.: 98.83%] [G loss: 8.400247]\n",
            "19472 [D loss: 0.087559, acc.: 97.46%] [G loss: 7.599673]\n",
            "19473 [D loss: 0.076549, acc.: 96.48%] [G loss: 9.007438]\n",
            "19474 [D loss: 0.094018, acc.: 96.29%] [G loss: 7.854153]\n",
            "19475 [D loss: 0.035452, acc.: 98.63%] [G loss: 7.048279]\n",
            "19476 [D loss: 0.033526, acc.: 98.83%] [G loss: 8.233793]\n",
            "19477 [D loss: 0.052145, acc.: 99.02%] [G loss: 9.028495]\n",
            "19478 [D loss: 0.058838, acc.: 98.83%] [G loss: 8.923063]\n",
            "19479 [D loss: 0.144454, acc.: 93.55%] [G loss: 12.327184]\n",
            "19480 [D loss: 0.407027, acc.: 90.04%] [G loss: 11.976948]\n",
            "19481 [D loss: 0.021386, acc.: 99.61%] [G loss: 8.968659]\n",
            "19482 [D loss: 0.117382, acc.: 96.88%] [G loss: 7.776886]\n",
            "19483 [D loss: 0.078621, acc.: 98.24%] [G loss: 9.154044]\n",
            "19484 [D loss: 0.064744, acc.: 97.85%] [G loss: 9.282192]\n",
            "19485 [D loss: 0.113062, acc.: 97.07%] [G loss: 7.685973]\n",
            "19486 [D loss: 0.065637, acc.: 97.46%] [G loss: 6.899493]\n",
            "19487 [D loss: 0.073587, acc.: 97.46%] [G loss: 6.395091]\n",
            "19488 [D loss: 0.028402, acc.: 99.02%] [G loss: 8.390078]\n",
            "19489 [D loss: 0.013451, acc.: 99.80%] [G loss: 8.489978]\n",
            "19490 [D loss: 0.019353, acc.: 99.41%] [G loss: 7.022246]\n",
            "19491 [D loss: 0.078829, acc.: 97.27%] [G loss: 7.204495]\n",
            "19492 [D loss: 0.029429, acc.: 98.83%] [G loss: 7.203354]\n",
            "19493 [D loss: 0.107473, acc.: 96.09%] [G loss: 6.728077]\n",
            "19494 [D loss: 0.112498, acc.: 96.09%] [G loss: 7.599476]\n",
            "19495 [D loss: 0.325816, acc.: 91.60%] [G loss: 10.290703]\n",
            "19496 [D loss: 0.129481, acc.: 94.53%] [G loss: 7.246605]\n",
            "19497 [D loss: 0.052357, acc.: 98.44%] [G loss: 7.375744]\n",
            "19498 [D loss: 0.053332, acc.: 98.44%] [G loss: 10.385030]\n",
            "19499 [D loss: 0.016243, acc.: 99.61%] [G loss: 8.563426]\n",
            "19500 [D loss: 0.009965, acc.: 99.80%] [G loss: 7.625900]\n",
            "19501 [D loss: 0.029116, acc.: 99.02%] [G loss: 6.863619]\n",
            "19502 [D loss: 0.013985, acc.: 99.61%] [G loss: 6.514591]\n",
            "19503 [D loss: 0.029322, acc.: 99.22%] [G loss: 6.586754]\n",
            "19504 [D loss: 0.028586, acc.: 99.02%] [G loss: 6.913889]\n",
            "19505 [D loss: 0.091243, acc.: 96.29%] [G loss: 9.100637]\n",
            "19506 [D loss: 0.071712, acc.: 97.07%] [G loss: 7.120686]\n",
            "19507 [D loss: 0.017503, acc.: 99.80%] [G loss: 8.293602]\n",
            "19508 [D loss: 0.025244, acc.: 99.61%] [G loss: 8.134983]\n",
            "19509 [D loss: 0.016052, acc.: 100.00%] [G loss: 8.676045]\n",
            "19510 [D loss: 0.023907, acc.: 99.61%] [G loss: 9.346676]\n",
            "19511 [D loss: 0.279046, acc.: 86.33%] [G loss: 23.825712]\n",
            "19512 [D loss: 3.127258, acc.: 52.93%] [G loss: 37.221306]\n",
            "19513 [D loss: 2.406892, acc.: 87.70%] [G loss: 31.505905]\n",
            "19514 [D loss: 2.373806, acc.: 89.84%] [G loss: 12.667675]\n",
            "19515 [D loss: 1.091327, acc.: 90.82%] [G loss: 13.492902]\n",
            "19516 [D loss: 0.309103, acc.: 94.14%] [G loss: 13.550047]\n",
            "19517 [D loss: 0.031497, acc.: 98.63%] [G loss: 8.546432]\n",
            "19518 [D loss: 0.037092, acc.: 99.22%] [G loss: 11.823944]\n",
            "19519 [D loss: 0.032821, acc.: 98.63%] [G loss: 8.537065]\n",
            "19520 [D loss: 0.105751, acc.: 96.88%] [G loss: 9.541395]\n",
            "19521 [D loss: 0.070050, acc.: 96.68%] [G loss: 10.615428]\n",
            "19522 [D loss: 0.576789, acc.: 79.10%] [G loss: 18.738935]\n",
            "19523 [D loss: 0.457322, acc.: 95.70%] [G loss: 17.082495]\n",
            "19524 [D loss: 0.638305, acc.: 93.16%] [G loss: 9.697628]\n",
            "19525 [D loss: 0.052528, acc.: 98.63%] [G loss: 7.449222]\n",
            "19526 [D loss: 0.021543, acc.: 100.00%] [G loss: 7.972002]\n",
            "19527 [D loss: 0.059510, acc.: 98.05%] [G loss: 7.098589]\n",
            "19528 [D loss: 0.081888, acc.: 95.70%] [G loss: 8.666473]\n",
            "19529 [D loss: 0.237304, acc.: 95.12%] [G loss: 7.698732]\n",
            "19530 [D loss: 0.018993, acc.: 99.80%] [G loss: 8.792509]\n",
            "19531 [D loss: 0.094890, acc.: 96.09%] [G loss: 9.315489]\n",
            "19532 [D loss: 0.118238, acc.: 96.29%] [G loss: 8.192103]\n",
            "19533 [D loss: 0.059242, acc.: 99.22%] [G loss: 10.301414]\n",
            "19534 [D loss: 0.036490, acc.: 96.88%] [G loss: 9.358546]\n",
            "19535 [D loss: 0.125910, acc.: 96.29%] [G loss: 8.801009]\n",
            "19536 [D loss: 0.040313, acc.: 98.24%] [G loss: 10.612501]\n",
            "19537 [D loss: 0.037259, acc.: 99.61%] [G loss: 7.809209]\n",
            "19538 [D loss: 0.033612, acc.: 98.44%] [G loss: 6.800664]\n",
            "19539 [D loss: 0.036859, acc.: 99.80%] [G loss: 9.065430]\n",
            "19540 [D loss: 0.155261, acc.: 93.75%] [G loss: 11.375275]\n",
            "19541 [D loss: 0.158215, acc.: 97.46%] [G loss: 9.978899]\n",
            "19542 [D loss: 0.347303, acc.: 91.02%] [G loss: 13.048859]\n",
            "19543 [D loss: 0.202159, acc.: 91.80%] [G loss: 12.602714]\n",
            "19544 [D loss: 0.375057, acc.: 96.29%] [G loss: 11.942675]\n",
            "19545 [D loss: 0.189021, acc.: 93.75%] [G loss: 8.825275]\n",
            "19546 [D loss: 0.079459, acc.: 97.46%] [G loss: 8.463020]\n",
            "19547 [D loss: 0.232416, acc.: 95.51%] [G loss: 6.794438]\n",
            "19548 [D loss: 0.305535, acc.: 95.51%] [G loss: 11.131342]\n",
            "19549 [D loss: 0.143487, acc.: 96.88%] [G loss: 9.668723]\n",
            "19550 [D loss: 0.035309, acc.: 99.80%] [G loss: 9.168595]\n",
            "19551 [D loss: 0.144021, acc.: 94.73%] [G loss: 11.238097]\n",
            "19552 [D loss: 0.235181, acc.: 95.90%] [G loss: 9.479212]\n",
            "19553 [D loss: 0.229258, acc.: 90.23%] [G loss: 14.274920]\n",
            "19554 [D loss: 0.480721, acc.: 95.31%] [G loss: 10.097491]\n",
            "19555 [D loss: 0.251220, acc.: 96.29%] [G loss: 6.224311]\n",
            "19556 [D loss: 0.114090, acc.: 97.46%] [G loss: 11.771734]\n",
            "19557 [D loss: 0.243718, acc.: 97.27%] [G loss: 8.188150]\n",
            "19558 [D loss: 0.331326, acc.: 83.01%] [G loss: 14.354767]\n",
            "19559 [D loss: 0.435187, acc.: 97.07%] [G loss: 17.124441]\n",
            "19560 [D loss: 0.719434, acc.: 93.36%] [G loss: 8.447783]\n",
            "19561 [D loss: 0.189647, acc.: 97.46%] [G loss: 7.241774]\n",
            "19562 [D loss: 0.045534, acc.: 98.24%] [G loss: 6.862982]\n",
            "19563 [D loss: 0.104493, acc.: 97.07%] [G loss: 7.704416]\n",
            "19564 [D loss: 0.050487, acc.: 98.24%] [G loss: 5.759821]\n",
            "19565 [D loss: 0.054467, acc.: 99.61%] [G loss: 8.442087]\n",
            "19566 [D loss: 0.102047, acc.: 98.05%] [G loss: 5.077972]\n",
            "19567 [D loss: 0.094935, acc.: 98.63%] [G loss: 12.937672]\n",
            "19568 [D loss: 0.305196, acc.: 96.48%] [G loss: 11.652325]\n",
            "19569 [D loss: 0.199113, acc.: 94.73%] [G loss: 7.314996]\n",
            "19570 [D loss: 0.068310, acc.: 99.80%] [G loss: 10.090948]\n",
            "19571 [D loss: 0.073668, acc.: 98.63%] [G loss: 10.587021]\n",
            "19572 [D loss: 0.168240, acc.: 97.27%] [G loss: 4.932199]\n",
            "19573 [D loss: 0.063023, acc.: 100.00%] [G loss: 9.051519]\n",
            "19574 [D loss: 0.113737, acc.: 97.07%] [G loss: 5.671046]\n",
            "19575 [D loss: 0.113157, acc.: 97.07%] [G loss: 9.063762]\n",
            "19576 [D loss: 0.284401, acc.: 95.31%] [G loss: 6.022609]\n",
            "19577 [D loss: 0.054027, acc.: 99.22%] [G loss: 5.899497]\n",
            "19578 [D loss: 0.015131, acc.: 99.80%] [G loss: 7.164301]\n",
            "19579 [D loss: 0.024061, acc.: 100.00%] [G loss: 8.046726]\n",
            "19580 [D loss: 0.077287, acc.: 96.68%] [G loss: 6.734806]\n",
            "19581 [D loss: 0.031110, acc.: 100.00%] [G loss: 7.931068]\n",
            "19582 [D loss: 0.037955, acc.: 98.44%] [G loss: 6.897093]\n",
            "19583 [D loss: 0.064774, acc.: 98.05%] [G loss: 8.351377]\n",
            "19584 [D loss: 0.131689, acc.: 97.85%] [G loss: 6.371874]\n",
            "19585 [D loss: 0.063815, acc.: 98.24%] [G loss: 6.456151]\n",
            "19586 [D loss: 0.161864, acc.: 96.09%] [G loss: 10.371453]\n",
            "19587 [D loss: 0.193046, acc.: 95.31%] [G loss: 6.918922]\n",
            "19588 [D loss: 0.268379, acc.: 93.16%] [G loss: 7.821802]\n",
            "19589 [D loss: 0.132897, acc.: 96.29%] [G loss: 5.295755]\n",
            "19590 [D loss: 0.066886, acc.: 99.02%] [G loss: 12.427910]\n",
            "19591 [D loss: 0.178009, acc.: 96.88%] [G loss: 5.661883]\n",
            "19592 [D loss: 0.041276, acc.: 98.63%] [G loss: 6.363302]\n",
            "19593 [D loss: 0.086460, acc.: 98.44%] [G loss: 10.817998]\n",
            "19594 [D loss: 0.203346, acc.: 96.88%] [G loss: 7.018390]\n",
            "19595 [D loss: 0.084504, acc.: 98.24%] [G loss: 5.358602]\n",
            "19596 [D loss: 0.080037, acc.: 97.85%] [G loss: 8.891113]\n",
            "19597 [D loss: 0.387892, acc.: 85.74%] [G loss: 17.964125]\n",
            "19598 [D loss: 0.942624, acc.: 91.41%] [G loss: 8.099703]\n",
            "19599 [D loss: 1.111860, acc.: 73.44%] [G loss: 23.178766]\n",
            "19600 [D loss: 1.771958, acc.: 86.33%] [G loss: 16.467600]\n",
            "19601 [D loss: 0.417053, acc.: 96.29%] [G loss: 15.927007]\n",
            "19602 [D loss: 0.261590, acc.: 97.27%] [G loss: 6.699736]\n",
            "19603 [D loss: 0.436088, acc.: 78.32%] [G loss: 17.267345]\n",
            "19604 [D loss: 0.528657, acc.: 96.48%] [G loss: 19.886787]\n",
            "19605 [D loss: 0.640341, acc.: 95.70%] [G loss: 12.533144]\n",
            "19606 [D loss: 0.285713, acc.: 97.07%] [G loss: 9.399206]\n",
            "19607 [D loss: 0.403192, acc.: 95.90%] [G loss: 6.250289]\n",
            "19608 [D loss: 0.183577, acc.: 95.31%] [G loss: 8.351232]\n",
            "19609 [D loss: 0.108202, acc.: 97.66%] [G loss: 6.833984]\n",
            "19610 [D loss: 0.061628, acc.: 98.83%] [G loss: 6.931129]\n",
            "19611 [D loss: 0.013485, acc.: 100.00%] [G loss: 6.382369]\n",
            "19612 [D loss: 0.017510, acc.: 99.22%] [G loss: 6.517801]\n",
            "19613 [D loss: 0.036117, acc.: 100.00%] [G loss: 6.463285]\n",
            "19614 [D loss: 0.018397, acc.: 100.00%] [G loss: 6.739386]\n",
            "19615 [D loss: 0.026023, acc.: 100.00%] [G loss: 5.576466]\n",
            "19616 [D loss: 0.019794, acc.: 100.00%] [G loss: 6.203007]\n",
            "19617 [D loss: 0.043727, acc.: 99.80%] [G loss: 6.653365]\n",
            "19618 [D loss: 0.028390, acc.: 98.83%] [G loss: 7.712920]\n",
            "19619 [D loss: 0.040454, acc.: 100.00%] [G loss: 7.276278]\n",
            "19620 [D loss: 0.115253, acc.: 97.07%] [G loss: 7.499429]\n",
            "19621 [D loss: 0.062214, acc.: 98.24%] [G loss: 6.117507]\n",
            "19622 [D loss: 0.218441, acc.: 88.87%] [G loss: 10.910513]\n",
            "19623 [D loss: 0.364550, acc.: 94.92%] [G loss: 8.453433]\n",
            "19624 [D loss: 0.200009, acc.: 96.09%] [G loss: 6.718070]\n",
            "19625 [D loss: 0.153693, acc.: 94.53%] [G loss: 7.434001]\n",
            "19626 [D loss: 0.094652, acc.: 98.05%] [G loss: 6.135452]\n",
            "19627 [D loss: 0.139491, acc.: 96.29%] [G loss: 9.583583]\n",
            "19628 [D loss: 0.164159, acc.: 96.09%] [G loss: 5.115963]\n",
            "19629 [D loss: 0.058550, acc.: 99.02%] [G loss: 7.674028]\n",
            "19630 [D loss: 0.094386, acc.: 97.07%] [G loss: 6.145836]\n",
            "19631 [D loss: 0.024468, acc.: 99.80%] [G loss: 5.241774]\n",
            "19632 [D loss: 0.018059, acc.: 99.80%] [G loss: 7.956570]\n",
            "19633 [D loss: 0.058723, acc.: 99.61%] [G loss: 7.036766]\n",
            "19634 [D loss: 0.104540, acc.: 97.07%] [G loss: 7.201394]\n",
            "19635 [D loss: 0.026282, acc.: 99.61%] [G loss: 6.029303]\n",
            "19636 [D loss: 0.029539, acc.: 99.61%] [G loss: 10.793680]\n",
            "19637 [D loss: 0.074759, acc.: 99.61%] [G loss: 9.817392]\n",
            "19638 [D loss: 0.422977, acc.: 90.23%] [G loss: 5.575130]\n",
            "19639 [D loss: 0.175452, acc.: 92.77%] [G loss: 15.747920]\n",
            "19640 [D loss: 0.764118, acc.: 94.53%] [G loss: 10.577058]\n",
            "19641 [D loss: 0.308138, acc.: 96.68%] [G loss: 9.979853]\n",
            "19642 [D loss: 0.083821, acc.: 96.29%] [G loss: 8.868285]\n",
            "19643 [D loss: 0.015312, acc.: 100.00%] [G loss: 6.284971]\n",
            "19644 [D loss: 0.091810, acc.: 99.61%] [G loss: 8.717300]\n",
            "19645 [D loss: 0.168904, acc.: 95.31%] [G loss: 5.252244]\n",
            "19646 [D loss: 0.107145, acc.: 97.46%] [G loss: 9.428383]\n",
            "19647 [D loss: 0.158397, acc.: 97.27%] [G loss: 9.041259]\n",
            "19648 [D loss: 0.100027, acc.: 97.85%] [G loss: 5.184548]\n",
            "19649 [D loss: 0.038582, acc.: 99.61%] [G loss: 5.964498]\n",
            "19650 [D loss: 0.054809, acc.: 98.44%] [G loss: 5.549144]\n",
            "19651 [D loss: 0.042110, acc.: 98.63%] [G loss: 5.424031]\n",
            "19652 [D loss: 0.017529, acc.: 99.41%] [G loss: 5.960899]\n",
            "19653 [D loss: 0.201105, acc.: 90.62%] [G loss: 12.255205]\n",
            "19654 [D loss: 0.416145, acc.: 95.31%] [G loss: 8.516781]\n",
            "19655 [D loss: 0.247092, acc.: 93.16%] [G loss: 7.379106]\n",
            "19656 [D loss: 0.107159, acc.: 97.85%] [G loss: 6.082055]\n",
            "19657 [D loss: 0.096287, acc.: 96.09%] [G loss: 8.297279]\n",
            "19658 [D loss: 0.245329, acc.: 92.97%] [G loss: 9.565018]\n",
            "19659 [D loss: 0.253388, acc.: 95.90%] [G loss: 6.207718]\n",
            "19660 [D loss: 0.136042, acc.: 96.09%] [G loss: 11.479109]\n",
            "19661 [D loss: 0.430584, acc.: 93.75%] [G loss: 5.302822]\n",
            "19662 [D loss: 0.148929, acc.: 94.14%] [G loss: 9.839276]\n",
            "19663 [D loss: 0.241957, acc.: 95.51%] [G loss: 7.254208]\n",
            "19664 [D loss: 0.089310, acc.: 98.44%] [G loss: 9.038741]\n",
            "19665 [D loss: 0.049178, acc.: 99.02%] [G loss: 7.999649]\n",
            "19666 [D loss: 0.047973, acc.: 98.63%] [G loss: 6.142117]\n",
            "19667 [D loss: 0.151410, acc.: 93.16%] [G loss: 12.820555]\n",
            "19668 [D loss: 0.407387, acc.: 94.92%] [G loss: 6.874303]\n",
            "19669 [D loss: 0.472409, acc.: 83.20%] [G loss: 15.960806]\n",
            "19670 [D loss: 0.652893, acc.: 95.12%] [G loss: 15.405668]\n",
            "19671 [D loss: 0.647483, acc.: 93.95%] [G loss: 6.596620]\n",
            "19672 [D loss: 0.406027, acc.: 82.03%] [G loss: 15.479924]\n",
            "19673 [D loss: 0.862161, acc.: 93.95%] [G loss: 21.262794]\n",
            "19674 [D loss: 0.534021, acc.: 95.31%] [G loss: 6.637977]\n",
            "19675 [D loss: 0.168637, acc.: 96.09%] [G loss: 5.892390]\n",
            "19676 [D loss: 0.079353, acc.: 97.07%] [G loss: 6.564232]\n",
            "19677 [D loss: 0.051069, acc.: 97.66%] [G loss: 5.622869]\n",
            "19678 [D loss: 0.025946, acc.: 100.00%] [G loss: 6.655345]\n",
            "19679 [D loss: 0.048581, acc.: 99.22%] [G loss: 8.954536]\n",
            "19680 [D loss: 0.057418, acc.: 98.44%] [G loss: 6.370638]\n",
            "19681 [D loss: 0.014630, acc.: 100.00%] [G loss: 6.146110]\n",
            "19682 [D loss: 0.042402, acc.: 98.24%] [G loss: 5.678998]\n",
            "19683 [D loss: 0.079197, acc.: 98.63%] [G loss: 7.247431]\n",
            "19684 [D loss: 0.091816, acc.: 97.66%] [G loss: 4.402776]\n",
            "19685 [D loss: 0.073215, acc.: 100.00%] [G loss: 10.925442]\n",
            "19686 [D loss: 0.254110, acc.: 94.53%] [G loss: 5.263317]\n",
            "19687 [D loss: 0.138925, acc.: 94.73%] [G loss: 13.318038]\n",
            "19688 [D loss: 0.148459, acc.: 97.07%] [G loss: 10.644880]\n",
            "19689 [D loss: 0.181146, acc.: 97.85%] [G loss: 6.767275]\n",
            "19690 [D loss: 0.133318, acc.: 98.05%] [G loss: 9.588044]\n",
            "19691 [D loss: 0.153107, acc.: 97.46%] [G loss: 11.655510]\n",
            "19692 [D loss: 0.260422, acc.: 96.09%] [G loss: 4.060725]\n",
            "19693 [D loss: 0.043048, acc.: 99.80%] [G loss: 6.845925]\n",
            "19694 [D loss: 0.031396, acc.: 97.66%] [G loss: 5.184703]\n",
            "19695 [D loss: 0.060050, acc.: 100.00%] [G loss: 7.795207]\n",
            "19696 [D loss: 0.123074, acc.: 97.46%] [G loss: 5.119737]\n",
            "19697 [D loss: 0.062780, acc.: 100.00%] [G loss: 7.437407]\n",
            "19698 [D loss: 0.101593, acc.: 98.05%] [G loss: 6.118114]\n",
            "19699 [D loss: 0.145518, acc.: 94.53%] [G loss: 10.045522]\n",
            "19700 [D loss: 0.270624, acc.: 96.48%] [G loss: 9.272468]\n",
            "19701 [D loss: 0.031835, acc.: 99.02%] [G loss: 6.030368]\n",
            "19702 [D loss: 0.096505, acc.: 98.63%] [G loss: 8.190804]\n",
            "19703 [D loss: 0.118013, acc.: 96.48%] [G loss: 4.742530]\n",
            "19704 [D loss: 0.108215, acc.: 99.22%] [G loss: 10.840101]\n",
            "19705 [D loss: 0.395469, acc.: 94.73%] [G loss: 5.588231]\n",
            "19706 [D loss: 0.326834, acc.: 81.25%] [G loss: 16.047817]\n",
            "19707 [D loss: 0.544113, acc.: 95.51%] [G loss: 16.632521]\n",
            "19708 [D loss: 0.822866, acc.: 94.34%] [G loss: 8.562057]\n",
            "19709 [D loss: 0.240192, acc.: 96.09%] [G loss: 5.697339]\n",
            "19710 [D loss: 0.144645, acc.: 95.90%] [G loss: 6.435788]\n",
            "19711 [D loss: 0.109052, acc.: 97.27%] [G loss: 5.067739]\n",
            "19712 [D loss: 0.082603, acc.: 98.05%] [G loss: 7.135191]\n",
            "19713 [D loss: 0.099557, acc.: 97.27%] [G loss: 5.566715]\n",
            "19714 [D loss: 0.038975, acc.: 100.00%] [G loss: 5.756169]\n",
            "19715 [D loss: 0.028841, acc.: 98.63%] [G loss: 5.684422]\n",
            "19716 [D loss: 0.053472, acc.: 100.00%] [G loss: 7.884075]\n",
            "19717 [D loss: 0.106136, acc.: 97.66%] [G loss: 5.338693]\n",
            "19718 [D loss: 0.041344, acc.: 100.00%] [G loss: 6.448257]\n",
            "19719 [D loss: 0.082470, acc.: 97.27%] [G loss: 5.565068]\n",
            "19720 [D loss: 0.070942, acc.: 98.24%] [G loss: 7.046716]\n",
            "19721 [D loss: 0.053681, acc.: 98.44%] [G loss: 6.455952]\n",
            "19722 [D loss: 0.050729, acc.: 98.44%] [G loss: 6.733133]\n",
            "19723 [D loss: 0.022928, acc.: 100.00%] [G loss: 6.031831]\n",
            "19724 [D loss: 0.072611, acc.: 98.05%] [G loss: 4.972943]\n",
            "19725 [D loss: 0.018207, acc.: 100.00%] [G loss: 6.348900]\n",
            "19726 [D loss: 0.082536, acc.: 98.24%] [G loss: 7.596582]\n",
            "19727 [D loss: 0.214166, acc.: 95.90%] [G loss: 4.465858]\n",
            "19728 [D loss: 0.147714, acc.: 96.88%] [G loss: 13.306348]\n",
            "19729 [D loss: 0.860145, acc.: 94.53%] [G loss: 7.166673]\n",
            "19730 [D loss: 0.131671, acc.: 97.07%] [G loss: 6.956453]\n",
            "19731 [D loss: 0.034116, acc.: 98.83%] [G loss: 5.693597]\n",
            "19732 [D loss: 0.083919, acc.: 98.24%] [G loss: 9.247010]\n",
            "19733 [D loss: 0.083058, acc.: 98.24%] [G loss: 7.504698]\n",
            "19734 [D loss: 0.414684, acc.: 82.23%] [G loss: 18.318687]\n",
            "19735 [D loss: 0.440939, acc.: 96.68%] [G loss: 20.344439]\n",
            "19736 [D loss: 0.878932, acc.: 95.90%] [G loss: 9.652070]\n",
            "19737 [D loss: 0.306183, acc.: 96.29%] [G loss: 5.526044]\n",
            "19738 [D loss: 0.077513, acc.: 97.66%] [G loss: 5.380152]\n",
            "19739 [D loss: 0.048535, acc.: 98.63%] [G loss: 6.997797]\n",
            "19740 [D loss: 0.082789, acc.: 98.63%] [G loss: 7.240059]\n",
            "19741 [D loss: 0.083017, acc.: 97.46%] [G loss: 5.232109]\n",
            "19742 [D loss: 0.063947, acc.: 97.46%] [G loss: 5.882634]\n",
            "19743 [D loss: 0.074688, acc.: 98.24%] [G loss: 6.766235]\n",
            "19744 [D loss: 0.087677, acc.: 97.66%] [G loss: 4.659591]\n",
            "19745 [D loss: 0.056413, acc.: 98.44%] [G loss: 5.791096]\n",
            "19746 [D loss: 0.139518, acc.: 96.68%] [G loss: 7.223257]\n",
            "19747 [D loss: 0.132165, acc.: 96.88%] [G loss: 5.052413]\n",
            "19748 [D loss: 0.142693, acc.: 96.09%] [G loss: 10.577057]\n",
            "19749 [D loss: 0.172733, acc.: 97.66%] [G loss: 9.673477]\n",
            "19750 [D loss: 0.208567, acc.: 97.07%] [G loss: 4.328859]\n",
            "19751 [D loss: 0.137353, acc.: 97.46%] [G loss: 11.791727]\n",
            "19752 [D loss: 0.284310, acc.: 97.46%] [G loss: 12.021626]\n",
            "19753 [D loss: 0.415378, acc.: 96.09%] [G loss: 5.396472]\n",
            "19754 [D loss: 0.261203, acc.: 88.09%] [G loss: 11.812014]\n",
            "19755 [D loss: 0.617921, acc.: 94.34%] [G loss: 8.885558]\n",
            "19756 [D loss: 0.305599, acc.: 94.73%] [G loss: 7.717292]\n",
            "19757 [D loss: 0.193692, acc.: 96.88%] [G loss: 5.993151]\n",
            "19758 [D loss: 0.152067, acc.: 96.48%] [G loss: 5.148709]\n",
            "19759 [D loss: 0.013841, acc.: 100.00%] [G loss: 5.503613]\n",
            "19760 [D loss: 0.031696, acc.: 100.00%] [G loss: 5.934308]\n",
            "19761 [D loss: 0.106822, acc.: 98.24%] [G loss: 7.089464]\n",
            "19762 [D loss: 0.124667, acc.: 97.46%] [G loss: 4.732997]\n",
            "19763 [D loss: 0.145526, acc.: 95.70%] [G loss: 9.887684]\n",
            "19764 [D loss: 0.176260, acc.: 96.48%] [G loss: 6.454545]\n",
            "19765 [D loss: 0.325457, acc.: 89.26%] [G loss: 11.609625]\n",
            "19766 [D loss: 0.412038, acc.: 95.12%] [G loss: 9.645512]\n",
            "19767 [D loss: 0.091520, acc.: 98.83%] [G loss: 6.252704]\n",
            "19768 [D loss: 0.213859, acc.: 95.51%] [G loss: 8.337129]\n",
            "19769 [D loss: 0.106780, acc.: 98.05%] [G loss: 7.900092]\n",
            "19770 [D loss: 0.096459, acc.: 97.46%] [G loss: 5.427278]\n",
            "19771 [D loss: 0.071448, acc.: 99.61%] [G loss: 5.880608]\n",
            "19772 [D loss: 0.070362, acc.: 97.85%] [G loss: 4.911147]\n",
            "19773 [D loss: 0.048579, acc.: 98.05%] [G loss: 5.418593]\n",
            "19774 [D loss: 0.096650, acc.: 97.27%] [G loss: 7.308206]\n",
            "19775 [D loss: 0.107762, acc.: 97.66%] [G loss: 5.645293]\n",
            "19776 [D loss: 0.049279, acc.: 98.24%] [G loss: 5.191345]\n",
            "19777 [D loss: 0.053297, acc.: 99.02%] [G loss: 5.701810]\n",
            "19778 [D loss: 0.069916, acc.: 97.85%] [G loss: 5.437353]\n",
            "19779 [D loss: 0.038188, acc.: 99.41%] [G loss: 5.167021]\n",
            "19780 [D loss: 0.090650, acc.: 98.05%] [G loss: 6.339956]\n",
            "19781 [D loss: 0.105155, acc.: 97.66%] [G loss: 4.756375]\n",
            "19782 [D loss: 0.122169, acc.: 99.80%] [G loss: 9.713850]\n",
            "19783 [D loss: 0.225656, acc.: 97.27%] [G loss: 8.507853]\n",
            "19784 [D loss: 0.141245, acc.: 97.27%] [G loss: 3.982659]\n",
            "19785 [D loss: 0.134577, acc.: 99.02%] [G loss: 11.163881]\n",
            "19786 [D loss: 0.197882, acc.: 97.07%] [G loss: 15.069619]\n",
            "19787 [D loss: 0.248690, acc.: 97.27%] [G loss: 6.140122]\n",
            "19788 [D loss: 0.193087, acc.: 96.09%] [G loss: 7.053216]\n",
            "19789 [D loss: 0.088942, acc.: 97.07%] [G loss: 5.500867]\n",
            "19790 [D loss: 0.068516, acc.: 98.63%] [G loss: 4.666255]\n",
            "19791 [D loss: 0.115318, acc.: 99.61%] [G loss: 8.060260]\n",
            "19792 [D loss: 0.139422, acc.: 97.66%] [G loss: 6.498965]\n",
            "19793 [D loss: 0.094962, acc.: 97.85%] [G loss: 4.054116]\n",
            "19794 [D loss: 0.062061, acc.: 100.00%] [G loss: 6.358989]\n",
            "19795 [D loss: 0.168648, acc.: 96.29%] [G loss: 6.163969]\n",
            "19796 [D loss: 0.121056, acc.: 97.66%] [G loss: 6.625814]\n",
            "19797 [D loss: 0.092815, acc.: 98.05%] [G loss: 4.948294]\n",
            "19798 [D loss: 0.053928, acc.: 98.63%] [G loss: 5.483738]\n",
            "19799 [D loss: 0.066963, acc.: 97.85%] [G loss: 4.789966]\n",
            "19800 [D loss: 0.059050, acc.: 97.85%] [G loss: 4.758841]\n",
            "19801 [D loss: 0.230886, acc.: 88.28%] [G loss: 12.851051]\n",
            "19802 [D loss: 0.410112, acc.: 95.51%] [G loss: 10.345742]\n",
            "19803 [D loss: 0.355182, acc.: 95.12%] [G loss: 6.524188]\n",
            "19804 [D loss: 0.152456, acc.: 95.70%] [G loss: 9.014869]\n",
            "19805 [D loss: 0.198466, acc.: 96.09%] [G loss: 7.547469]\n",
            "19806 [D loss: 0.132398, acc.: 97.85%] [G loss: 4.391629]\n",
            "19807 [D loss: 0.084187, acc.: 99.22%] [G loss: 6.942463]\n",
            "19808 [D loss: 0.106508, acc.: 97.46%] [G loss: 5.743026]\n",
            "19809 [D loss: 0.088877, acc.: 97.85%] [G loss: 5.766313]\n",
            "19810 [D loss: 0.113524, acc.: 96.68%] [G loss: 5.843379]\n",
            "19811 [D loss: 0.065313, acc.: 98.05%] [G loss: 5.416773]\n",
            "19812 [D loss: 0.132229, acc.: 96.88%] [G loss: 8.565134]\n",
            "19813 [D loss: 0.190025, acc.: 96.68%] [G loss: 4.881217]\n",
            "19814 [D loss: 0.076415, acc.: 98.05%] [G loss: 6.719284]\n",
            "19815 [D loss: 0.269810, acc.: 91.21%] [G loss: 12.915522]\n",
            "19816 [D loss: 0.319079, acc.: 96.29%] [G loss: 9.823425]\n",
            "19817 [D loss: 0.166525, acc.: 98.05%] [G loss: 6.650868]\n",
            "19818 [D loss: 0.259148, acc.: 93.16%] [G loss: 7.972739]\n",
            "19819 [D loss: 0.134354, acc.: 97.46%] [G loss: 6.134355]\n",
            "19820 [D loss: 0.092007, acc.: 98.05%] [G loss: 4.421443]\n",
            "19821 [D loss: 0.043659, acc.: 100.00%] [G loss: 5.597355]\n",
            "19822 [D loss: 0.083626, acc.: 97.27%] [G loss: 5.450616]\n",
            "19823 [D loss: 0.076782, acc.: 97.46%] [G loss: 6.133259]\n",
            "19824 [D loss: 0.039156, acc.: 98.24%] [G loss: 6.271672]\n",
            "19825 [D loss: 0.116413, acc.: 97.27%] [G loss: 8.278085]\n",
            "19826 [D loss: 0.141575, acc.: 97.46%] [G loss: 6.818347]\n",
            "19827 [D loss: 0.114336, acc.: 97.46%] [G loss: 4.609517]\n",
            "19828 [D loss: 0.103870, acc.: 97.07%] [G loss: 7.836574]\n",
            "19829 [D loss: 0.152336, acc.: 96.88%] [G loss: 5.152365]\n",
            "19830 [D loss: 0.077560, acc.: 98.83%] [G loss: 6.462468]\n",
            "19831 [D loss: 0.122867, acc.: 96.68%] [G loss: 5.251741]\n",
            "19832 [D loss: 0.047227, acc.: 98.44%] [G loss: 5.628630]\n",
            "19833 [D loss: 0.075707, acc.: 97.85%] [G loss: 6.192060]\n",
            "19834 [D loss: 0.090363, acc.: 97.85%] [G loss: 4.574070]\n",
            "19835 [D loss: 0.057634, acc.: 98.63%] [G loss: 6.136480]\n",
            "19836 [D loss: 0.089368, acc.: 97.85%] [G loss: 6.084101]\n",
            "19837 [D loss: 0.052305, acc.: 98.05%] [G loss: 5.860206]\n",
            "19838 [D loss: 0.133250, acc.: 96.48%] [G loss: 7.705572]\n",
            "19839 [D loss: 0.123409, acc.: 96.88%] [G loss: 5.221530]\n",
            "19840 [D loss: 0.326060, acc.: 88.87%] [G loss: 13.443388]\n",
            "19841 [D loss: 0.352108, acc.: 96.88%] [G loss: 12.358426]\n",
            "19842 [D loss: 0.385391, acc.: 97.27%] [G loss: 9.513236]\n",
            "19843 [D loss: 0.143530, acc.: 96.68%] [G loss: 4.539053]\n",
            "19844 [D loss: 0.177283, acc.: 92.58%] [G loss: 9.807682]\n",
            "19845 [D loss: 0.290376, acc.: 97.07%] [G loss: 10.269267]\n",
            "19846 [D loss: 0.302500, acc.: 97.46%] [G loss: 6.400822]\n",
            "19847 [D loss: 0.084264, acc.: 97.85%] [G loss: 5.587172]\n",
            "19848 [D loss: 0.079883, acc.: 97.46%] [G loss: 5.107128]\n",
            "19849 [D loss: 0.095842, acc.: 97.66%] [G loss: 6.730093]\n",
            "19850 [D loss: 0.082165, acc.: 97.27%] [G loss: 5.286610]\n",
            "19851 [D loss: 0.073452, acc.: 97.27%] [G loss: 5.707070]\n",
            "19852 [D loss: 0.103880, acc.: 96.88%] [G loss: 6.652035]\n",
            "19853 [D loss: 0.157063, acc.: 95.90%] [G loss: 4.446055]\n",
            "19854 [D loss: 0.126052, acc.: 97.07%] [G loss: 7.477433]\n",
            "19855 [D loss: 0.153180, acc.: 96.48%] [G loss: 5.207438]\n",
            "19856 [D loss: 0.179141, acc.: 94.14%] [G loss: 8.347887]\n",
            "19857 [D loss: 0.361745, acc.: 94.53%] [G loss: 4.171217]\n",
            "19858 [D loss: 0.236125, acc.: 89.45%] [G loss: 14.034391]\n",
            "19859 [D loss: 0.277770, acc.: 97.07%] [G loss: 14.280340]\n",
            "19860 [D loss: 0.504144, acc.: 96.48%] [G loss: 8.546328]\n",
            "19861 [D loss: 0.395866, acc.: 92.97%] [G loss: 5.177631]\n",
            "19862 [D loss: 0.120079, acc.: 94.34%] [G loss: 8.252808]\n",
            "19863 [D loss: 0.149790, acc.: 96.29%] [G loss: 6.317252]\n",
            "19864 [D loss: 0.187615, acc.: 90.82%] [G loss: 9.309328]\n",
            "19865 [D loss: 0.171421, acc.: 95.90%] [G loss: 6.165456]\n",
            "19866 [D loss: 0.088137, acc.: 97.85%] [G loss: 4.810450]\n",
            "19867 [D loss: 0.006965, acc.: 100.00%] [G loss: 6.216099]\n",
            "19868 [D loss: 0.016105, acc.: 100.00%] [G loss: 5.307206]\n",
            "19869 [D loss: 0.054698, acc.: 100.00%] [G loss: 8.251417]\n",
            "19870 [D loss: 0.139646, acc.: 96.88%] [G loss: 5.000102]\n",
            "19871 [D loss: 0.057164, acc.: 100.00%] [G loss: 7.618474]\n",
            "19872 [D loss: 0.071668, acc.: 98.44%] [G loss: 6.991915]\n",
            "19873 [D loss: 0.323743, acc.: 92.77%] [G loss: 10.428346]\n",
            "19874 [D loss: 0.325271, acc.: 96.09%] [G loss: 6.547194]\n",
            "19875 [D loss: 0.724553, acc.: 66.80%] [G loss: 21.764191]\n",
            "19876 [D loss: 0.673428, acc.: 95.70%] [G loss: 21.763107]\n",
            "19877 [D loss: 0.927734, acc.: 95.70%] [G loss: 15.323030]\n",
            "19878 [D loss: 0.706228, acc.: 95.90%] [G loss: 10.396223]\n",
            "19879 [D loss: 0.218845, acc.: 96.48%] [G loss: 6.911710]\n",
            "19880 [D loss: 0.178596, acc.: 96.68%] [G loss: 4.391319]\n",
            "19881 [D loss: 0.137582, acc.: 96.88%] [G loss: 6.462396]\n",
            "19882 [D loss: 0.078807, acc.: 97.46%] [G loss: 5.757461]\n",
            "19883 [D loss: 0.136416, acc.: 95.90%] [G loss: 4.725397]\n",
            "19884 [D loss: 0.045307, acc.: 98.44%] [G loss: 4.888391]\n",
            "19885 [D loss: 0.102181, acc.: 97.66%] [G loss: 5.617866]\n",
            "19886 [D loss: 0.056647, acc.: 98.63%] [G loss: 5.472045]\n",
            "19887 [D loss: 0.072323, acc.: 97.85%] [G loss: 4.729196]\n",
            "19888 [D loss: 0.084692, acc.: 97.46%] [G loss: 5.520453]\n",
            "19889 [D loss: 0.072528, acc.: 98.05%] [G loss: 4.637362]\n",
            "19890 [D loss: 0.079745, acc.: 97.27%] [G loss: 5.094720]\n",
            "19891 [D loss: 0.068341, acc.: 97.66%] [G loss: 4.861422]\n",
            "19892 [D loss: 0.105189, acc.: 97.46%] [G loss: 5.720654]\n",
            "19893 [D loss: 0.091262, acc.: 97.27%] [G loss: 4.993826]\n",
            "19894 [D loss: 0.114001, acc.: 97.07%] [G loss: 5.899714]\n",
            "19895 [D loss: 0.056681, acc.: 98.05%] [G loss: 6.026824]\n",
            "19896 [D loss: 0.132309, acc.: 96.68%] [G loss: 5.336783]\n",
            "19897 [D loss: 0.057257, acc.: 98.05%] [G loss: 5.387862]\n",
            "19898 [D loss: 0.093142, acc.: 97.46%] [G loss: 4.688951]\n",
            "19899 [D loss: 0.033175, acc.: 99.02%] [G loss: 5.450399]\n",
            "19900 [D loss: 0.068403, acc.: 98.44%] [G loss: 5.341302]\n",
            "19901 [D loss: 0.129423, acc.: 96.68%] [G loss: 5.650488]\n",
            "19902 [D loss: 0.057805, acc.: 99.22%] [G loss: 6.560133]\n",
            "19903 [D loss: 0.165844, acc.: 96.88%] [G loss: 4.551564]\n",
            "19904 [D loss: 0.033274, acc.: 100.00%] [G loss: 5.521106]\n",
            "19905 [D loss: 0.085823, acc.: 97.07%] [G loss: 5.796437]\n",
            "19906 [D loss: 0.077533, acc.: 97.66%] [G loss: 5.219369]\n",
            "19907 [D loss: 0.185155, acc.: 94.34%] [G loss: 9.616387]\n",
            "19908 [D loss: 0.249431, acc.: 95.31%] [G loss: 7.397053]\n",
            "19909 [D loss: 0.165447, acc.: 96.29%] [G loss: 5.021811]\n",
            "19910 [D loss: 0.157530, acc.: 97.07%] [G loss: 6.976905]\n",
            "19911 [D loss: 0.178627, acc.: 95.70%] [G loss: 4.114172]\n",
            "19912 [D loss: 0.107902, acc.: 97.66%] [G loss: 6.404496]\n",
            "19913 [D loss: 0.062471, acc.: 98.05%] [G loss: 5.758140]\n",
            "19914 [D loss: 0.074664, acc.: 97.85%] [G loss: 5.398893]\n",
            "19915 [D loss: 0.136573, acc.: 98.83%] [G loss: 8.545596]\n",
            "19916 [D loss: 0.177808, acc.: 96.88%] [G loss: 7.910722]\n",
            "19917 [D loss: 0.092410, acc.: 98.63%] [G loss: 6.539028]\n",
            "19918 [D loss: 0.137760, acc.: 97.07%] [G loss: 4.959156]\n",
            "19919 [D loss: 0.211433, acc.: 92.19%] [G loss: 8.451271]\n",
            "19920 [D loss: 0.192466, acc.: 97.66%] [G loss: 9.507363]\n",
            "19921 [D loss: 0.226066, acc.: 96.88%] [G loss: 4.733985]\n",
            "19922 [D loss: 0.156124, acc.: 96.09%] [G loss: 6.953499]\n",
            "19923 [D loss: 0.106039, acc.: 97.46%] [G loss: 5.964080]\n",
            "19924 [D loss: 0.120929, acc.: 96.68%] [G loss: 5.306340]\n",
            "19925 [D loss: 0.094684, acc.: 97.27%] [G loss: 5.010335]\n",
            "19926 [D loss: 0.053976, acc.: 98.24%] [G loss: 4.875684]\n",
            "19927 [D loss: 0.073993, acc.: 97.85%] [G loss: 5.678679]\n",
            "19928 [D loss: 0.075704, acc.: 97.27%] [G loss: 4.691360]\n",
            "19929 [D loss: 0.073450, acc.: 97.66%] [G loss: 4.855671]\n",
            "19930 [D loss: 0.062896, acc.: 98.24%] [G loss: 5.534401]\n",
            "19931 [D loss: 0.078488, acc.: 97.07%] [G loss: 4.591749]\n",
            "19932 [D loss: 0.062629, acc.: 99.80%] [G loss: 8.430471]\n",
            "19933 [D loss: 0.311654, acc.: 95.12%] [G loss: 6.780501]\n",
            "19934 [D loss: 0.059726, acc.: 98.05%] [G loss: 6.699866]\n",
            "19935 [D loss: 0.152682, acc.: 95.90%] [G loss: 5.398787]\n",
            "19936 [D loss: 0.033195, acc.: 98.63%] [G loss: 6.704907]\n",
            "19937 [D loss: 0.082267, acc.: 97.85%] [G loss: 6.384068]\n",
            "19938 [D loss: 0.040232, acc.: 98.63%] [G loss: 5.012379]\n",
            "19939 [D loss: 0.098158, acc.: 96.88%] [G loss: 6.895419]\n",
            "19940 [D loss: 0.126681, acc.: 96.68%] [G loss: 5.623141]\n",
            "19941 [D loss: 0.068994, acc.: 98.05%] [G loss: 5.286016]\n",
            "19942 [D loss: 0.068702, acc.: 98.05%] [G loss: 4.745747]\n",
            "19943 [D loss: 0.050509, acc.: 98.63%] [G loss: 5.183720]\n",
            "19944 [D loss: 0.066869, acc.: 97.66%] [G loss: 5.242569]\n",
            "19945 [D loss: 0.063954, acc.: 97.85%] [G loss: 4.651755]\n",
            "19946 [D loss: 0.130091, acc.: 96.68%] [G loss: 8.529721]\n",
            "19947 [D loss: 0.229872, acc.: 96.48%] [G loss: 5.807894]\n",
            "19948 [D loss: 0.204940, acc.: 94.14%] [G loss: 5.628596]\n",
            "19949 [D loss: 0.040604, acc.: 98.44%] [G loss: 6.716960]\n",
            "19950 [D loss: 0.188147, acc.: 93.36%] [G loss: 9.384723]\n",
            "19951 [D loss: 0.160520, acc.: 96.88%] [G loss: 7.817709]\n",
            "19952 [D loss: 0.145178, acc.: 97.27%] [G loss: 4.754660]\n",
            "19953 [D loss: 0.047700, acc.: 98.24%] [G loss: 5.196884]\n",
            "19954 [D loss: 0.045479, acc.: 98.83%] [G loss: 5.091609]\n",
            "19955 [D loss: 0.064274, acc.: 98.24%] [G loss: 5.865569]\n",
            "19956 [D loss: 0.064385, acc.: 98.24%] [G loss: 5.215553]\n",
            "19957 [D loss: 0.142084, acc.: 95.90%] [G loss: 7.731965]\n",
            "19958 [D loss: 0.108824, acc.: 97.07%] [G loss: 5.031031]\n",
            "19959 [D loss: 0.061775, acc.: 98.44%] [G loss: 4.879353]\n",
            "19960 [D loss: 0.016718, acc.: 99.41%] [G loss: 5.413442]\n",
            "19961 [D loss: 0.155613, acc.: 96.88%] [G loss: 7.086897]\n",
            "19962 [D loss: 0.089749, acc.: 97.66%] [G loss: 7.020903]\n",
            "19963 [D loss: 0.168626, acc.: 96.29%] [G loss: 4.591873]\n",
            "19964 [D loss: 0.098910, acc.: 97.85%] [G loss: 7.993630]\n",
            "19965 [D loss: 0.098892, acc.: 97.46%] [G loss: 5.974624]\n",
            "19966 [D loss: 0.156451, acc.: 96.09%] [G loss: 5.810807]\n",
            "19967 [D loss: 0.083102, acc.: 98.05%] [G loss: 5.027360]\n",
            "19968 [D loss: 0.240664, acc.: 91.99%] [G loss: 11.608884]\n",
            "19969 [D loss: 0.318671, acc.: 95.31%] [G loss: 6.964587]\n",
            "19970 [D loss: 0.196234, acc.: 94.92%] [G loss: 5.541463]\n",
            "19971 [D loss: 0.098499, acc.: 97.46%] [G loss: 5.733863]\n",
            "19972 [D loss: 0.119148, acc.: 96.88%] [G loss: 7.520889]\n",
            "19973 [D loss: 0.120945, acc.: 97.07%] [G loss: 6.392002]\n",
            "19974 [D loss: 0.066176, acc.: 97.85%] [G loss: 6.315031]\n",
            "19975 [D loss: 0.068084, acc.: 97.85%] [G loss: 6.082786]\n",
            "19976 [D loss: 0.173060, acc.: 95.90%] [G loss: 6.661032]\n",
            "19977 [D loss: 0.117374, acc.: 97.27%] [G loss: 5.585285]\n",
            "19978 [D loss: 0.149708, acc.: 96.09%] [G loss: 7.343723]\n",
            "19979 [D loss: 0.071940, acc.: 97.46%] [G loss: 4.589775]\n",
            "19980 [D loss: 0.068977, acc.: 98.24%] [G loss: 5.210306]\n",
            "19981 [D loss: 0.054116, acc.: 98.24%] [G loss: 5.238482]\n",
            "19982 [D loss: 0.081342, acc.: 98.24%] [G loss: 7.969537]\n",
            "19983 [D loss: 0.112214, acc.: 97.07%] [G loss: 6.102225]\n",
            "19984 [D loss: 0.082999, acc.: 97.46%] [G loss: 7.271149]\n",
            "19985 [D loss: 0.090328, acc.: 97.27%] [G loss: 4.844483]\n",
            "19986 [D loss: 0.086928, acc.: 97.66%] [G loss: 6.208921]\n",
            "19987 [D loss: 0.007626, acc.: 99.61%] [G loss: 8.880343]\n",
            "19988 [D loss: 0.095539, acc.: 98.24%] [G loss: 5.452446]\n",
            "19989 [D loss: 0.062896, acc.: 99.22%] [G loss: 5.907027]\n",
            "19990 [D loss: 0.094319, acc.: 97.66%] [G loss: 5.264580]\n",
            "19991 [D loss: 0.109291, acc.: 96.88%] [G loss: 6.206598]\n",
            "19992 [D loss: 0.077452, acc.: 98.24%] [G loss: 5.139147]\n",
            "19993 [D loss: 0.096570, acc.: 97.27%] [G loss: 6.514474]\n",
            "19994 [D loss: 0.031365, acc.: 98.44%] [G loss: 7.120027]\n",
            "19995 [D loss: 0.164245, acc.: 96.09%] [G loss: 8.550293]\n",
            "19996 [D loss: 0.185859, acc.: 96.88%] [G loss: 8.428863]\n",
            "19997 [D loss: 0.160473, acc.: 97.46%] [G loss: 4.195392]\n",
            "19998 [D loss: 0.198241, acc.: 93.36%] [G loss: 9.094257]\n",
            "19999 [D loss: 0.279202, acc.: 96.29%] [G loss: 7.601752]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8FCGznvbxoK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a3e28abc-7d08-4620-f47b-310de2464b1a"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply\n",
        "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class CGAN():\n",
        "    def __init__(self):\n",
        "        # Input shape\n",
        "        self.img_rows = 256\n",
        "        self.img_cols = 256\n",
        "        self.channels = 1\n",
        "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "        self.num_classes = 10\n",
        "        self.latent_dim = 128\n",
        "\n",
        "        optimizer = Adam(0.0001, 0.5)\n",
        "\n",
        "        # Build and compile the discriminator\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        self.discriminator.compile(loss=['binary_crossentropy'],\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "        # Build the generator\n",
        "        self.generator = self.build_generator()\n",
        "\n",
        "        # The generator takes noise and the target label as input\n",
        "        # and generates the corresponding digit of that label\n",
        "        noise = Input(shape=(self.latent_dim,))\n",
        "        label = Input(shape=(1,))\n",
        "        img = self.generator([noise, label])\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # The discriminator takes generated image as input and determines validity\n",
        "        # and the label of that image\n",
        "        valid = self.discriminator([img, label])\n",
        "\n",
        "        # The combined model  (stacked generator and discriminator)\n",
        "        # Trains generator to fool discriminator\n",
        "        self.combined = Model([noise, label], valid)\n",
        "        self.combined.compile(loss=['binary_crossentropy'],\n",
        "            optimizer=optimizer)\n",
        "\n",
        "    def build_generator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Dense(256, input_dim=self.latent_dim))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Dense(512))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Dense(1024))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
        "        model.add(Reshape(self.img_shape))\n",
        "\n",
        "        model.summary()\n",
        "\n",
        "        noise = Input(shape=(self.latent_dim,))\n",
        "        label = Input(shape=(1,), dtype='int32')\n",
        "        label_embedding = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))\n",
        "\n",
        "        model_input = multiply([noise, label_embedding])\n",
        "        img = model(model_input)\n",
        "\n",
        "        return Model([noise, label], img)\n",
        "\n",
        "    def build_discriminator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Dense(512, input_dim=np.prod(self.img_shape)))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dense(512))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.4))\n",
        "        model.add(Dense(512))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.4))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        model.summary()\n",
        "\n",
        "        img = Input(shape=self.img_shape)\n",
        "        label = Input(shape=(1,), dtype='int32')\n",
        "\n",
        "        label_embedding = Flatten()(Embedding(self.num_classes, np.prod(self.img_shape))(label))\n",
        "        flat_img = Flatten()(img)\n",
        "\n",
        "        model_input = multiply([flat_img, label_embedding])\n",
        "\n",
        "        validity = model(model_input)\n",
        "\n",
        "        return Model([img, label], validity)\n",
        "\n",
        "    def train(self, epochs, batch_size=300, sample_interval=50):\n",
        "\n",
        "        # Load the dataset\n",
        "        X_train, y_train = datagen.next()\n",
        "\n",
        "        # Configure input\n",
        "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
        "        X_train = np.expand_dims(X_train, axis=3)\n",
        "        y_train = y_train.reshape(-1, 1)\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = np.ones((batch_size, 1))\n",
        "        fake = np.zeros((batch_size, 1))\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "\n",
        "            # Select a random half batch of images\n",
        "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "            imgs, labels = X_train[idx], y_train[idx]\n",
        "\n",
        "            # Sample noise as generator input\n",
        "            noise = np.random.normal(0, 1, (batch_size, 128))\n",
        "\n",
        "            # Generate a half batch of new images\n",
        "            gen_imgs = self.generator.predict([noise, labels])\n",
        "\n",
        "            # Train the discriminator\n",
        "            d_loss_real = self.discriminator.train_on_batch([imgs, labels], valid)\n",
        "            d_loss_fake = self.discriminator.train_on_batch([gen_imgs, labels], fake)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Generator\n",
        "            # ---------------------\n",
        "\n",
        "            # Condition on labels\n",
        "            sampled_labels = np.random.randint(0, 10, batch_size).reshape(-1, 1)\n",
        "\n",
        "            # Train the generator\n",
        "            g_loss = self.combined.train_on_batch([noise, sampled_labels], valid)\n",
        "\n",
        "            # Plot the progress\n",
        "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "\n",
        "            # If at save interval => save generated image samples\n",
        "            if epoch % sample_interval == 0:\n",
        "                self.sample_images(epoch)\n",
        "\n",
        "    def sample_images(self, epoch):\n",
        "        r, c = 2, 5\n",
        "        noise = np.random.normal(0, 1, (r * c, 128))\n",
        "        sampled_labels = np.arange(0, 10).reshape(-1, 1)\n",
        "\n",
        "        gen_imgs = self.generator.predict([noise, sampled_labels])\n",
        "\n",
        "        # Rescale images 0 - 1\n",
        "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "        fig, axs = plt.subplots(r, c)\n",
        "        cnt = 0\n",
        "        for i in range(r):\n",
        "            for j in range(c):\n",
        "                axs[i,j].imshow(gen_imgs[cnt,:,:,0], cmap='gray')\n",
        "#               axs[i,j].set_title(\"Digit: %d\" % sampled_labels[cnt])\n",
        "                axs[i,j].axis('off')\n",
        "                cnt += 1\n",
        "        fig.savefig(\"/content/drive/MyDrive/Masters/Semester 2/Data Mining & Visualisation/Assessments/Assessment1/5000 EPOCHS/\" + str(epoch))\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    cgan = CGAN()\n",
        "    cgan.train(epochs=5000, batch_size=300, sample_interval=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 512)               33554944  \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 513       \n",
            "=================================================================\n",
            "Total params: 34,080,769\n",
            "Trainable params: 34,080,769\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_4 (Dense)              (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1024)              525312    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 65536)             67174400  \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 256, 256, 1)       0         \n",
            "=================================================================\n",
            "Total params: 67,871,488\n",
            "Trainable params: 67,867,904\n",
            "Non-trainable params: 3,584\n",
            "_________________________________________________________________\n",
            "0 [D loss: 0.694780, acc.: 21.67%] [G loss: 0.691976]\n",
            "1 [D loss: 0.618183, acc.: 50.00%] [G loss: 0.690686]\n",
            "2 [D loss: 0.533979, acc.: 69.50%] [G loss: 0.688833]\n",
            "3 [D loss: 0.452322, acc.: 100.00%] [G loss: 0.686920]\n",
            "4 [D loss: 0.398058, acc.: 100.00%] [G loss: 0.684606]\n",
            "5 [D loss: 0.355297, acc.: 100.00%] [G loss: 0.676157]\n",
            "6 [D loss: 0.307141, acc.: 100.00%] [G loss: 0.676755]\n",
            "7 [D loss: 0.247912, acc.: 100.00%] [G loss: 0.669778]\n",
            "8 [D loss: 0.178552, acc.: 100.00%] [G loss: 0.670725]\n",
            "9 [D loss: 0.107121, acc.: 100.00%] [G loss: 0.647517]\n",
            "10 [D loss: 0.055327, acc.: 100.00%] [G loss: 0.627890]\n",
            "11 [D loss: 0.024756, acc.: 100.00%] [G loss: 0.600524]\n",
            "12 [D loss: 0.012997, acc.: 100.00%] [G loss: 0.594429]\n",
            "13 [D loss: 0.007750, acc.: 100.00%] [G loss: 0.563602]\n",
            "14 [D loss: 0.004960, acc.: 100.00%] [G loss: 0.546730]\n",
            "15 [D loss: 0.003354, acc.: 100.00%] [G loss: 0.503316]\n",
            "16 [D loss: 0.002983, acc.: 100.00%] [G loss: 0.506840]\n",
            "17 [D loss: 0.002183, acc.: 100.00%] [G loss: 0.436626]\n",
            "18 [D loss: 0.001994, acc.: 100.00%] [G loss: 0.386716]\n",
            "19 [D loss: 0.002005, acc.: 100.00%] [G loss: 0.362112]\n",
            "20 [D loss: 0.001682, acc.: 100.00%] [G loss: 0.329679]\n",
            "21 [D loss: 0.001851, acc.: 100.00%] [G loss: 0.291878]\n",
            "22 [D loss: 0.001945, acc.: 100.00%] [G loss: 0.295753]\n",
            "23 [D loss: 0.004589, acc.: 99.83%] [G loss: 0.282674]\n",
            "24 [D loss: 0.005546, acc.: 100.00%] [G loss: 0.235451]\n",
            "25 [D loss: 0.018485, acc.: 99.83%] [G loss: 0.218972]\n",
            "26 [D loss: 0.069074, acc.: 96.33%] [G loss: 0.226794]\n",
            "27 [D loss: 0.113604, acc.: 93.83%] [G loss: 0.205708]\n",
            "28 [D loss: 0.174502, acc.: 88.50%] [G loss: 0.234402]\n",
            "29 [D loss: 0.190520, acc.: 84.50%] [G loss: 0.266248]\n",
            "30 [D loss: 0.222915, acc.: 88.00%] [G loss: 0.316518]\n",
            "31 [D loss: 0.212506, acc.: 98.67%] [G loss: 0.369338]\n",
            "32 [D loss: 0.152692, acc.: 98.17%] [G loss: 0.476232]\n",
            "33 [D loss: 0.110236, acc.: 97.67%] [G loss: 0.481162]\n",
            "34 [D loss: 0.083710, acc.: 98.83%] [G loss: 0.690280]\n",
            "35 [D loss: 0.068737, acc.: 99.00%] [G loss: 0.724787]\n",
            "36 [D loss: 0.075294, acc.: 97.67%] [G loss: 0.601982]\n",
            "37 [D loss: 0.091325, acc.: 97.00%] [G loss: 0.587422]\n",
            "38 [D loss: 0.075307, acc.: 98.17%] [G loss: 0.540791]\n",
            "39 [D loss: 0.086825, acc.: 96.83%] [G loss: 0.712739]\n",
            "40 [D loss: 0.106829, acc.: 95.67%] [G loss: 0.511449]\n",
            "41 [D loss: 0.099173, acc.: 95.50%] [G loss: 0.629565]\n",
            "42 [D loss: 0.072840, acc.: 98.17%] [G loss: 0.629301]\n",
            "43 [D loss: 0.078126, acc.: 98.33%] [G loss: 0.584818]\n",
            "44 [D loss: 0.074356, acc.: 98.00%] [G loss: 0.509529]\n",
            "45 [D loss: 0.066245, acc.: 98.67%] [G loss: 0.504537]\n",
            "46 [D loss: 0.075082, acc.: 97.67%] [G loss: 0.591795]\n",
            "47 [D loss: 0.059932, acc.: 98.33%] [G loss: 0.505508]\n",
            "48 [D loss: 0.067568, acc.: 98.83%] [G loss: 0.486775]\n",
            "49 [D loss: 0.061716, acc.: 98.00%] [G loss: 0.549566]\n",
            "50 [D loss: 0.048959, acc.: 99.33%] [G loss: 0.555777]\n",
            "51 [D loss: 0.048250, acc.: 99.50%] [G loss: 0.768373]\n",
            "52 [D loss: 0.040134, acc.: 99.83%] [G loss: 0.551387]\n",
            "53 [D loss: 0.035285, acc.: 100.00%] [G loss: 0.599624]\n",
            "54 [D loss: 0.032196, acc.: 99.83%] [G loss: 0.597252]\n",
            "55 [D loss: 0.034242, acc.: 99.67%] [G loss: 0.519632]\n",
            "56 [D loss: 0.036279, acc.: 99.67%] [G loss: 0.506596]\n",
            "57 [D loss: 0.031391, acc.: 99.83%] [G loss: 0.503274]\n",
            "58 [D loss: 0.041661, acc.: 99.33%] [G loss: 0.472742]\n",
            "59 [D loss: 0.023340, acc.: 100.00%] [G loss: 0.495856]\n",
            "60 [D loss: 0.028731, acc.: 99.50%] [G loss: 0.599388]\n",
            "61 [D loss: 0.026081, acc.: 99.67%] [G loss: 0.729179]\n",
            "62 [D loss: 0.026889, acc.: 100.00%] [G loss: 0.703881]\n",
            "63 [D loss: 0.024899, acc.: 100.00%] [G loss: 0.762884]\n",
            "64 [D loss: 0.026748, acc.: 99.83%] [G loss: 0.687794]\n",
            "65 [D loss: 0.026816, acc.: 99.83%] [G loss: 0.744549]\n",
            "66 [D loss: 0.021702, acc.: 100.00%] [G loss: 0.819856]\n",
            "67 [D loss: 0.027763, acc.: 99.67%] [G loss: 0.548610]\n",
            "68 [D loss: 0.024582, acc.: 100.00%] [G loss: 0.395345]\n",
            "69 [D loss: 0.019924, acc.: 100.00%] [G loss: 0.624914]\n",
            "70 [D loss: 0.015415, acc.: 99.83%] [G loss: 0.810434]\n",
            "71 [D loss: 0.014116, acc.: 100.00%] [G loss: 0.585829]\n",
            "72 [D loss: 0.018189, acc.: 99.83%] [G loss: 0.686455]\n",
            "73 [D loss: 0.019994, acc.: 99.67%] [G loss: 0.653235]\n",
            "74 [D loss: 0.013227, acc.: 100.00%] [G loss: 0.717851]\n",
            "75 [D loss: 0.014221, acc.: 100.00%] [G loss: 0.878404]\n",
            "76 [D loss: 0.013872, acc.: 100.00%] [G loss: 0.708776]\n",
            "77 [D loss: 0.019104, acc.: 100.00%] [G loss: 0.852164]\n",
            "78 [D loss: 0.012049, acc.: 100.00%] [G loss: 0.809010]\n",
            "79 [D loss: 0.012307, acc.: 100.00%] [G loss: 0.679023]\n",
            "80 [D loss: 0.018141, acc.: 99.67%] [G loss: 0.892491]\n",
            "81 [D loss: 0.016337, acc.: 100.00%] [G loss: 0.523080]\n",
            "82 [D loss: 0.012149, acc.: 99.83%] [G loss: 0.840428]\n",
            "83 [D loss: 0.008469, acc.: 100.00%] [G loss: 0.843624]\n",
            "84 [D loss: 0.009475, acc.: 100.00%] [G loss: 0.693748]\n",
            "85 [D loss: 0.007759, acc.: 100.00%] [G loss: 1.010710]\n",
            "86 [D loss: 0.007578, acc.: 100.00%] [G loss: 0.818522]\n",
            "87 [D loss: 0.005909, acc.: 100.00%] [G loss: 0.796419]\n",
            "88 [D loss: 0.010884, acc.: 99.83%] [G loss: 0.932541]\n",
            "89 [D loss: 0.012219, acc.: 99.67%] [G loss: 0.815834]\n",
            "90 [D loss: 0.019756, acc.: 99.67%] [G loss: 0.773386]\n",
            "91 [D loss: 0.010386, acc.: 99.83%] [G loss: 0.991092]\n",
            "92 [D loss: 0.005116, acc.: 100.00%] [G loss: 0.878724]\n",
            "93 [D loss: 0.006928, acc.: 100.00%] [G loss: 1.079664]\n",
            "94 [D loss: 0.006719, acc.: 100.00%] [G loss: 1.024172]\n",
            "95 [D loss: 0.007399, acc.: 100.00%] [G loss: 0.706564]\n",
            "96 [D loss: 0.006634, acc.: 100.00%] [G loss: 1.052959]\n",
            "97 [D loss: 0.007092, acc.: 100.00%] [G loss: 0.712648]\n",
            "98 [D loss: 0.005323, acc.: 100.00%] [G loss: 1.082525]\n",
            "99 [D loss: 0.006654, acc.: 100.00%] [G loss: 0.961377]\n",
            "100 [D loss: 0.005256, acc.: 100.00%] [G loss: 1.235831]\n",
            "101 [D loss: 0.006262, acc.: 100.00%] [G loss: 1.237845]\n",
            "102 [D loss: 0.007442, acc.: 100.00%] [G loss: 1.195321]\n",
            "103 [D loss: 0.007454, acc.: 100.00%] [G loss: 1.115600]\n",
            "104 [D loss: 0.006661, acc.: 100.00%] [G loss: 1.918061]\n",
            "105 [D loss: 0.003213, acc.: 100.00%] [G loss: 1.266672]\n",
            "106 [D loss: 0.002824, acc.: 100.00%] [G loss: 0.948831]\n",
            "107 [D loss: 0.008723, acc.: 99.67%] [G loss: 1.340262]\n",
            "108 [D loss: 0.004411, acc.: 100.00%] [G loss: 0.859003]\n",
            "109 [D loss: 0.003790, acc.: 100.00%] [G loss: 1.209315]\n",
            "110 [D loss: 0.005507, acc.: 99.83%] [G loss: 1.197561]\n",
            "111 [D loss: 0.004543, acc.: 100.00%] [G loss: 1.143350]\n",
            "112 [D loss: 0.003216, acc.: 100.00%] [G loss: 1.038488]\n",
            "113 [D loss: 0.003002, acc.: 100.00%] [G loss: 0.988813]\n",
            "114 [D loss: 0.006691, acc.: 100.00%] [G loss: 1.129381]\n",
            "115 [D loss: 0.004654, acc.: 100.00%] [G loss: 1.275656]\n",
            "116 [D loss: 0.003539, acc.: 100.00%] [G loss: 1.200717]\n",
            "117 [D loss: 0.003480, acc.: 100.00%] [G loss: 0.966750]\n",
            "118 [D loss: 0.003092, acc.: 100.00%] [G loss: 1.298915]\n",
            "119 [D loss: 0.003405, acc.: 100.00%] [G loss: 1.456136]\n",
            "120 [D loss: 0.005026, acc.: 100.00%] [G loss: 1.268713]\n",
            "121 [D loss: 0.003874, acc.: 100.00%] [G loss: 1.725864]\n",
            "122 [D loss: 0.004662, acc.: 100.00%] [G loss: 1.901152]\n",
            "123 [D loss: 0.004107, acc.: 100.00%] [G loss: 2.913572]\n",
            "124 [D loss: 0.006749, acc.: 99.83%] [G loss: 1.074059]\n",
            "125 [D loss: 0.009355, acc.: 99.83%] [G loss: 1.111602]\n",
            "126 [D loss: 0.716771, acc.: 92.00%] [G loss: 1.174460]\n",
            "127 [D loss: 4.352187, acc.: 73.17%] [G loss: 0.346270]\n",
            "128 [D loss: 0.448625, acc.: 82.33%] [G loss: 0.463218]\n",
            "129 [D loss: 0.341854, acc.: 83.67%] [G loss: 0.560042]\n",
            "130 [D loss: 0.213318, acc.: 89.00%] [G loss: 0.696258]\n",
            "131 [D loss: 0.200540, acc.: 88.00%] [G loss: 0.805481]\n",
            "132 [D loss: 0.163993, acc.: 90.33%] [G loss: 1.003593]\n",
            "133 [D loss: 0.109771, acc.: 96.67%] [G loss: 1.201588]\n",
            "134 [D loss: 0.096267, acc.: 99.00%] [G loss: 1.187145]\n",
            "135 [D loss: 0.077236, acc.: 99.83%] [G loss: 1.465774]\n",
            "136 [D loss: 0.053038, acc.: 99.83%] [G loss: 1.368679]\n",
            "137 [D loss: 0.063555, acc.: 99.33%] [G loss: 1.584762]\n",
            "138 [D loss: 0.041692, acc.: 99.83%] [G loss: 1.407553]\n",
            "139 [D loss: 0.043934, acc.: 99.83%] [G loss: 1.258029]\n",
            "140 [D loss: 0.043227, acc.: 99.83%] [G loss: 0.960429]\n",
            "141 [D loss: 0.066760, acc.: 98.67%] [G loss: 0.602141]\n",
            "142 [D loss: 0.061947, acc.: 98.33%] [G loss: 0.730754]\n",
            "143 [D loss: 0.065967, acc.: 98.17%] [G loss: 0.696996]\n",
            "144 [D loss: 0.078191, acc.: 97.83%] [G loss: 0.644599]\n",
            "145 [D loss: 0.075278, acc.: 96.83%] [G loss: 0.674477]\n",
            "146 [D loss: 0.236009, acc.: 90.00%] [G loss: 0.604393]\n",
            "147 [D loss: 0.066115, acc.: 97.33%] [G loss: 0.782786]\n",
            "148 [D loss: 0.205768, acc.: 90.17%] [G loss: 0.750711]\n",
            "149 [D loss: 0.145239, acc.: 93.17%] [G loss: 0.886524]\n",
            "150 [D loss: 0.809932, acc.: 65.67%] [G loss: 0.287033]\n",
            "151 [D loss: 0.276351, acc.: 87.83%] [G loss: 0.506380]\n",
            "152 [D loss: 0.141206, acc.: 94.83%] [G loss: 0.579772]\n",
            "153 [D loss: 0.097656, acc.: 96.50%] [G loss: 0.665787]\n",
            "154 [D loss: 0.101230, acc.: 96.83%] [G loss: 0.480610]\n",
            "155 [D loss: 0.111001, acc.: 95.00%] [G loss: 0.596647]\n",
            "156 [D loss: 0.144687, acc.: 95.00%] [G loss: 0.383679]\n",
            "157 [D loss: 0.158591, acc.: 94.50%] [G loss: 0.447958]\n",
            "158 [D loss: 0.234631, acc.: 90.67%] [G loss: 0.539504]\n",
            "159 [D loss: 0.277675, acc.: 87.50%] [G loss: 0.436133]\n",
            "160 [D loss: 0.154312, acc.: 94.50%] [G loss: 0.419513]\n",
            "161 [D loss: 0.170308, acc.: 94.50%] [G loss: 0.714784]\n",
            "162 [D loss: 0.177502, acc.: 93.17%] [G loss: 0.500023]\n",
            "163 [D loss: 0.221148, acc.: 91.83%] [G loss: 0.529942]\n",
            "164 [D loss: 0.127653, acc.: 93.50%] [G loss: 0.588757]\n",
            "165 [D loss: 0.166801, acc.: 94.17%] [G loss: 0.825526]\n",
            "166 [D loss: 0.069421, acc.: 98.33%] [G loss: 0.980362]\n",
            "167 [D loss: 0.089297, acc.: 97.50%] [G loss: 0.665053]\n",
            "168 [D loss: 0.141020, acc.: 95.83%] [G loss: 0.601051]\n",
            "169 [D loss: 0.101073, acc.: 97.00%] [G loss: 0.704081]\n",
            "170 [D loss: 0.263486, acc.: 88.83%] [G loss: 0.859610]\n",
            "171 [D loss: 0.077002, acc.: 97.67%] [G loss: 0.802763]\n",
            "172 [D loss: 0.227081, acc.: 90.83%] [G loss: 0.502979]\n",
            "173 [D loss: 0.087541, acc.: 97.50%] [G loss: 0.532913]\n",
            "174 [D loss: 0.151316, acc.: 95.00%] [G loss: 0.726091]\n",
            "175 [D loss: 0.116515, acc.: 95.83%] [G loss: 0.747351]\n",
            "176 [D loss: 0.273528, acc.: 88.33%] [G loss: 0.521452]\n",
            "177 [D loss: 0.099079, acc.: 96.83%] [G loss: 0.919908]\n",
            "178 [D loss: 0.200036, acc.: 92.67%] [G loss: 0.663638]\n",
            "179 [D loss: 0.074841, acc.: 98.67%] [G loss: 0.700917]\n",
            "180 [D loss: 0.096822, acc.: 97.83%] [G loss: 0.682695]\n",
            "181 [D loss: 0.122927, acc.: 96.83%] [G loss: 0.684674]\n",
            "182 [D loss: 0.121504, acc.: 96.83%] [G loss: 0.693733]\n",
            "183 [D loss: 0.145582, acc.: 95.17%] [G loss: 0.675277]\n",
            "184 [D loss: 0.086542, acc.: 99.17%] [G loss: 0.803838]\n",
            "185 [D loss: 0.089543, acc.: 98.00%] [G loss: 0.637219]\n",
            "186 [D loss: 0.111601, acc.: 97.50%] [G loss: 0.708545]\n",
            "187 [D loss: 0.108430, acc.: 97.83%] [G loss: 0.661013]\n",
            "188 [D loss: 0.106621, acc.: 98.00%] [G loss: 0.737163]\n",
            "189 [D loss: 0.084642, acc.: 98.33%] [G loss: 0.713347]\n",
            "190 [D loss: 0.124228, acc.: 97.67%] [G loss: 0.589613]\n",
            "191 [D loss: 0.080215, acc.: 99.00%] [G loss: 0.684085]\n",
            "192 [D loss: 0.075545, acc.: 98.67%] [G loss: 0.689341]\n",
            "193 [D loss: 0.092975, acc.: 97.83%] [G loss: 0.666556]\n",
            "194 [D loss: 0.090850, acc.: 97.67%] [G loss: 0.744707]\n",
            "195 [D loss: 0.106244, acc.: 98.00%] [G loss: 0.666151]\n",
            "196 [D loss: 0.054973, acc.: 99.33%] [G loss: 0.696363]\n",
            "197 [D loss: 0.090981, acc.: 97.83%] [G loss: 0.569079]\n",
            "198 [D loss: 0.070253, acc.: 98.67%] [G loss: 0.765893]\n",
            "199 [D loss: 0.125227, acc.: 96.50%] [G loss: 0.886529]\n",
            "200 [D loss: 0.066806, acc.: 99.00%] [G loss: 0.742721]\n",
            "201 [D loss: 0.085039, acc.: 99.00%] [G loss: 0.614933]\n",
            "202 [D loss: 0.054658, acc.: 99.50%] [G loss: 0.709202]\n",
            "203 [D loss: 0.083582, acc.: 98.17%] [G loss: 0.717620]\n",
            "204 [D loss: 0.119572, acc.: 97.00%] [G loss: 0.752811]\n",
            "205 [D loss: 0.041487, acc.: 99.67%] [G loss: 0.903993]\n",
            "206 [D loss: 0.066392, acc.: 99.17%] [G loss: 0.805428]\n",
            "207 [D loss: 0.061231, acc.: 99.00%] [G loss: 0.703796]\n",
            "208 [D loss: 0.102082, acc.: 98.50%] [G loss: 0.652217]\n",
            "209 [D loss: 0.132701, acc.: 96.67%] [G loss: 0.778523]\n",
            "210 [D loss: 0.054561, acc.: 99.83%] [G loss: 0.759579]\n",
            "211 [D loss: 0.081272, acc.: 98.50%] [G loss: 0.803938]\n",
            "212 [D loss: 0.067117, acc.: 99.50%] [G loss: 0.803382]\n",
            "213 [D loss: 0.071318, acc.: 99.50%] [G loss: 0.885901]\n",
            "214 [D loss: 0.076230, acc.: 98.83%] [G loss: 0.750799]\n",
            "215 [D loss: 0.059928, acc.: 99.67%] [G loss: 0.785730]\n",
            "216 [D loss: 0.091921, acc.: 98.50%] [G loss: 0.669890]\n",
            "217 [D loss: 0.042357, acc.: 99.83%] [G loss: 0.571793]\n",
            "218 [D loss: 0.034732, acc.: 99.83%] [G loss: 0.731522]\n",
            "219 [D loss: 0.041618, acc.: 99.83%] [G loss: 0.784728]\n",
            "220 [D loss: 0.069575, acc.: 98.83%] [G loss: 0.640104]\n",
            "221 [D loss: 0.240343, acc.: 91.33%] [G loss: 1.103873]\n",
            "222 [D loss: 0.027380, acc.: 100.00%] [G loss: 0.970555]\n",
            "223 [D loss: 0.054971, acc.: 99.50%] [G loss: 0.751982]\n",
            "224 [D loss: 0.052288, acc.: 98.67%] [G loss: 0.668441]\n",
            "225 [D loss: 0.024779, acc.: 99.83%] [G loss: 1.121344]\n",
            "226 [D loss: 0.054418, acc.: 99.33%] [G loss: 0.604871]\n",
            "227 [D loss: 0.076925, acc.: 98.17%] [G loss: 0.624153]\n",
            "228 [D loss: 0.353027, acc.: 84.00%] [G loss: 1.055195]\n",
            "229 [D loss: 0.021031, acc.: 100.00%] [G loss: 1.527321]\n",
            "230 [D loss: 0.121156, acc.: 97.00%] [G loss: 0.733536]\n",
            "231 [D loss: 0.120071, acc.: 94.83%] [G loss: 0.698965]\n",
            "232 [D loss: 0.011597, acc.: 100.00%] [G loss: 0.882708]\n",
            "233 [D loss: 0.044414, acc.: 99.67%] [G loss: 0.860137]\n",
            "234 [D loss: 0.059029, acc.: 99.17%] [G loss: 0.583718]\n",
            "235 [D loss: 0.084087, acc.: 99.33%] [G loss: 0.644540]\n",
            "236 [D loss: 0.051459, acc.: 99.83%] [G loss: 0.703523]\n",
            "237 [D loss: 0.073527, acc.: 99.33%] [G loss: 0.938399]\n",
            "238 [D loss: 0.316146, acc.: 84.67%] [G loss: 1.221351]\n",
            "239 [D loss: 0.024133, acc.: 100.00%] [G loss: 1.288033]\n",
            "240 [D loss: 0.082776, acc.: 98.17%] [G loss: 0.681149]\n",
            "241 [D loss: 0.063630, acc.: 98.17%] [G loss: 0.803795]\n",
            "242 [D loss: 0.019477, acc.: 100.00%] [G loss: 1.000462]\n",
            "243 [D loss: 0.090611, acc.: 97.67%] [G loss: 0.501062]\n",
            "244 [D loss: 0.070942, acc.: 98.83%] [G loss: 1.025261]\n",
            "245 [D loss: 0.230740, acc.: 90.67%] [G loss: 1.126168]\n",
            "246 [D loss: 0.106772, acc.: 96.67%] [G loss: 0.930988]\n",
            "247 [D loss: 0.059060, acc.: 97.83%] [G loss: 1.065742]\n",
            "248 [D loss: 0.037734, acc.: 100.00%] [G loss: 0.759371]\n",
            "249 [D loss: 0.070590, acc.: 98.17%] [G loss: 0.711407]\n",
            "250 [D loss: 0.105831, acc.: 98.50%] [G loss: 0.958023]\n",
            "251 [D loss: 0.034877, acc.: 99.83%] [G loss: 0.799378]\n",
            "252 [D loss: 0.072313, acc.: 99.17%] [G loss: 0.969782]\n",
            "253 [D loss: 0.035742, acc.: 100.00%] [G loss: 0.657039]\n",
            "254 [D loss: 0.040630, acc.: 99.67%] [G loss: 0.936430]\n",
            "255 [D loss: 0.057419, acc.: 99.17%] [G loss: 0.818833]\n",
            "256 [D loss: 0.100515, acc.: 96.67%] [G loss: 1.096830]\n",
            "257 [D loss: 0.027211, acc.: 100.00%] [G loss: 1.070153]\n",
            "258 [D loss: 0.042359, acc.: 99.83%] [G loss: 0.901441]\n",
            "259 [D loss: 0.037991, acc.: 99.33%] [G loss: 0.974670]\n",
            "260 [D loss: 0.083138, acc.: 98.50%] [G loss: 1.012024]\n",
            "261 [D loss: 0.029926, acc.: 100.00%] [G loss: 0.982208]\n",
            "262 [D loss: 0.091393, acc.: 97.50%] [G loss: 1.204345]\n",
            "263 [D loss: 0.061748, acc.: 99.00%] [G loss: 1.230027]\n",
            "264 [D loss: 0.044284, acc.: 99.00%] [G loss: 1.430065]\n",
            "265 [D loss: 0.044109, acc.: 99.67%] [G loss: 1.136167]\n",
            "266 [D loss: 0.064599, acc.: 98.67%] [G loss: 1.081190]\n",
            "267 [D loss: 0.063823, acc.: 99.00%] [G loss: 1.014663]\n",
            "268 [D loss: 0.093072, acc.: 97.83%] [G loss: 1.002230]\n",
            "269 [D loss: 0.019109, acc.: 100.00%] [G loss: 0.963324]\n",
            "270 [D loss: 0.042995, acc.: 99.33%] [G loss: 0.673264]\n",
            "271 [D loss: 0.027127, acc.: 99.83%] [G loss: 0.645956]\n",
            "272 [D loss: 0.027398, acc.: 99.83%] [G loss: 0.837411]\n",
            "273 [D loss: 0.052188, acc.: 98.50%] [G loss: 0.994892]\n",
            "274 [D loss: 0.195202, acc.: 90.33%] [G loss: 1.462517]\n",
            "275 [D loss: 0.041833, acc.: 99.33%] [G loss: 1.707714]\n",
            "276 [D loss: 0.012688, acc.: 100.00%] [G loss: 1.298783]\n",
            "277 [D loss: 0.019446, acc.: 99.33%] [G loss: 0.967302]\n",
            "278 [D loss: 0.034454, acc.: 99.50%] [G loss: 0.842049]\n",
            "279 [D loss: 0.051157, acc.: 99.00%] [G loss: 0.759639]\n",
            "280 [D loss: 0.067929, acc.: 98.50%] [G loss: 0.735795]\n",
            "281 [D loss: 0.029120, acc.: 99.67%] [G loss: 0.713608]\n",
            "282 [D loss: 0.108054, acc.: 95.50%] [G loss: 1.384533]\n",
            "283 [D loss: 0.075014, acc.: 98.33%] [G loss: 0.950492]\n",
            "284 [D loss: 0.047400, acc.: 99.00%] [G loss: 0.961743]\n",
            "285 [D loss: 0.067920, acc.: 98.33%] [G loss: 0.893342]\n",
            "286 [D loss: 0.038992, acc.: 99.17%] [G loss: 0.785344]\n",
            "287 [D loss: 0.038923, acc.: 99.33%] [G loss: 1.011233]\n",
            "288 [D loss: 0.032476, acc.: 100.00%] [G loss: 0.800803]\n",
            "289 [D loss: 0.060088, acc.: 99.00%] [G loss: 1.315920]\n",
            "290 [D loss: 0.366617, acc.: 87.17%] [G loss: 1.554013]\n",
            "291 [D loss: 0.142391, acc.: 94.00%] [G loss: 0.925888]\n",
            "292 [D loss: 0.111978, acc.: 96.00%] [G loss: 1.017855]\n",
            "293 [D loss: 0.021676, acc.: 99.83%] [G loss: 0.969015]\n",
            "294 [D loss: 0.049677, acc.: 98.83%] [G loss: 1.042241]\n",
            "295 [D loss: 0.113559, acc.: 95.17%] [G loss: 1.325522]\n",
            "296 [D loss: 0.096292, acc.: 95.67%] [G loss: 0.840925]\n",
            "297 [D loss: 0.066593, acc.: 98.33%] [G loss: 1.014200]\n",
            "298 [D loss: 0.080851, acc.: 97.67%] [G loss: 0.943296]\n",
            "299 [D loss: 0.067744, acc.: 98.00%] [G loss: 0.930812]\n",
            "300 [D loss: 0.197146, acc.: 91.17%] [G loss: 1.238667]\n",
            "301 [D loss: 0.053992, acc.: 98.83%] [G loss: 1.256061]\n",
            "302 [D loss: 0.091784, acc.: 97.67%] [G loss: 1.291651]\n",
            "303 [D loss: 0.084338, acc.: 97.67%] [G loss: 1.275248]\n",
            "304 [D loss: 0.050177, acc.: 99.00%] [G loss: 0.871647]\n",
            "305 [D loss: 0.272455, acc.: 86.33%] [G loss: 1.491826]\n",
            "306 [D loss: 1.130706, acc.: 50.00%] [G loss: 1.972807]\n",
            "307 [D loss: 0.037650, acc.: 99.67%] [G loss: 2.563663]\n",
            "308 [D loss: 0.264884, acc.: 86.17%] [G loss: 0.782523]\n",
            "309 [D loss: 0.192393, acc.: 89.67%] [G loss: 1.092337]\n",
            "310 [D loss: 0.022516, acc.: 100.00%] [G loss: 0.713287]\n",
            "311 [D loss: 0.062568, acc.: 98.83%] [G loss: 0.901000]\n",
            "312 [D loss: 0.139167, acc.: 95.67%] [G loss: 0.919383]\n",
            "313 [D loss: 0.182652, acc.: 92.33%] [G loss: 0.764267]\n",
            "314 [D loss: 0.291286, acc.: 85.67%] [G loss: 0.948419]\n",
            "315 [D loss: 0.181711, acc.: 94.00%] [G loss: 0.585146]\n",
            "316 [D loss: 0.100397, acc.: 96.67%] [G loss: 1.084832]\n",
            "317 [D loss: 0.132359, acc.: 96.00%] [G loss: 1.242994]\n",
            "318 [D loss: 0.438069, acc.: 78.83%] [G loss: 1.285410]\n",
            "319 [D loss: 0.633937, acc.: 68.50%] [G loss: 0.839467]\n",
            "320 [D loss: 0.061458, acc.: 99.00%] [G loss: 0.972966]\n",
            "321 [D loss: 0.094999, acc.: 98.00%] [G loss: 0.695002]\n",
            "322 [D loss: 0.077806, acc.: 98.33%] [G loss: 0.991006]\n",
            "323 [D loss: 0.094172, acc.: 97.50%] [G loss: 0.863911]\n",
            "324 [D loss: 0.105430, acc.: 97.33%] [G loss: 0.700291]\n",
            "325 [D loss: 0.220517, acc.: 90.67%] [G loss: 0.883273]\n",
            "326 [D loss: 0.136695, acc.: 95.67%] [G loss: 0.773108]\n",
            "327 [D loss: 0.122921, acc.: 95.83%] [G loss: 1.088427]\n",
            "328 [D loss: 0.271228, acc.: 86.67%] [G loss: 0.835298]\n",
            "329 [D loss: 0.195761, acc.: 90.50%] [G loss: 0.987962]\n",
            "330 [D loss: 0.079259, acc.: 99.00%] [G loss: 0.821260]\n",
            "331 [D loss: 0.116867, acc.: 97.17%] [G loss: 0.796904]\n",
            "332 [D loss: 0.198875, acc.: 90.33%] [G loss: 0.691988]\n",
            "333 [D loss: 0.169341, acc.: 92.83%] [G loss: 0.799455]\n",
            "334 [D loss: 0.046514, acc.: 99.50%] [G loss: 1.006131]\n",
            "335 [D loss: 0.058447, acc.: 99.33%] [G loss: 0.633511]\n",
            "336 [D loss: 0.069937, acc.: 98.33%] [G loss: 0.799004]\n",
            "337 [D loss: 0.090103, acc.: 97.33%] [G loss: 1.335102]\n",
            "338 [D loss: 0.092575, acc.: 98.17%] [G loss: 0.799168]\n",
            "339 [D loss: 0.070695, acc.: 99.17%] [G loss: 0.922932]\n",
            "340 [D loss: 0.105897, acc.: 97.50%] [G loss: 0.937132]\n",
            "341 [D loss: 0.271551, acc.: 87.83%] [G loss: 1.186286]\n",
            "342 [D loss: 0.083433, acc.: 97.67%] [G loss: 0.667778]\n",
            "343 [D loss: 0.066107, acc.: 98.83%] [G loss: 0.945210]\n",
            "344 [D loss: 0.070523, acc.: 98.00%] [G loss: 0.934294]\n",
            "345 [D loss: 0.113095, acc.: 96.50%] [G loss: 1.064738]\n",
            "346 [D loss: 1.426551, acc.: 55.00%] [G loss: 0.747527]\n",
            "347 [D loss: 0.389519, acc.: 90.00%] [G loss: 1.425749]\n",
            "348 [D loss: 0.487231, acc.: 78.17%] [G loss: 0.513069]\n",
            "349 [D loss: 0.067838, acc.: 98.17%] [G loss: 0.857591]\n",
            "350 [D loss: 0.071998, acc.: 97.50%] [G loss: 0.866468]\n",
            "351 [D loss: 0.091421, acc.: 98.17%] [G loss: 0.761990]\n",
            "352 [D loss: 0.106127, acc.: 97.33%] [G loss: 0.704641]\n",
            "353 [D loss: 0.176187, acc.: 92.83%] [G loss: 0.722011]\n",
            "354 [D loss: 0.178664, acc.: 93.83%] [G loss: 0.849486]\n",
            "355 [D loss: 0.241351, acc.: 90.50%] [G loss: 0.600187]\n",
            "356 [D loss: 0.107159, acc.: 98.17%] [G loss: 0.811134]\n",
            "357 [D loss: 0.194227, acc.: 93.00%] [G loss: 0.666190]\n",
            "358 [D loss: 0.146649, acc.: 95.83%] [G loss: 0.682016]\n",
            "359 [D loss: 0.154173, acc.: 95.67%] [G loss: 0.840739]\n",
            "360 [D loss: 0.180216, acc.: 94.17%] [G loss: 0.656602]\n",
            "361 [D loss: 0.144580, acc.: 95.50%] [G loss: 0.746685]\n",
            "362 [D loss: 0.186980, acc.: 93.67%] [G loss: 0.655891]\n",
            "363 [D loss: 0.502729, acc.: 74.83%] [G loss: 0.964344]\n",
            "364 [D loss: 0.113427, acc.: 96.67%] [G loss: 0.828367]\n",
            "365 [D loss: 0.135503, acc.: 96.67%] [G loss: 0.830668]\n",
            "366 [D loss: 0.101949, acc.: 97.50%] [G loss: 0.550290]\n",
            "367 [D loss: 0.138063, acc.: 95.67%] [G loss: 0.645288]\n",
            "368 [D loss: 0.128254, acc.: 97.17%] [G loss: 0.958322]\n",
            "369 [D loss: 0.191011, acc.: 93.17%] [G loss: 0.741377]\n",
            "370 [D loss: 0.192064, acc.: 93.17%] [G loss: 0.691752]\n",
            "371 [D loss: 0.142021, acc.: 96.50%] [G loss: 0.690720]\n",
            "372 [D loss: 0.133892, acc.: 96.67%] [G loss: 0.617440]\n",
            "373 [D loss: 0.218345, acc.: 92.33%] [G loss: 0.844477]\n",
            "374 [D loss: 0.323733, acc.: 84.67%] [G loss: 0.677954]\n",
            "375 [D loss: 0.112206, acc.: 97.00%] [G loss: 0.696014]\n",
            "376 [D loss: 0.136259, acc.: 95.50%] [G loss: 0.779843]\n",
            "377 [D loss: 0.086732, acc.: 98.67%] [G loss: 0.562747]\n",
            "378 [D loss: 0.091402, acc.: 98.67%] [G loss: 0.669682]\n",
            "379 [D loss: 0.111686, acc.: 97.67%] [G loss: 0.589992]\n",
            "380 [D loss: 0.085199, acc.: 98.67%] [G loss: 0.856723]\n",
            "381 [D loss: 0.110331, acc.: 97.33%] [G loss: 0.722285]\n",
            "382 [D loss: 0.218060, acc.: 93.33%] [G loss: 0.702811]\n",
            "383 [D loss: 0.103736, acc.: 97.83%] [G loss: 0.842165]\n",
            "384 [D loss: 0.120459, acc.: 96.83%] [G loss: 0.832587]\n",
            "385 [D loss: 0.126596, acc.: 96.00%] [G loss: 0.624878]\n",
            "386 [D loss: 0.229343, acc.: 92.00%] [G loss: 0.614152]\n",
            "387 [D loss: 0.132657, acc.: 95.50%] [G loss: 0.701062]\n",
            "388 [D loss: 0.081465, acc.: 99.50%] [G loss: 0.690535]\n",
            "389 [D loss: 0.095374, acc.: 98.50%] [G loss: 0.813669]\n",
            "390 [D loss: 0.175252, acc.: 93.00%] [G loss: 0.714010]\n",
            "391 [D loss: 0.271044, acc.: 88.33%] [G loss: 1.066429]\n",
            "392 [D loss: 0.185235, acc.: 94.00%] [G loss: 0.725645]\n",
            "393 [D loss: 0.097855, acc.: 97.83%] [G loss: 0.980153]\n",
            "394 [D loss: 0.081567, acc.: 98.17%] [G loss: 0.656978]\n",
            "395 [D loss: 0.109919, acc.: 97.33%] [G loss: 1.174721]\n",
            "396 [D loss: 0.314750, acc.: 86.17%] [G loss: 0.827875]\n",
            "397 [D loss: 0.135679, acc.: 96.50%] [G loss: 0.863643]\n",
            "398 [D loss: 0.105690, acc.: 98.33%] [G loss: 0.795293]\n",
            "399 [D loss: 0.096491, acc.: 98.83%] [G loss: 0.734487]\n",
            "400 [D loss: 0.127944, acc.: 96.33%] [G loss: 0.896437]\n",
            "401 [D loss: 0.234185, acc.: 91.33%] [G loss: 0.803451]\n",
            "402 [D loss: 0.185824, acc.: 94.33%] [G loss: 0.942333]\n",
            "403 [D loss: 0.167714, acc.: 94.50%] [G loss: 0.890130]\n",
            "404 [D loss: 0.530654, acc.: 78.50%] [G loss: 0.796638]\n",
            "405 [D loss: 1.206554, acc.: 48.50%] [G loss: 1.230250]\n",
            "406 [D loss: 0.869949, acc.: 65.17%] [G loss: 0.565867]\n",
            "407 [D loss: 0.052069, acc.: 99.00%] [G loss: 0.739067]\n",
            "408 [D loss: 0.083077, acc.: 98.67%] [G loss: 0.724793]\n",
            "409 [D loss: 0.177754, acc.: 93.50%] [G loss: 0.484647]\n",
            "410 [D loss: 0.179331, acc.: 94.00%] [G loss: 0.582346]\n",
            "411 [D loss: 0.239857, acc.: 91.17%] [G loss: 0.750663]\n",
            "412 [D loss: 0.735310, acc.: 62.33%] [G loss: 0.915566]\n",
            "413 [D loss: 0.300880, acc.: 88.50%] [G loss: 0.501238]\n",
            "414 [D loss: 0.205586, acc.: 91.50%] [G loss: 0.623451]\n",
            "415 [D loss: 0.180025, acc.: 95.33%] [G loss: 0.532155]\n",
            "416 [D loss: 0.209806, acc.: 93.67%] [G loss: 0.544581]\n",
            "417 [D loss: 0.176621, acc.: 94.50%] [G loss: 0.576436]\n",
            "418 [D loss: 0.277945, acc.: 88.00%] [G loss: 0.449280]\n",
            "419 [D loss: 0.142560, acc.: 96.67%] [G loss: 0.681691]\n",
            "420 [D loss: 0.135347, acc.: 97.33%] [G loss: 0.814383]\n",
            "421 [D loss: 0.351593, acc.: 83.17%] [G loss: 0.669634]\n",
            "422 [D loss: 0.242809, acc.: 88.67%] [G loss: 0.636448]\n",
            "423 [D loss: 0.114862, acc.: 98.50%] [G loss: 0.726676]\n",
            "424 [D loss: 0.228217, acc.: 91.83%] [G loss: 0.697965]\n",
            "425 [D loss: 0.346177, acc.: 83.00%] [G loss: 0.624271]\n",
            "426 [D loss: 0.148587, acc.: 95.17%] [G loss: 0.683285]\n",
            "427 [D loss: 0.168885, acc.: 95.33%] [G loss: 0.629155]\n",
            "428 [D loss: 0.321607, acc.: 86.17%] [G loss: 0.928892]\n",
            "429 [D loss: 0.209763, acc.: 93.33%] [G loss: 0.539230]\n",
            "430 [D loss: 0.122984, acc.: 97.17%] [G loss: 0.569454]\n",
            "431 [D loss: 0.114184, acc.: 98.00%] [G loss: 0.756235]\n",
            "432 [D loss: 0.231313, acc.: 91.50%] [G loss: 0.457331]\n",
            "433 [D loss: 0.079777, acc.: 98.17%] [G loss: 0.630900]\n",
            "434 [D loss: 0.144533, acc.: 97.00%] [G loss: 0.522878]\n",
            "435 [D loss: 0.108582, acc.: 98.67%] [G loss: 0.754941]\n",
            "436 [D loss: 0.148679, acc.: 96.50%] [G loss: 0.624312]\n",
            "437 [D loss: 0.343988, acc.: 83.33%] [G loss: 0.734537]\n",
            "438 [D loss: 0.451037, acc.: 74.50%] [G loss: 0.530291]\n",
            "439 [D loss: 0.415572, acc.: 77.17%] [G loss: 0.467264]\n",
            "440 [D loss: 0.094535, acc.: 97.00%] [G loss: 0.933129]\n",
            "441 [D loss: 0.265934, acc.: 88.50%] [G loss: 0.582886]\n",
            "442 [D loss: 0.064598, acc.: 99.50%] [G loss: 0.767136]\n",
            "443 [D loss: 0.160108, acc.: 95.83%] [G loss: 0.613737]\n",
            "444 [D loss: 0.116090, acc.: 97.33%] [G loss: 0.704270]\n",
            "445 [D loss: 0.205848, acc.: 92.17%] [G loss: 0.634417]\n",
            "446 [D loss: 0.385399, acc.: 80.17%] [G loss: 0.623503]\n",
            "447 [D loss: 0.218061, acc.: 92.00%] [G loss: 0.487064]\n",
            "448 [D loss: 0.147488, acc.: 96.17%] [G loss: 0.576528]\n",
            "449 [D loss: 0.129357, acc.: 97.33%] [G loss: 0.531233]\n",
            "450 [D loss: 0.180004, acc.: 95.00%] [G loss: 0.660873]\n",
            "451 [D loss: 0.128415, acc.: 97.83%] [G loss: 0.671994]\n",
            "452 [D loss: 0.171824, acc.: 93.83%] [G loss: 0.615128]\n",
            "453 [D loss: 0.187882, acc.: 93.17%] [G loss: 0.558424]\n",
            "454 [D loss: 0.102525, acc.: 99.00%] [G loss: 0.588890]\n",
            "455 [D loss: 0.125319, acc.: 98.17%] [G loss: 0.770990]\n",
            "456 [D loss: 0.100306, acc.: 98.83%] [G loss: 0.777310]\n",
            "457 [D loss: 0.132183, acc.: 97.50%] [G loss: 0.600165]\n",
            "458 [D loss: 0.458329, acc.: 78.33%] [G loss: 1.048165]\n",
            "459 [D loss: 0.345059, acc.: 84.17%] [G loss: 0.571385]\n",
            "460 [D loss: 0.144508, acc.: 95.17%] [G loss: 0.603036]\n",
            "461 [D loss: 0.120389, acc.: 97.83%] [G loss: 0.857593]\n",
            "462 [D loss: 0.148638, acc.: 97.17%] [G loss: 0.744670]\n",
            "463 [D loss: 0.100324, acc.: 98.00%] [G loss: 0.461204]\n",
            "464 [D loss: 0.096019, acc.: 98.67%] [G loss: 0.632001]\n",
            "465 [D loss: 0.116136, acc.: 97.50%] [G loss: 0.676649]\n",
            "466 [D loss: 0.157290, acc.: 95.17%] [G loss: 0.898837]\n",
            "467 [D loss: 0.243333, acc.: 89.67%] [G loss: 0.810755]\n",
            "468 [D loss: 0.104879, acc.: 98.50%] [G loss: 0.539740]\n",
            "469 [D loss: 0.128680, acc.: 96.50%] [G loss: 0.884479]\n",
            "470 [D loss: 0.154240, acc.: 95.67%] [G loss: 0.951403]\n",
            "471 [D loss: 0.183600, acc.: 94.50%] [G loss: 0.435668]\n",
            "472 [D loss: 0.110383, acc.: 98.50%] [G loss: 0.671188]\n",
            "473 [D loss: 0.173780, acc.: 94.33%] [G loss: 0.680483]\n",
            "474 [D loss: 0.099135, acc.: 98.33%] [G loss: 0.655663]\n",
            "475 [D loss: 0.098639, acc.: 98.33%] [G loss: 0.498306]\n",
            "476 [D loss: 0.089995, acc.: 98.17%] [G loss: 0.782444]\n",
            "477 [D loss: 0.175256, acc.: 94.50%] [G loss: 0.782230]\n",
            "478 [D loss: 0.573307, acc.: 68.67%] [G loss: 0.839734]\n",
            "479 [D loss: 0.332180, acc.: 85.33%] [G loss: 0.424799]\n",
            "480 [D loss: 0.120311, acc.: 96.83%] [G loss: 0.500293]\n",
            "481 [D loss: 0.101562, acc.: 96.67%] [G loss: 0.528312]\n",
            "482 [D loss: 0.067800, acc.: 98.17%] [G loss: 0.450097]\n",
            "483 [D loss: 0.065819, acc.: 98.83%] [G loss: 0.544100]\n",
            "484 [D loss: 0.111084, acc.: 97.17%] [G loss: 0.718811]\n",
            "485 [D loss: 0.124827, acc.: 95.83%] [G loss: 0.660480]\n",
            "486 [D loss: 0.089344, acc.: 98.33%] [G loss: 0.607579]\n",
            "487 [D loss: 0.164139, acc.: 94.17%] [G loss: 0.736748]\n",
            "488 [D loss: 0.305283, acc.: 85.67%] [G loss: 0.818574]\n",
            "489 [D loss: 0.203809, acc.: 92.17%] [G loss: 0.527095]\n",
            "490 [D loss: 0.124509, acc.: 97.00%] [G loss: 0.588660]\n",
            "491 [D loss: 0.115702, acc.: 95.83%] [G loss: 0.695937]\n",
            "492 [D loss: 0.110882, acc.: 97.17%] [G loss: 0.597570]\n",
            "493 [D loss: 0.300560, acc.: 85.33%] [G loss: 0.802278]\n",
            "494 [D loss: 0.204580, acc.: 91.50%] [G loss: 0.401149]\n",
            "495 [D loss: 0.112389, acc.: 97.17%] [G loss: 0.643599]\n",
            "496 [D loss: 0.122977, acc.: 97.00%] [G loss: 0.514909]\n",
            "497 [D loss: 0.106605, acc.: 97.17%] [G loss: 0.624925]\n",
            "498 [D loss: 0.131937, acc.: 95.33%] [G loss: 0.503050]\n",
            "499 [D loss: 0.139824, acc.: 96.17%] [G loss: 0.711375]\n",
            "500 [D loss: 0.213112, acc.: 92.00%] [G loss: 0.621272]\n",
            "501 [D loss: 0.141965, acc.: 95.50%] [G loss: 0.773739]\n",
            "502 [D loss: 0.099891, acc.: 97.00%] [G loss: 0.605893]\n",
            "503 [D loss: 0.130804, acc.: 96.33%] [G loss: 0.563064]\n",
            "504 [D loss: 0.200824, acc.: 93.33%] [G loss: 0.500646]\n",
            "505 [D loss: 0.423658, acc.: 80.00%] [G loss: 0.595556]\n",
            "506 [D loss: 0.134200, acc.: 94.67%] [G loss: 0.496377]\n",
            "507 [D loss: 0.079959, acc.: 98.33%] [G loss: 0.548840]\n",
            "508 [D loss: 0.079437, acc.: 98.50%] [G loss: 0.541178]\n",
            "509 [D loss: 0.144569, acc.: 95.33%] [G loss: 0.632472]\n",
            "510 [D loss: 0.110257, acc.: 96.33%] [G loss: 0.485337]\n",
            "511 [D loss: 0.121558, acc.: 97.67%] [G loss: 0.830546]\n",
            "512 [D loss: 0.182833, acc.: 93.33%] [G loss: 0.416101]\n",
            "513 [D loss: 0.126002, acc.: 96.50%] [G loss: 1.113648]\n",
            "514 [D loss: 0.111120, acc.: 97.17%] [G loss: 0.578897]\n",
            "515 [D loss: 0.107938, acc.: 97.50%] [G loss: 0.660750]\n",
            "516 [D loss: 0.149036, acc.: 94.83%] [G loss: 0.839650]\n",
            "517 [D loss: 0.322499, acc.: 85.33%] [G loss: 0.683712]\n",
            "518 [D loss: 0.143561, acc.: 94.00%] [G loss: 0.575450]\n",
            "519 [D loss: 0.135399, acc.: 97.33%] [G loss: 0.715254]\n",
            "520 [D loss: 0.134744, acc.: 95.33%] [G loss: 0.502878]\n",
            "521 [D loss: 0.082373, acc.: 98.67%] [G loss: 0.544049]\n",
            "522 [D loss: 0.113703, acc.: 97.33%] [G loss: 0.573970]\n",
            "523 [D loss: 0.260167, acc.: 88.17%] [G loss: 0.550483]\n",
            "524 [D loss: 0.103757, acc.: 96.67%] [G loss: 0.546937]\n",
            "525 [D loss: 0.096049, acc.: 97.67%] [G loss: 0.464409]\n",
            "526 [D loss: 0.093583, acc.: 96.33%] [G loss: 0.749771]\n",
            "527 [D loss: 0.126963, acc.: 95.83%] [G loss: 0.797443]\n",
            "528 [D loss: 0.156730, acc.: 94.00%] [G loss: 0.730187]\n",
            "529 [D loss: 0.112198, acc.: 96.67%] [G loss: 0.504931]\n",
            "530 [D loss: 0.091048, acc.: 97.50%] [G loss: 0.758568]\n",
            "531 [D loss: 0.119334, acc.: 95.50%] [G loss: 0.582674]\n",
            "532 [D loss: 0.111939, acc.: 96.67%] [G loss: 0.793132]\n",
            "533 [D loss: 0.147010, acc.: 94.83%] [G loss: 0.714457]\n",
            "534 [D loss: 0.088796, acc.: 96.83%] [G loss: 0.706638]\n",
            "535 [D loss: 0.118494, acc.: 94.50%] [G loss: 0.599314]\n",
            "536 [D loss: 0.104843, acc.: 96.67%] [G loss: 0.481119]\n",
            "537 [D loss: 0.064841, acc.: 98.00%] [G loss: 0.581825]\n",
            "538 [D loss: 0.110669, acc.: 96.83%] [G loss: 0.727595]\n",
            "539 [D loss: 0.081160, acc.: 97.17%] [G loss: 0.514529]\n",
            "540 [D loss: 0.064197, acc.: 98.67%] [G loss: 0.573020]\n",
            "541 [D loss: 0.118768, acc.: 96.17%] [G loss: 0.630332]\n",
            "542 [D loss: 0.088263, acc.: 96.50%] [G loss: 0.575109]\n",
            "543 [D loss: 0.069875, acc.: 98.83%] [G loss: 0.621772]\n",
            "544 [D loss: 0.077586, acc.: 97.83%] [G loss: 0.647498]\n",
            "545 [D loss: 0.082030, acc.: 98.50%] [G loss: 0.858431]\n",
            "546 [D loss: 0.115561, acc.: 96.67%] [G loss: 0.741012]\n",
            "547 [D loss: 0.093298, acc.: 98.50%] [G loss: 0.856596]\n",
            "548 [D loss: 0.097023, acc.: 97.17%] [G loss: 0.649190]\n",
            "549 [D loss: 0.093564, acc.: 97.67%] [G loss: 0.553844]\n",
            "550 [D loss: 0.080700, acc.: 98.67%] [G loss: 0.729976]\n",
            "551 [D loss: 0.095080, acc.: 98.00%] [G loss: 0.793084]\n",
            "552 [D loss: 0.081445, acc.: 99.00%] [G loss: 0.826139]\n",
            "553 [D loss: 0.109148, acc.: 96.83%] [G loss: 0.971149]\n",
            "554 [D loss: 0.233368, acc.: 90.67%] [G loss: 0.970691]\n",
            "555 [D loss: 0.124453, acc.: 94.50%] [G loss: 0.635419]\n",
            "556 [D loss: 0.080180, acc.: 97.83%] [G loss: 1.149514]\n",
            "557 [D loss: 0.040536, acc.: 98.83%] [G loss: 0.681843]\n",
            "558 [D loss: 0.110810, acc.: 96.83%] [G loss: 0.905305]\n",
            "559 [D loss: 0.142292, acc.: 95.50%] [G loss: 0.955383]\n",
            "560 [D loss: 0.388536, acc.: 80.00%] [G loss: 0.939189]\n",
            "561 [D loss: 0.889692, acc.: 73.17%] [G loss: 0.921750]\n",
            "562 [D loss: 0.882579, acc.: 58.83%] [G loss: 0.993687]\n",
            "563 [D loss: 0.736733, acc.: 79.83%] [G loss: 1.054053]\n",
            "564 [D loss: 0.075230, acc.: 97.67%] [G loss: 0.521663]\n",
            "565 [D loss: 0.104512, acc.: 97.33%] [G loss: 0.685400]\n",
            "566 [D loss: 0.105794, acc.: 97.50%] [G loss: 0.578164]\n",
            "567 [D loss: 0.145123, acc.: 93.50%] [G loss: 0.509866]\n",
            "568 [D loss: 0.308960, acc.: 87.50%] [G loss: 0.726874]\n",
            "569 [D loss: 0.601196, acc.: 72.50%] [G loss: 0.494048]\n",
            "570 [D loss: 0.171454, acc.: 94.50%] [G loss: 0.390078]\n",
            "571 [D loss: 0.331995, acc.: 84.33%] [G loss: 0.572202]\n",
            "572 [D loss: 0.236376, acc.: 89.67%] [G loss: 0.583439]\n",
            "573 [D loss: 0.247332, acc.: 90.50%] [G loss: 0.541405]\n",
            "574 [D loss: 0.412003, acc.: 80.50%] [G loss: 0.501867]\n",
            "575 [D loss: 0.305775, acc.: 88.00%] [G loss: 0.573786]\n",
            "576 [D loss: 0.161084, acc.: 95.00%] [G loss: 0.702182]\n",
            "577 [D loss: 0.189844, acc.: 92.17%] [G loss: 0.676395]\n",
            "578 [D loss: 0.180090, acc.: 94.33%] [G loss: 0.562025]\n",
            "579 [D loss: 0.179061, acc.: 94.50%] [G loss: 0.676387]\n",
            "580 [D loss: 0.301803, acc.: 86.83%] [G loss: 0.800969]\n",
            "581 [D loss: 0.540505, acc.: 74.00%] [G loss: 0.801907]\n",
            "582 [D loss: 0.196887, acc.: 90.33%] [G loss: 0.478920]\n",
            "583 [D loss: 0.117792, acc.: 97.67%] [G loss: 0.853782]\n",
            "584 [D loss: 0.115530, acc.: 97.33%] [G loss: 0.743667]\n",
            "585 [D loss: 0.162267, acc.: 95.50%] [G loss: 0.812587]\n",
            "586 [D loss: 0.350871, acc.: 83.50%] [G loss: 1.141968]\n",
            "587 [D loss: 0.243603, acc.: 90.67%] [G loss: 0.840529]\n",
            "588 [D loss: 0.232647, acc.: 91.83%] [G loss: 0.630741]\n",
            "589 [D loss: 0.192956, acc.: 94.33%] [G loss: 0.878670]\n",
            "590 [D loss: 0.391999, acc.: 80.83%] [G loss: 0.692697]\n",
            "591 [D loss: 0.335358, acc.: 83.67%] [G loss: 1.431088]\n",
            "592 [D loss: 0.109506, acc.: 98.17%] [G loss: 0.424835]\n",
            "593 [D loss: 0.119838, acc.: 97.17%] [G loss: 0.679186]\n",
            "594 [D loss: 0.076602, acc.: 98.67%] [G loss: 0.456683]\n",
            "595 [D loss: 0.109810, acc.: 97.83%] [G loss: 0.541470]\n",
            "596 [D loss: 0.157511, acc.: 95.83%] [G loss: 0.498356]\n",
            "597 [D loss: 0.182979, acc.: 94.67%] [G loss: 0.510048]\n",
            "598 [D loss: 0.118270, acc.: 97.00%] [G loss: 0.742525]\n",
            "599 [D loss: 0.203569, acc.: 91.50%] [G loss: 0.587082]\n",
            "600 [D loss: 0.118175, acc.: 97.00%] [G loss: 0.604389]\n",
            "601 [D loss: 0.239199, acc.: 92.17%] [G loss: 0.909983]\n",
            "602 [D loss: 0.538017, acc.: 71.50%] [G loss: 0.676943]\n",
            "603 [D loss: 0.182402, acc.: 92.00%] [G loss: 0.702419]\n",
            "604 [D loss: 0.175309, acc.: 93.83%] [G loss: 0.677315]\n",
            "605 [D loss: 0.185948, acc.: 94.67%] [G loss: 0.717601]\n",
            "606 [D loss: 0.164845, acc.: 94.83%] [G loss: 0.861251]\n",
            "607 [D loss: 0.342707, acc.: 85.33%] [G loss: 0.725267]\n",
            "608 [D loss: 0.256578, acc.: 89.67%] [G loss: 0.711076]\n",
            "609 [D loss: 0.169612, acc.: 94.00%] [G loss: 0.611154]\n",
            "610 [D loss: 0.112113, acc.: 98.17%] [G loss: 0.789621]\n",
            "611 [D loss: 0.173125, acc.: 94.50%] [G loss: 0.622361]\n",
            "612 [D loss: 0.270064, acc.: 87.67%] [G loss: 1.056920]\n",
            "613 [D loss: 0.145204, acc.: 95.17%] [G loss: 0.835446]\n",
            "614 [D loss: 0.234375, acc.: 91.50%] [G loss: 0.901661]\n",
            "615 [D loss: 0.254844, acc.: 88.33%] [G loss: 1.064635]\n",
            "616 [D loss: 0.183390, acc.: 94.33%] [G loss: 0.658902]\n",
            "617 [D loss: 0.215216, acc.: 90.83%] [G loss: 0.545105]\n",
            "618 [D loss: 0.142610, acc.: 96.50%] [G loss: 0.538143]\n",
            "619 [D loss: 0.132753, acc.: 97.50%] [G loss: 0.846547]\n",
            "620 [D loss: 0.134875, acc.: 96.33%] [G loss: 0.784468]\n",
            "621 [D loss: 0.238963, acc.: 91.33%] [G loss: 0.828752]\n",
            "622 [D loss: 0.328404, acc.: 84.67%] [G loss: 0.766502]\n",
            "623 [D loss: 0.127429, acc.: 97.00%] [G loss: 0.583789]\n",
            "624 [D loss: 0.162979, acc.: 95.83%] [G loss: 0.810824]\n",
            "625 [D loss: 0.250288, acc.: 90.83%] [G loss: 0.709614]\n",
            "626 [D loss: 0.205596, acc.: 92.17%] [G loss: 0.578412]\n",
            "627 [D loss: 0.137262, acc.: 95.83%] [G loss: 0.705929]\n",
            "628 [D loss: 0.145336, acc.: 95.50%] [G loss: 1.060084]\n",
            "629 [D loss: 0.320534, acc.: 85.67%] [G loss: 0.824985]\n",
            "630 [D loss: 0.171621, acc.: 93.67%] [G loss: 0.621943]\n",
            "631 [D loss: 0.160777, acc.: 94.67%] [G loss: 0.743464]\n",
            "632 [D loss: 0.099268, acc.: 97.50%] [G loss: 0.936443]\n",
            "633 [D loss: 0.185371, acc.: 94.33%] [G loss: 0.844683]\n",
            "634 [D loss: 0.272066, acc.: 86.00%] [G loss: 0.593746]\n",
            "635 [D loss: 0.104548, acc.: 97.00%] [G loss: 0.786676]\n",
            "636 [D loss: 0.277446, acc.: 86.17%] [G loss: 0.873257]\n",
            "637 [D loss: 0.533652, acc.: 73.00%] [G loss: 0.921035]\n",
            "638 [D loss: 0.289136, acc.: 86.67%] [G loss: 0.645680]\n",
            "639 [D loss: 0.087265, acc.: 98.00%] [G loss: 0.552268]\n",
            "640 [D loss: 0.212768, acc.: 91.17%] [G loss: 0.624506]\n",
            "641 [D loss: 0.117480, acc.: 95.50%] [G loss: 0.722890]\n",
            "642 [D loss: 0.133058, acc.: 95.83%] [G loss: 0.746115]\n",
            "643 [D loss: 0.194412, acc.: 92.50%] [G loss: 0.643296]\n",
            "644 [D loss: 0.097933, acc.: 97.50%] [G loss: 0.634779]\n",
            "645 [D loss: 0.195320, acc.: 92.83%] [G loss: 0.791277]\n",
            "646 [D loss: 0.612084, acc.: 66.67%] [G loss: 1.202878]\n",
            "647 [D loss: 0.488415, acc.: 75.17%] [G loss: 0.725794]\n",
            "648 [D loss: 0.405642, acc.: 77.50%] [G loss: 0.931515]\n",
            "649 [D loss: 0.189894, acc.: 90.50%] [G loss: 0.654650]\n",
            "650 [D loss: 0.140847, acc.: 96.50%] [G loss: 0.530611]\n",
            "651 [D loss: 0.105562, acc.: 97.67%] [G loss: 0.470283]\n",
            "652 [D loss: 0.215425, acc.: 91.67%] [G loss: 0.934396]\n",
            "653 [D loss: 0.199667, acc.: 93.83%] [G loss: 0.781492]\n",
            "654 [D loss: 0.150085, acc.: 95.83%] [G loss: 0.674541]\n",
            "655 [D loss: 0.133192, acc.: 96.50%] [G loss: 0.668866]\n",
            "656 [D loss: 0.373429, acc.: 82.50%] [G loss: 0.913137]\n",
            "657 [D loss: 0.350096, acc.: 85.17%] [G loss: 0.881768]\n",
            "658 [D loss: 0.188650, acc.: 93.33%] [G loss: 0.639686]\n",
            "659 [D loss: 0.103217, acc.: 97.83%] [G loss: 0.609502]\n",
            "660 [D loss: 0.091000, acc.: 98.50%] [G loss: 0.613855]\n",
            "661 [D loss: 0.143301, acc.: 97.17%] [G loss: 0.895608]\n",
            "662 [D loss: 0.200004, acc.: 94.33%] [G loss: 0.749559]\n",
            "663 [D loss: 0.109453, acc.: 98.33%] [G loss: 0.575905]\n",
            "664 [D loss: 0.159635, acc.: 95.50%] [G loss: 0.507889]\n",
            "665 [D loss: 0.098212, acc.: 97.67%] [G loss: 0.697901]\n",
            "666 [D loss: 0.143702, acc.: 96.67%] [G loss: 0.878083]\n",
            "667 [D loss: 0.279699, acc.: 88.00%] [G loss: 0.692835]\n",
            "668 [D loss: 0.114991, acc.: 97.17%] [G loss: 0.589190]\n",
            "669 [D loss: 0.123465, acc.: 97.67%] [G loss: 0.555059]\n",
            "670 [D loss: 0.093157, acc.: 99.00%] [G loss: 0.805082]\n",
            "671 [D loss: 0.172831, acc.: 95.17%] [G loss: 0.659839]\n",
            "672 [D loss: 0.196224, acc.: 92.83%] [G loss: 0.794899]\n",
            "673 [D loss: 0.092396, acc.: 98.00%] [G loss: 0.523392]\n",
            "674 [D loss: 0.085872, acc.: 98.50%] [G loss: 0.655732]\n",
            "675 [D loss: 0.133027, acc.: 96.50%] [G loss: 0.574470]\n",
            "676 [D loss: 0.098352, acc.: 98.33%] [G loss: 0.675170]\n",
            "677 [D loss: 0.118505, acc.: 98.00%] [G loss: 0.818671]\n",
            "678 [D loss: 0.096995, acc.: 98.67%] [G loss: 0.772065]\n",
            "679 [D loss: 0.137902, acc.: 96.17%] [G loss: 0.757797]\n",
            "680 [D loss: 0.138906, acc.: 96.17%] [G loss: 0.790681]\n",
            "681 [D loss: 0.121821, acc.: 96.67%] [G loss: 0.499138]\n",
            "682 [D loss: 0.371388, acc.: 80.67%] [G loss: 0.948268]\n",
            "683 [D loss: 0.418437, acc.: 82.33%] [G loss: 0.846308]\n",
            "684 [D loss: 0.129123, acc.: 95.17%] [G loss: 0.842412]\n",
            "685 [D loss: 0.107137, acc.: 97.83%] [G loss: 0.822572]\n",
            "686 [D loss: 0.080357, acc.: 98.33%] [G loss: 2.237133]\n",
            "687 [D loss: 0.087654, acc.: 99.00%] [G loss: 3.020902]\n",
            "688 [D loss: 0.115201, acc.: 98.00%] [G loss: 7.771225]\n",
            "689 [D loss: 0.106152, acc.: 97.67%] [G loss: 4.963204]\n",
            "690 [D loss: 0.110193, acc.: 97.83%] [G loss: 1.109391]\n",
            "691 [D loss: 0.157803, acc.: 94.50%] [G loss: 1.584121]\n",
            "692 [D loss: 0.357217, acc.: 84.33%] [G loss: 0.971021]\n",
            "693 [D loss: 0.168471, acc.: 93.17%] [G loss: 0.735916]\n",
            "694 [D loss: 0.059022, acc.: 99.83%] [G loss: 0.989384]\n",
            "695 [D loss: 0.096745, acc.: 97.33%] [G loss: 1.016379]\n",
            "696 [D loss: 0.057312, acc.: 99.50%] [G loss: 0.680319]\n",
            "697 [D loss: 0.077919, acc.: 98.83%] [G loss: 0.756733]\n",
            "698 [D loss: 0.104057, acc.: 97.33%] [G loss: 1.152992]\n",
            "699 [D loss: 0.140836, acc.: 96.00%] [G loss: 0.850231]\n",
            "700 [D loss: 0.136803, acc.: 95.83%] [G loss: 0.726359]\n",
            "701 [D loss: 0.110902, acc.: 97.33%] [G loss: 0.927452]\n",
            "702 [D loss: 0.080805, acc.: 98.67%] [G loss: 1.223308]\n",
            "703 [D loss: 0.100240, acc.: 98.00%] [G loss: 1.154076]\n",
            "704 [D loss: 0.182548, acc.: 94.00%] [G loss: 0.982441]\n",
            "705 [D loss: 0.125798, acc.: 96.00%] [G loss: 1.445825]\n",
            "706 [D loss: 0.066572, acc.: 98.17%] [G loss: 1.084713]\n",
            "707 [D loss: 0.101485, acc.: 98.17%] [G loss: 0.951760]\n",
            "708 [D loss: 0.113395, acc.: 97.50%] [G loss: 0.934429]\n",
            "709 [D loss: 0.117182, acc.: 96.83%] [G loss: 0.798140]\n",
            "710 [D loss: 0.368427, acc.: 81.00%] [G loss: 0.915816]\n",
            "711 [D loss: 0.535821, acc.: 70.83%] [G loss: 1.151257]\n",
            "712 [D loss: 0.332071, acc.: 83.67%] [G loss: 1.095319]\n",
            "713 [D loss: 0.039094, acc.: 99.83%] [G loss: 0.527641]\n",
            "714 [D loss: 0.054115, acc.: 99.50%] [G loss: 1.213630]\n",
            "715 [D loss: 0.069438, acc.: 99.17%] [G loss: 0.600804]\n",
            "716 [D loss: 0.066803, acc.: 98.50%] [G loss: 0.821748]\n",
            "717 [D loss: 0.119463, acc.: 96.50%] [G loss: 0.706587]\n",
            "718 [D loss: 0.086374, acc.: 98.67%] [G loss: 1.186468]\n",
            "719 [D loss: 0.100457, acc.: 98.33%] [G loss: 0.796371]\n",
            "720 [D loss: 0.173255, acc.: 94.17%] [G loss: 0.910930]\n",
            "721 [D loss: 0.133727, acc.: 96.17%] [G loss: 0.655417]\n",
            "722 [D loss: 0.072906, acc.: 99.33%] [G loss: 0.799883]\n",
            "723 [D loss: 0.103295, acc.: 97.50%] [G loss: 0.771297]\n",
            "724 [D loss: 0.110240, acc.: 97.67%] [G loss: 0.852744]\n",
            "725 [D loss: 0.195744, acc.: 92.17%] [G loss: 0.972831]\n",
            "726 [D loss: 0.162860, acc.: 94.50%] [G loss: 0.841447]\n",
            "727 [D loss: 0.056082, acc.: 99.83%] [G loss: 0.486214]\n",
            "728 [D loss: 0.068903, acc.: 99.00%] [G loss: 0.850734]\n",
            "729 [D loss: 0.075131, acc.: 99.00%] [G loss: 0.960308]\n",
            "730 [D loss: 0.094062, acc.: 98.50%] [G loss: 1.024029]\n",
            "731 [D loss: 0.080068, acc.: 98.83%] [G loss: 0.778353]\n",
            "732 [D loss: 0.110962, acc.: 96.50%] [G loss: 1.160574]\n",
            "733 [D loss: 0.705041, acc.: 67.33%] [G loss: 1.649665]\n",
            "734 [D loss: 2.510726, acc.: 33.50%] [G loss: 1.344204]\n",
            "735 [D loss: 0.481849, acc.: 82.33%] [G loss: 0.667925]\n",
            "736 [D loss: 0.196499, acc.: 91.50%] [G loss: 0.476334]\n",
            "737 [D loss: 0.058583, acc.: 99.50%] [G loss: 0.566393]\n",
            "738 [D loss: 0.055697, acc.: 99.50%] [G loss: 0.807031]\n",
            "739 [D loss: 0.054694, acc.: 100.00%] [G loss: 0.495060]\n",
            "740 [D loss: 0.100060, acc.: 98.50%] [G loss: 0.345553]\n",
            "741 [D loss: 0.093555, acc.: 97.83%] [G loss: 0.457514]\n",
            "742 [D loss: 0.118018, acc.: 97.17%] [G loss: 0.690054]\n",
            "743 [D loss: 0.129982, acc.: 97.00%] [G loss: 0.334542]\n",
            "744 [D loss: 0.131737, acc.: 97.00%] [G loss: 0.605260]\n",
            "745 [D loss: 0.178090, acc.: 94.67%] [G loss: 1.057116]\n",
            "746 [D loss: 0.260366, acc.: 90.00%] [G loss: 1.333368]\n",
            "747 [D loss: 0.361685, acc.: 85.00%] [G loss: 0.845713]\n",
            "748 [D loss: 0.208770, acc.: 93.33%] [G loss: 0.568226]\n",
            "749 [D loss: 0.147841, acc.: 94.83%] [G loss: 0.538697]\n",
            "750 [D loss: 0.152750, acc.: 94.83%] [G loss: 0.627426]\n",
            "751 [D loss: 0.149760, acc.: 95.83%] [G loss: 0.592525]\n",
            "752 [D loss: 0.139269, acc.: 96.00%] [G loss: 0.632339]\n",
            "753 [D loss: 0.137816, acc.: 96.17%] [G loss: 0.794055]\n",
            "754 [D loss: 0.098466, acc.: 98.50%] [G loss: 0.610014]\n",
            "755 [D loss: 0.137409, acc.: 97.33%] [G loss: 0.775054]\n",
            "756 [D loss: 0.169683, acc.: 95.33%] [G loss: 0.680321]\n",
            "757 [D loss: 0.115563, acc.: 98.00%] [G loss: 0.589233]\n",
            "758 [D loss: 0.088643, acc.: 98.50%] [G loss: 0.595466]\n",
            "759 [D loss: 0.109774, acc.: 98.17%] [G loss: 0.777368]\n",
            "760 [D loss: 0.101767, acc.: 98.33%] [G loss: 0.719799]\n",
            "761 [D loss: 0.114802, acc.: 96.83%] [G loss: 0.721021]\n",
            "762 [D loss: 0.098322, acc.: 97.50%] [G loss: 0.954022]\n",
            "763 [D loss: 0.166386, acc.: 94.83%] [G loss: 0.353128]\n",
            "764 [D loss: 0.067350, acc.: 99.50%] [G loss: 0.751021]\n",
            "765 [D loss: 0.099868, acc.: 97.83%] [G loss: 0.892810]\n",
            "766 [D loss: 0.126662, acc.: 96.67%] [G loss: 1.154424]\n",
            "767 [D loss: 0.086364, acc.: 98.00%] [G loss: 0.556187]\n",
            "768 [D loss: 0.120651, acc.: 97.83%] [G loss: 0.662964]\n",
            "769 [D loss: 0.157701, acc.: 96.33%] [G loss: 0.836140]\n",
            "770 [D loss: 0.094661, acc.: 98.33%] [G loss: 0.760777]\n",
            "771 [D loss: 0.134656, acc.: 95.67%] [G loss: 0.778977]\n",
            "772 [D loss: 0.102832, acc.: 97.83%] [G loss: 0.505702]\n",
            "773 [D loss: 0.082497, acc.: 98.83%] [G loss: 0.794665]\n",
            "774 [D loss: 0.115810, acc.: 98.17%] [G loss: 0.631121]\n",
            "775 [D loss: 0.092926, acc.: 98.33%] [G loss: 0.776659]\n",
            "776 [D loss: 0.089330, acc.: 98.67%] [G loss: 0.977794]\n",
            "777 [D loss: 0.153898, acc.: 95.33%] [G loss: 0.933050]\n",
            "778 [D loss: 0.067519, acc.: 99.67%] [G loss: 1.164425]\n",
            "779 [D loss: 0.116898, acc.: 96.33%] [G loss: 1.008184]\n",
            "780 [D loss: 0.284725, acc.: 89.67%] [G loss: 0.857067]\n",
            "781 [D loss: 0.129842, acc.: 95.50%] [G loss: 0.704646]\n",
            "782 [D loss: 0.092490, acc.: 97.33%] [G loss: 0.603686]\n",
            "783 [D loss: 0.090805, acc.: 98.33%] [G loss: 0.667205]\n",
            "784 [D loss: 0.073419, acc.: 98.67%] [G loss: 1.081497]\n",
            "785 [D loss: 0.098780, acc.: 98.17%] [G loss: 0.720841]\n",
            "786 [D loss: 0.081078, acc.: 98.67%] [G loss: 0.821701]\n",
            "787 [D loss: 0.126462, acc.: 96.83%] [G loss: 0.578344]\n",
            "788 [D loss: 0.203948, acc.: 92.83%] [G loss: 1.044517]\n",
            "789 [D loss: 0.096833, acc.: 98.83%] [G loss: 0.731602]\n",
            "790 [D loss: 0.072655, acc.: 99.00%] [G loss: 0.518295]\n",
            "791 [D loss: 0.087596, acc.: 98.17%] [G loss: 0.667645]\n",
            "792 [D loss: 0.148311, acc.: 96.00%] [G loss: 0.612795]\n",
            "793 [D loss: 0.070061, acc.: 99.50%] [G loss: 0.599979]\n",
            "794 [D loss: 0.069001, acc.: 99.00%] [G loss: 0.594939]\n",
            "795 [D loss: 0.078892, acc.: 99.33%] [G loss: 0.645925]\n",
            "796 [D loss: 0.099388, acc.: 98.67%] [G loss: 1.068064]\n",
            "797 [D loss: 0.092834, acc.: 98.33%] [G loss: 0.630881]\n",
            "798 [D loss: 0.071861, acc.: 99.17%] [G loss: 0.743124]\n",
            "799 [D loss: 0.107496, acc.: 97.00%] [G loss: 0.693387]\n",
            "800 [D loss: 0.200501, acc.: 93.83%] [G loss: 0.773997]\n",
            "801 [D loss: 0.093762, acc.: 98.50%] [G loss: 0.588714]\n",
            "802 [D loss: 0.071530, acc.: 98.33%] [G loss: 0.649425]\n",
            "803 [D loss: 0.078931, acc.: 99.00%] [G loss: 0.506461]\n",
            "804 [D loss: 0.056072, acc.: 99.00%] [G loss: 0.786500]\n",
            "805 [D loss: 0.069465, acc.: 99.00%] [G loss: 1.045066]\n",
            "806 [D loss: 0.104200, acc.: 97.00%] [G loss: 0.592055]\n",
            "807 [D loss: 0.097007, acc.: 98.00%] [G loss: 0.786324]\n",
            "808 [D loss: 0.096968, acc.: 98.50%] [G loss: 0.908396]\n",
            "809 [D loss: 0.106360, acc.: 98.00%] [G loss: 1.101262]\n",
            "810 [D loss: 0.141675, acc.: 95.83%] [G loss: 1.189063]\n",
            "811 [D loss: 0.073775, acc.: 98.83%] [G loss: 0.726969]\n",
            "812 [D loss: 0.165602, acc.: 93.50%] [G loss: 1.101141]\n",
            "813 [D loss: 0.104496, acc.: 98.17%] [G loss: 1.043826]\n",
            "814 [D loss: 0.058677, acc.: 99.17%] [G loss: 0.566495]\n",
            "815 [D loss: 0.062817, acc.: 99.00%] [G loss: 0.679233]\n",
            "816 [D loss: 0.082216, acc.: 98.83%] [G loss: 0.717942]\n",
            "817 [D loss: 0.061718, acc.: 99.33%] [G loss: 0.982247]\n",
            "818 [D loss: 0.094809, acc.: 97.67%] [G loss: 0.892011]\n",
            "819 [D loss: 0.264398, acc.: 88.50%] [G loss: 1.170957]\n",
            "820 [D loss: 0.236484, acc.: 88.67%] [G loss: 0.814254]\n",
            "821 [D loss: 0.046994, acc.: 99.33%] [G loss: 0.839032]\n",
            "822 [D loss: 0.068582, acc.: 98.83%] [G loss: 0.584159]\n",
            "823 [D loss: 0.042442, acc.: 99.50%] [G loss: 0.716092]\n",
            "824 [D loss: 0.060997, acc.: 99.33%] [G loss: 0.660363]\n",
            "825 [D loss: 0.079197, acc.: 98.33%] [G loss: 0.880092]\n",
            "826 [D loss: 0.080724, acc.: 99.17%] [G loss: 0.539517]\n",
            "827 [D loss: 0.130224, acc.: 97.00%] [G loss: 1.050195]\n",
            "828 [D loss: 0.118506, acc.: 98.33%] [G loss: 0.646958]\n",
            "829 [D loss: 0.089294, acc.: 99.17%] [G loss: 1.140256]\n",
            "830 [D loss: 0.085867, acc.: 98.67%] [G loss: 1.329797]\n",
            "831 [D loss: 0.054401, acc.: 99.83%] [G loss: 1.273690]\n",
            "832 [D loss: 0.065916, acc.: 98.83%] [G loss: 0.732875]\n",
            "833 [D loss: 0.118019, acc.: 97.50%] [G loss: 0.982034]\n",
            "834 [D loss: 0.069197, acc.: 99.67%] [G loss: 0.751679]\n",
            "835 [D loss: 0.079258, acc.: 99.00%] [G loss: 0.558365]\n",
            "836 [D loss: 0.137889, acc.: 97.17%] [G loss: 0.690801]\n",
            "837 [D loss: 0.091886, acc.: 98.17%] [G loss: 0.862288]\n",
            "838 [D loss: 0.056051, acc.: 98.67%] [G loss: 0.670659]\n",
            "839 [D loss: 0.061558, acc.: 99.50%] [G loss: 0.784858]\n",
            "840 [D loss: 0.055721, acc.: 99.50%] [G loss: 0.845280]\n",
            "841 [D loss: 0.083395, acc.: 99.00%] [G loss: 0.935994]\n",
            "842 [D loss: 0.131901, acc.: 96.17%] [G loss: 0.798008]\n",
            "843 [D loss: 0.265252, acc.: 86.67%] [G loss: 1.217310]\n",
            "844 [D loss: 0.854589, acc.: 67.50%] [G loss: 2.183534]\n",
            "845 [D loss: 4.073087, acc.: 23.67%] [G loss: 1.149613]\n",
            "846 [D loss: 0.382874, acc.: 85.33%] [G loss: 1.533696]\n",
            "847 [D loss: 0.278881, acc.: 85.67%] [G loss: 0.621386]\n",
            "848 [D loss: 0.100638, acc.: 96.33%] [G loss: 0.409865]\n",
            "849 [D loss: 0.044439, acc.: 99.33%] [G loss: 0.499153]\n",
            "850 [D loss: 0.045040, acc.: 99.67%] [G loss: 0.329661]\n",
            "851 [D loss: 0.051974, acc.: 99.83%] [G loss: 0.550013]\n",
            "852 [D loss: 0.081552, acc.: 99.17%] [G loss: 0.421426]\n",
            "853 [D loss: 0.081532, acc.: 99.33%] [G loss: 0.466902]\n",
            "854 [D loss: 0.096034, acc.: 98.67%] [G loss: 0.541695]\n",
            "855 [D loss: 0.107118, acc.: 98.17%] [G loss: 0.517430]\n",
            "856 [D loss: 0.167799, acc.: 95.33%] [G loss: 0.477740]\n",
            "857 [D loss: 0.161592, acc.: 95.17%] [G loss: 0.580227]\n",
            "858 [D loss: 0.243471, acc.: 90.50%] [G loss: 0.446439]\n",
            "859 [D loss: 0.225206, acc.: 91.00%] [G loss: 0.508164]\n",
            "860 [D loss: 0.147731, acc.: 95.50%] [G loss: 0.545282]\n",
            "861 [D loss: 0.144868, acc.: 96.00%] [G loss: 0.480684]\n",
            "862 [D loss: 0.162137, acc.: 95.67%] [G loss: 0.550559]\n",
            "863 [D loss: 0.184210, acc.: 94.67%] [G loss: 0.367837]\n",
            "864 [D loss: 0.174544, acc.: 94.00%] [G loss: 0.815529]\n",
            "865 [D loss: 0.154264, acc.: 95.00%] [G loss: 0.777855]\n",
            "866 [D loss: 0.176859, acc.: 94.17%] [G loss: 0.538885]\n",
            "867 [D loss: 0.156816, acc.: 94.33%] [G loss: 0.469578]\n",
            "868 [D loss: 0.130971, acc.: 96.00%] [G loss: 0.606473]\n",
            "869 [D loss: 0.135312, acc.: 95.33%] [G loss: 0.687366]\n",
            "870 [D loss: 0.144055, acc.: 96.67%] [G loss: 0.819206]\n",
            "871 [D loss: 0.119131, acc.: 96.33%] [G loss: 0.818321]\n",
            "872 [D loss: 0.121555, acc.: 98.00%] [G loss: 0.653361]\n",
            "873 [D loss: 0.110780, acc.: 97.33%] [G loss: 0.584587]\n",
            "874 [D loss: 0.126205, acc.: 96.17%] [G loss: 0.740421]\n",
            "875 [D loss: 0.118101, acc.: 97.67%] [G loss: 0.636020]\n",
            "876 [D loss: 0.088851, acc.: 99.00%] [G loss: 0.561800]\n",
            "877 [D loss: 0.096670, acc.: 99.00%] [G loss: 0.611333]\n",
            "878 [D loss: 0.092027, acc.: 98.67%] [G loss: 0.736530]\n",
            "879 [D loss: 0.079322, acc.: 98.83%] [G loss: 1.498857]\n",
            "880 [D loss: 0.074117, acc.: 99.33%] [G loss: 0.802080]\n",
            "881 [D loss: 0.084109, acc.: 99.00%] [G loss: 0.863467]\n",
            "882 [D loss: 0.139378, acc.: 96.00%] [G loss: 0.950233]\n",
            "883 [D loss: 0.088194, acc.: 98.17%] [G loss: 0.883276]\n",
            "884 [D loss: 0.092529, acc.: 97.83%] [G loss: 1.060284]\n",
            "885 [D loss: 0.235200, acc.: 89.83%] [G loss: 0.712912]\n",
            "886 [D loss: 0.125552, acc.: 95.33%] [G loss: 0.607925]\n",
            "887 [D loss: 0.071255, acc.: 98.50%] [G loss: 0.977599]\n",
            "888 [D loss: 0.059406, acc.: 99.33%] [G loss: 0.797306]\n",
            "889 [D loss: 0.083823, acc.: 99.67%] [G loss: 0.456456]\n",
            "890 [D loss: 0.057878, acc.: 99.00%] [G loss: 0.687448]\n",
            "891 [D loss: 0.109788, acc.: 97.50%] [G loss: 0.712627]\n",
            "892 [D loss: 0.220064, acc.: 91.00%] [G loss: 0.829903]\n",
            "893 [D loss: 0.045721, acc.: 99.17%] [G loss: 0.661117]\n",
            "894 [D loss: 0.107652, acc.: 97.67%] [G loss: 0.795971]\n",
            "895 [D loss: 0.049055, acc.: 99.33%] [G loss: 1.037728]\n",
            "896 [D loss: 0.069962, acc.: 99.33%] [G loss: 0.585057]\n",
            "897 [D loss: 0.061649, acc.: 99.00%] [G loss: 0.604877]\n",
            "898 [D loss: 0.071292, acc.: 99.33%] [G loss: 0.899972]\n",
            "899 [D loss: 0.122099, acc.: 97.33%] [G loss: 0.795007]\n",
            "900 [D loss: 0.095128, acc.: 98.33%] [G loss: 0.590602]\n",
            "901 [D loss: 0.059284, acc.: 99.33%] [G loss: 0.749928]\n",
            "902 [D loss: 0.078149, acc.: 98.67%] [G loss: 0.853797]\n",
            "903 [D loss: 0.087911, acc.: 98.67%] [G loss: 0.509351]\n",
            "904 [D loss: 0.057194, acc.: 99.17%] [G loss: 0.686272]\n",
            "905 [D loss: 0.109667, acc.: 97.50%] [G loss: 0.734591]\n",
            "906 [D loss: 0.059617, acc.: 99.33%] [G loss: 0.537712]\n",
            "907 [D loss: 0.103665, acc.: 99.00%] [G loss: 0.658846]\n",
            "908 [D loss: 0.069033, acc.: 99.17%] [G loss: 0.827967]\n",
            "909 [D loss: 0.070677, acc.: 99.33%] [G loss: 0.673381]\n",
            "910 [D loss: 0.069781, acc.: 99.17%] [G loss: 0.795741]\n",
            "911 [D loss: 0.110065, acc.: 97.67%] [G loss: 0.875998]\n",
            "912 [D loss: 0.158130, acc.: 96.00%] [G loss: 0.926296]\n",
            "913 [D loss: 0.124665, acc.: 96.33%] [G loss: 0.827507]\n",
            "914 [D loss: 0.058510, acc.: 99.33%] [G loss: 0.547928]\n",
            "915 [D loss: 0.090378, acc.: 99.17%] [G loss: 0.929522]\n",
            "916 [D loss: 0.076803, acc.: 98.50%] [G loss: 0.813618]\n",
            "917 [D loss: 0.102383, acc.: 97.83%] [G loss: 1.058298]\n",
            "918 [D loss: 0.077331, acc.: 99.33%] [G loss: 0.627021]\n",
            "919 [D loss: 0.062674, acc.: 99.67%] [G loss: 0.864913]\n",
            "920 [D loss: 0.070250, acc.: 99.00%] [G loss: 0.727744]\n",
            "921 [D loss: 0.156771, acc.: 94.67%] [G loss: 0.875461]\n",
            "922 [D loss: 0.168991, acc.: 94.50%] [G loss: 1.029632]\n",
            "923 [D loss: 0.080828, acc.: 98.83%] [G loss: 0.598394]\n",
            "924 [D loss: 0.060496, acc.: 99.67%] [G loss: 0.605419]\n",
            "925 [D loss: 0.063458, acc.: 98.83%] [G loss: 0.829529]\n",
            "926 [D loss: 0.097002, acc.: 97.67%] [G loss: 0.612842]\n",
            "927 [D loss: 0.067551, acc.: 99.50%] [G loss: 0.736754]\n",
            "928 [D loss: 0.101453, acc.: 98.17%] [G loss: 0.619109]\n",
            "929 [D loss: 0.048647, acc.: 99.67%] [G loss: 0.725453]\n",
            "930 [D loss: 0.095532, acc.: 98.00%] [G loss: 0.957723]\n",
            "931 [D loss: 0.069048, acc.: 99.17%] [G loss: 0.756005]\n",
            "932 [D loss: 0.136815, acc.: 97.00%] [G loss: 0.751467]\n",
            "933 [D loss: 0.215008, acc.: 91.83%] [G loss: 0.627350]\n",
            "934 [D loss: 0.071525, acc.: 99.00%] [G loss: 0.590643]\n",
            "935 [D loss: 0.075397, acc.: 99.00%] [G loss: 0.722869]\n",
            "936 [D loss: 0.055152, acc.: 99.00%] [G loss: 0.572183]\n",
            "937 [D loss: 0.044492, acc.: 99.67%] [G loss: 0.693460]\n",
            "938 [D loss: 0.062725, acc.: 99.00%] [G loss: 0.701240]\n",
            "939 [D loss: 0.064267, acc.: 99.17%] [G loss: 0.784267]\n",
            "940 [D loss: 0.075153, acc.: 98.67%] [G loss: 0.638973]\n",
            "941 [D loss: 0.073231, acc.: 99.17%] [G loss: 0.648164]\n",
            "942 [D loss: 0.059236, acc.: 99.17%] [G loss: 0.676973]\n",
            "943 [D loss: 0.072200, acc.: 99.50%] [G loss: 0.449034]\n",
            "944 [D loss: 0.058424, acc.: 99.17%] [G loss: 0.614155]\n",
            "945 [D loss: 0.064764, acc.: 99.00%] [G loss: 0.378276]\n",
            "946 [D loss: 0.052406, acc.: 99.83%] [G loss: 0.958362]\n",
            "947 [D loss: 0.059967, acc.: 99.50%] [G loss: 1.030476]\n",
            "948 [D loss: 0.061805, acc.: 98.83%] [G loss: 0.751028]\n",
            "949 [D loss: 0.116112, acc.: 98.00%] [G loss: 0.847626]\n",
            "950 [D loss: 0.060789, acc.: 99.17%] [G loss: 0.528543]\n",
            "951 [D loss: 0.094121, acc.: 98.17%] [G loss: 0.656597]\n",
            "952 [D loss: 0.071259, acc.: 99.00%] [G loss: 0.625004]\n",
            "953 [D loss: 0.093028, acc.: 98.67%] [G loss: 0.596552]\n",
            "954 [D loss: 0.054418, acc.: 99.17%] [G loss: 0.694929]\n",
            "955 [D loss: 0.049877, acc.: 99.33%] [G loss: 0.636649]\n",
            "956 [D loss: 0.056892, acc.: 99.67%] [G loss: 0.738990]\n",
            "957 [D loss: 0.057373, acc.: 99.33%] [G loss: 0.454394]\n",
            "958 [D loss: 0.105184, acc.: 97.83%] [G loss: 0.796296]\n",
            "959 [D loss: 0.036158, acc.: 100.00%] [G loss: 0.756161]\n",
            "960 [D loss: 0.072793, acc.: 98.83%] [G loss: 0.839267]\n",
            "961 [D loss: 0.077409, acc.: 99.50%] [G loss: 0.640456]\n",
            "962 [D loss: 0.048888, acc.: 99.83%] [G loss: 0.664763]\n",
            "963 [D loss: 0.047559, acc.: 99.83%] [G loss: 0.466877]\n",
            "964 [D loss: 0.067015, acc.: 99.33%] [G loss: 0.947544]\n",
            "965 [D loss: 0.131451, acc.: 96.83%] [G loss: 0.771039]\n",
            "966 [D loss: 0.055185, acc.: 99.17%] [G loss: 0.520780]\n",
            "967 [D loss: 0.052165, acc.: 99.00%] [G loss: 0.853656]\n",
            "968 [D loss: 0.067404, acc.: 99.00%] [G loss: 0.708718]\n",
            "969 [D loss: 0.041499, acc.: 100.00%] [G loss: 0.683734]\n",
            "970 [D loss: 0.104211, acc.: 98.33%] [G loss: 0.622510]\n",
            "971 [D loss: 0.066848, acc.: 99.00%] [G loss: 0.628949]\n",
            "972 [D loss: 0.042299, acc.: 99.50%] [G loss: 0.725919]\n",
            "973 [D loss: 0.044018, acc.: 99.67%] [G loss: 1.016291]\n",
            "974 [D loss: 0.046002, acc.: 99.17%] [G loss: 0.811159]\n",
            "975 [D loss: 0.049144, acc.: 99.33%] [G loss: 1.115381]\n",
            "976 [D loss: 0.048308, acc.: 99.67%] [G loss: 0.730517]\n",
            "977 [D loss: 0.052177, acc.: 99.33%] [G loss: 0.643019]\n",
            "978 [D loss: 0.064818, acc.: 99.50%] [G loss: 0.591107]\n",
            "979 [D loss: 0.057254, acc.: 99.00%] [G loss: 0.997524]\n",
            "980 [D loss: 0.078421, acc.: 98.50%] [G loss: 0.751906]\n",
            "981 [D loss: 0.061171, acc.: 98.17%] [G loss: 0.623333]\n",
            "982 [D loss: 0.047322, acc.: 99.67%] [G loss: 0.746313]\n",
            "983 [D loss: 0.041861, acc.: 99.33%] [G loss: 0.846838]\n",
            "984 [D loss: 0.040645, acc.: 100.00%] [G loss: 0.799653]\n",
            "985 [D loss: 0.035359, acc.: 99.83%] [G loss: 0.556939]\n",
            "986 [D loss: 0.043853, acc.: 99.50%] [G loss: 0.662689]\n",
            "987 [D loss: 0.071387, acc.: 98.67%] [G loss: 0.957488]\n",
            "988 [D loss: 0.037253, acc.: 99.50%] [G loss: 0.650541]\n",
            "989 [D loss: 0.047422, acc.: 99.67%] [G loss: 0.643259]\n",
            "990 [D loss: 0.094919, acc.: 98.50%] [G loss: 0.690162]\n",
            "991 [D loss: 0.039235, acc.: 99.67%] [G loss: 0.642837]\n",
            "992 [D loss: 0.038162, acc.: 99.83%] [G loss: 0.684687]\n",
            "993 [D loss: 0.031838, acc.: 100.00%] [G loss: 0.654141]\n",
            "994 [D loss: 0.042156, acc.: 100.00%] [G loss: 0.798389]\n",
            "995 [D loss: 0.057714, acc.: 99.50%] [G loss: 0.912056]\n",
            "996 [D loss: 0.042764, acc.: 99.50%] [G loss: 0.559984]\n",
            "997 [D loss: 0.078845, acc.: 98.33%] [G loss: 1.085880]\n",
            "998 [D loss: 0.153541, acc.: 96.00%] [G loss: 1.166528]\n",
            "999 [D loss: 0.049343, acc.: 98.83%] [G loss: 0.710009]\n",
            "1000 [D loss: 0.026543, acc.: 100.00%] [G loss: 0.830478]\n",
            "1001 [D loss: 0.042378, acc.: 99.33%] [G loss: 1.049546]\n",
            "1002 [D loss: 0.028880, acc.: 100.00%] [G loss: 0.699509]\n",
            "1003 [D loss: 0.044903, acc.: 99.33%] [G loss: 0.745498]\n",
            "1004 [D loss: 0.052408, acc.: 99.67%] [G loss: 0.652468]\n",
            "1005 [D loss: 0.063741, acc.: 99.00%] [G loss: 0.679432]\n",
            "1006 [D loss: 0.035995, acc.: 99.83%] [G loss: 0.689731]\n",
            "1007 [D loss: 0.042635, acc.: 99.67%] [G loss: 0.689913]\n",
            "1008 [D loss: 0.063596, acc.: 99.33%] [G loss: 0.737007]\n",
            "1009 [D loss: 0.042470, acc.: 99.83%] [G loss: 0.388232]\n",
            "1010 [D loss: 0.047264, acc.: 99.67%] [G loss: 0.597890]\n",
            "1011 [D loss: 0.032377, acc.: 99.83%] [G loss: 0.605781]\n",
            "1012 [D loss: 0.036655, acc.: 99.67%] [G loss: 0.782604]\n",
            "1013 [D loss: 0.036893, acc.: 99.67%] [G loss: 0.801256]\n",
            "1014 [D loss: 0.045388, acc.: 99.50%] [G loss: 0.995328]\n",
            "1015 [D loss: 0.054309, acc.: 99.33%] [G loss: 0.702180]\n",
            "1016 [D loss: 0.086075, acc.: 98.33%] [G loss: 0.894820]\n",
            "1017 [D loss: 0.027214, acc.: 100.00%] [G loss: 0.813410]\n",
            "1018 [D loss: 0.047866, acc.: 99.33%] [G loss: 0.423321]\n",
            "1019 [D loss: 0.033178, acc.: 99.83%] [G loss: 0.613126]\n",
            "1020 [D loss: 0.041280, acc.: 99.17%] [G loss: 0.846788]\n",
            "1021 [D loss: 0.037698, acc.: 99.83%] [G loss: 0.915595]\n",
            "1022 [D loss: 0.037354, acc.: 99.50%] [G loss: 1.092098]\n",
            "1023 [D loss: 0.033409, acc.: 99.83%] [G loss: 0.583219]\n",
            "1024 [D loss: 0.049760, acc.: 99.17%] [G loss: 0.919534]\n",
            "1025 [D loss: 0.094434, acc.: 98.17%] [G loss: 0.924971]\n",
            "1026 [D loss: 0.354280, acc.: 84.50%] [G loss: 1.146892]\n",
            "1027 [D loss: 0.614183, acc.: 78.83%] [G loss: 1.020723]\n",
            "1028 [D loss: 0.514014, acc.: 78.17%] [G loss: 2.038967]\n",
            "1029 [D loss: 3.036125, acc.: 56.00%] [G loss: 1.097414]\n",
            "1030 [D loss: 0.032181, acc.: 99.00%] [G loss: 1.121941]\n",
            "1031 [D loss: 0.221603, acc.: 92.83%] [G loss: 0.811541]\n",
            "1032 [D loss: 0.117676, acc.: 95.50%] [G loss: 0.693078]\n",
            "1033 [D loss: 0.048665, acc.: 99.50%] [G loss: 0.631593]\n",
            "1034 [D loss: 0.259296, acc.: 88.50%] [G loss: 0.574648]\n",
            "1035 [D loss: 0.085393, acc.: 97.17%] [G loss: 0.700856]\n",
            "1036 [D loss: 0.105107, acc.: 96.67%] [G loss: 0.517908]\n",
            "1037 [D loss: 0.112154, acc.: 96.00%] [G loss: 0.607107]\n",
            "1038 [D loss: 0.216172, acc.: 90.67%] [G loss: 0.702115]\n",
            "1039 [D loss: 0.388019, acc.: 81.50%] [G loss: 0.985025]\n",
            "1040 [D loss: 0.764022, acc.: 63.67%] [G loss: 0.986715]\n",
            "1041 [D loss: 0.120086, acc.: 95.00%] [G loss: 0.668667]\n",
            "1042 [D loss: 0.176831, acc.: 94.00%] [G loss: 0.477518]\n",
            "1043 [D loss: 0.122020, acc.: 96.50%] [G loss: 0.567990]\n",
            "1044 [D loss: 0.227896, acc.: 92.00%] [G loss: 0.819014]\n",
            "1045 [D loss: 0.479759, acc.: 75.83%] [G loss: 0.548143]\n",
            "1046 [D loss: 0.334985, acc.: 83.50%] [G loss: 0.484402]\n",
            "1047 [D loss: 0.164704, acc.: 94.83%] [G loss: 0.558421]\n",
            "1048 [D loss: 0.083590, acc.: 97.83%] [G loss: 0.551628]\n",
            "1049 [D loss: 0.096963, acc.: 97.83%] [G loss: 0.376467]\n",
            "1050 [D loss: 0.091344, acc.: 98.33%] [G loss: 0.518845]\n",
            "1051 [D loss: 0.138386, acc.: 96.00%] [G loss: 0.545276]\n",
            "1052 [D loss: 0.090438, acc.: 97.83%] [G loss: 0.793355]\n",
            "1053 [D loss: 0.118017, acc.: 97.17%] [G loss: 0.504781]\n",
            "1054 [D loss: 0.097599, acc.: 96.83%] [G loss: 0.692919]\n",
            "1055 [D loss: 0.108948, acc.: 97.17%] [G loss: 0.734515]\n",
            "1056 [D loss: 0.092720, acc.: 98.67%] [G loss: 0.750627]\n",
            "1057 [D loss: 0.148757, acc.: 95.00%] [G loss: 1.099291]\n",
            "1058 [D loss: 0.108604, acc.: 98.00%] [G loss: 0.712195]\n",
            "1059 [D loss: 0.084793, acc.: 98.00%] [G loss: 0.785250]\n",
            "1060 [D loss: 0.184631, acc.: 94.00%] [G loss: 0.672433]\n",
            "1061 [D loss: 0.107697, acc.: 96.50%] [G loss: 0.574459]\n",
            "1062 [D loss: 0.047526, acc.: 99.67%] [G loss: 0.772613]\n",
            "1063 [D loss: 0.051965, acc.: 99.67%] [G loss: 0.680997]\n",
            "1064 [D loss: 0.060216, acc.: 99.67%] [G loss: 0.545845]\n",
            "1065 [D loss: 0.070940, acc.: 99.17%] [G loss: 0.629821]\n",
            "1066 [D loss: 0.074164, acc.: 99.33%] [G loss: 0.820928]\n",
            "1067 [D loss: 0.168784, acc.: 94.67%] [G loss: 0.730812]\n",
            "1068 [D loss: 0.059611, acc.: 99.33%] [G loss: 0.675521]\n",
            "1069 [D loss: 0.046228, acc.: 99.67%] [G loss: 0.688066]\n",
            "1070 [D loss: 0.025673, acc.: 100.00%] [G loss: 0.494050]\n",
            "1071 [D loss: 0.038558, acc.: 99.83%] [G loss: 0.599253]\n",
            "1072 [D loss: 0.043245, acc.: 99.00%] [G loss: 0.517573]\n",
            "1073 [D loss: 0.062734, acc.: 99.33%] [G loss: 0.547015]\n",
            "1074 [D loss: 0.047780, acc.: 98.83%] [G loss: 0.688010]\n",
            "1075 [D loss: 0.056452, acc.: 99.17%] [G loss: 0.579830]\n",
            "1076 [D loss: 0.048126, acc.: 99.67%] [G loss: 0.759814]\n",
            "1077 [D loss: 0.053867, acc.: 99.83%] [G loss: 0.682557]\n",
            "1078 [D loss: 0.042890, acc.: 99.17%] [G loss: 0.602706]\n",
            "1079 [D loss: 0.047237, acc.: 99.67%] [G loss: 0.580121]\n",
            "1080 [D loss: 0.053654, acc.: 98.67%] [G loss: 0.579241]\n",
            "1081 [D loss: 0.061146, acc.: 99.83%] [G loss: 0.411913]\n",
            "1082 [D loss: 0.029862, acc.: 100.00%] [G loss: 0.565331]\n",
            "1083 [D loss: 0.035628, acc.: 99.50%] [G loss: 0.970569]\n",
            "1084 [D loss: 0.043894, acc.: 99.83%] [G loss: 0.422936]\n",
            "1085 [D loss: 0.041129, acc.: 99.50%] [G loss: 0.473761]\n",
            "1086 [D loss: 0.048064, acc.: 99.67%] [G loss: 0.593186]\n",
            "1087 [D loss: 0.030474, acc.: 99.67%] [G loss: 0.843489]\n",
            "1088 [D loss: 0.041167, acc.: 99.50%] [G loss: 0.490515]\n",
            "1089 [D loss: 0.043206, acc.: 99.33%] [G loss: 0.631384]\n",
            "1090 [D loss: 0.047331, acc.: 99.50%] [G loss: 0.520253]\n",
            "1091 [D loss: 0.030753, acc.: 100.00%] [G loss: 0.808001]\n",
            "1092 [D loss: 0.032222, acc.: 100.00%] [G loss: 0.607163]\n",
            "1093 [D loss: 0.028764, acc.: 100.00%] [G loss: 0.621686]\n",
            "1094 [D loss: 0.048882, acc.: 99.50%] [G loss: 0.884319]\n",
            "1095 [D loss: 0.110399, acc.: 97.33%] [G loss: 0.734305]\n",
            "1096 [D loss: 0.032117, acc.: 99.83%] [G loss: 0.671831]\n",
            "1097 [D loss: 0.028458, acc.: 99.83%] [G loss: 0.613613]\n",
            "1098 [D loss: 0.028038, acc.: 99.83%] [G loss: 0.737461]\n",
            "1099 [D loss: 0.036635, acc.: 99.33%] [G loss: 0.738748]\n",
            "1100 [D loss: 0.051619, acc.: 99.33%] [G loss: 0.473801]\n",
            "1101 [D loss: 0.046213, acc.: 99.83%] [G loss: 0.588239]\n",
            "1102 [D loss: 0.033565, acc.: 99.67%] [G loss: 0.753463]\n",
            "1103 [D loss: 0.046542, acc.: 99.50%] [G loss: 0.483170]\n",
            "1104 [D loss: 0.056488, acc.: 99.33%] [G loss: 0.572677]\n",
            "1105 [D loss: 0.026192, acc.: 100.00%] [G loss: 0.769770]\n",
            "1106 [D loss: 0.035433, acc.: 99.67%] [G loss: 0.770644]\n",
            "1107 [D loss: 0.035615, acc.: 99.83%] [G loss: 0.826174]\n",
            "1108 [D loss: 0.046125, acc.: 99.17%] [G loss: 0.707153]\n",
            "1109 [D loss: 0.022743, acc.: 100.00%] [G loss: 0.550448]\n",
            "1110 [D loss: 0.044028, acc.: 99.00%] [G loss: 0.545598]\n",
            "1111 [D loss: 0.040625, acc.: 99.50%] [G loss: 0.618023]\n",
            "1112 [D loss: 0.038216, acc.: 99.67%] [G loss: 0.471509]\n",
            "1113 [D loss: 0.032201, acc.: 99.83%] [G loss: 0.638403]\n",
            "1114 [D loss: 0.029635, acc.: 99.67%] [G loss: 0.750335]\n",
            "1115 [D loss: 0.049787, acc.: 99.17%] [G loss: 0.428373]\n",
            "1116 [D loss: 0.048160, acc.: 99.67%] [G loss: 0.441081]\n",
            "1117 [D loss: 0.034911, acc.: 99.50%] [G loss: 0.647789]\n",
            "1118 [D loss: 0.046880, acc.: 99.50%] [G loss: 0.703014]\n",
            "1119 [D loss: 0.030722, acc.: 99.17%] [G loss: 0.543089]\n",
            "1120 [D loss: 0.025466, acc.: 99.83%] [G loss: 0.695863]\n",
            "1121 [D loss: 0.042104, acc.: 99.67%] [G loss: 0.936085]\n",
            "1122 [D loss: 0.034235, acc.: 99.67%] [G loss: 0.637629]\n",
            "1123 [D loss: 0.035649, acc.: 99.67%] [G loss: 0.850834]\n",
            "1124 [D loss: 0.033333, acc.: 99.67%] [G loss: 0.514569]\n",
            "1125 [D loss: 0.040205, acc.: 99.33%] [G loss: 0.538966]\n",
            "1126 [D loss: 0.054044, acc.: 99.33%] [G loss: 0.602413]\n",
            "1127 [D loss: 0.049745, acc.: 99.50%] [G loss: 0.549238]\n",
            "1128 [D loss: 0.039007, acc.: 99.33%] [G loss: 0.774131]\n",
            "1129 [D loss: 0.053382, acc.: 99.00%] [G loss: 0.514874]\n",
            "1130 [D loss: 0.040691, acc.: 99.67%] [G loss: 0.398411]\n",
            "1131 [D loss: 0.027933, acc.: 99.83%] [G loss: 0.588959]\n",
            "1132 [D loss: 0.033873, acc.: 99.83%] [G loss: 0.752717]\n",
            "1133 [D loss: 0.028183, acc.: 99.83%] [G loss: 0.628868]\n",
            "1134 [D loss: 0.051786, acc.: 99.33%] [G loss: 0.777763]\n",
            "1135 [D loss: 0.029469, acc.: 100.00%] [G loss: 0.672973]\n",
            "1136 [D loss: 0.031566, acc.: 99.67%] [G loss: 0.705299]\n",
            "1137 [D loss: 0.038077, acc.: 99.67%] [G loss: 0.567304]\n",
            "1138 [D loss: 0.037409, acc.: 99.33%] [G loss: 0.543302]\n",
            "1139 [D loss: 0.043316, acc.: 99.50%] [G loss: 0.568994]\n",
            "1140 [D loss: 0.037834, acc.: 99.33%] [G loss: 0.619653]\n",
            "1141 [D loss: 0.034626, acc.: 99.67%] [G loss: 0.709678]\n",
            "1142 [D loss: 0.037163, acc.: 99.50%] [G loss: 0.843608]\n",
            "1143 [D loss: 0.057657, acc.: 99.17%] [G loss: 0.810244]\n",
            "1144 [D loss: 0.056280, acc.: 99.00%] [G loss: 0.815040]\n",
            "1145 [D loss: 0.037637, acc.: 99.67%] [G loss: 0.478063]\n",
            "1146 [D loss: 0.035652, acc.: 99.67%] [G loss: 0.506588]\n",
            "1147 [D loss: 0.045734, acc.: 99.33%] [G loss: 0.801360]\n",
            "1148 [D loss: 0.036638, acc.: 99.50%] [G loss: 0.555597]\n",
            "1149 [D loss: 0.035173, acc.: 99.83%] [G loss: 0.781871]\n",
            "1150 [D loss: 0.049098, acc.: 99.00%] [G loss: 0.557757]\n",
            "1151 [D loss: 0.051829, acc.: 99.00%] [G loss: 0.778640]\n",
            "1152 [D loss: 0.023872, acc.: 100.00%] [G loss: 0.724988]\n",
            "1153 [D loss: 0.035431, acc.: 99.67%] [G loss: 0.607277]\n",
            "1154 [D loss: 0.042011, acc.: 99.33%] [G loss: 0.927528]\n",
            "1155 [D loss: 0.060132, acc.: 99.50%] [G loss: 0.714508]\n",
            "1156 [D loss: 0.036995, acc.: 99.17%] [G loss: 0.696544]\n",
            "1157 [D loss: 0.046329, acc.: 99.17%] [G loss: 0.558266]\n",
            "1158 [D loss: 0.031797, acc.: 99.67%] [G loss: 0.663803]\n",
            "1159 [D loss: 0.025500, acc.: 99.83%] [G loss: 0.911720]\n",
            "1160 [D loss: 0.029454, acc.: 99.50%] [G loss: 0.490978]\n",
            "1161 [D loss: 0.021310, acc.: 100.00%] [G loss: 0.573729]\n",
            "1162 [D loss: 0.025532, acc.: 99.83%] [G loss: 0.629747]\n",
            "1163 [D loss: 0.018058, acc.: 99.83%] [G loss: 0.793369]\n",
            "1164 [D loss: 0.043382, acc.: 99.17%] [G loss: 0.521042]\n",
            "1165 [D loss: 0.025020, acc.: 100.00%] [G loss: 1.132239]\n",
            "1166 [D loss: 0.029297, acc.: 99.83%] [G loss: 0.783473]\n",
            "1167 [D loss: 0.039129, acc.: 99.33%] [G loss: 0.420714]\n",
            "1168 [D loss: 0.038967, acc.: 99.33%] [G loss: 0.454048]\n",
            "1169 [D loss: 0.041016, acc.: 99.67%] [G loss: 0.603207]\n",
            "1170 [D loss: 0.029862, acc.: 99.67%] [G loss: 0.870347]\n",
            "1171 [D loss: 0.031416, acc.: 99.50%] [G loss: 0.587874]\n",
            "1172 [D loss: 0.029925, acc.: 99.50%] [G loss: 0.759798]\n",
            "1173 [D loss: 0.030084, acc.: 99.67%] [G loss: 0.575709]\n",
            "1174 [D loss: 0.018140, acc.: 100.00%] [G loss: 0.689648]\n",
            "1175 [D loss: 0.041830, acc.: 99.17%] [G loss: 0.565749]\n",
            "1176 [D loss: 0.047983, acc.: 99.17%] [G loss: 0.728487]\n",
            "1177 [D loss: 0.035188, acc.: 99.50%] [G loss: 0.747283]\n",
            "1178 [D loss: 0.018324, acc.: 99.83%] [G loss: 0.935190]\n",
            "1179 [D loss: 0.022234, acc.: 100.00%] [G loss: 0.666171]\n",
            "1180 [D loss: 0.038925, acc.: 99.33%] [G loss: 0.711369]\n",
            "1181 [D loss: 0.044197, acc.: 99.33%] [G loss: 0.858960]\n",
            "1182 [D loss: 0.044118, acc.: 99.50%] [G loss: 0.427501]\n",
            "1183 [D loss: 0.029749, acc.: 99.83%] [G loss: 0.960414]\n",
            "1184 [D loss: 0.027940, acc.: 99.83%] [G loss: 0.795231]\n",
            "1185 [D loss: 0.027082, acc.: 100.00%] [G loss: 0.757821]\n",
            "1186 [D loss: 0.029927, acc.: 100.00%] [G loss: 0.792214]\n",
            "1187 [D loss: 0.025609, acc.: 99.83%] [G loss: 0.981683]\n",
            "1188 [D loss: 0.027675, acc.: 99.67%] [G loss: 0.530735]\n",
            "1189 [D loss: 0.020947, acc.: 100.00%] [G loss: 0.664011]\n",
            "1190 [D loss: 0.029115, acc.: 99.67%] [G loss: 1.006646]\n",
            "1191 [D loss: 0.030577, acc.: 99.50%] [G loss: 0.754423]\n",
            "1192 [D loss: 0.048049, acc.: 99.50%] [G loss: 0.694037]\n",
            "1193 [D loss: 0.030527, acc.: 99.50%] [G loss: 0.641049]\n",
            "1194 [D loss: 0.033670, acc.: 99.50%] [G loss: 0.775431]\n",
            "1195 [D loss: 0.026259, acc.: 99.50%] [G loss: 0.494151]\n",
            "1196 [D loss: 0.026611, acc.: 99.67%] [G loss: 0.836592]\n",
            "1197 [D loss: 0.029054, acc.: 99.50%] [G loss: 0.873932]\n",
            "1198 [D loss: 0.029304, acc.: 99.67%] [G loss: 0.631814]\n",
            "1199 [D loss: 0.033986, acc.: 99.67%] [G loss: 0.731194]\n",
            "1200 [D loss: 0.044536, acc.: 99.50%] [G loss: 0.651452]\n",
            "1201 [D loss: 0.017785, acc.: 99.83%] [G loss: 0.830245]\n",
            "1202 [D loss: 0.026540, acc.: 99.50%] [G loss: 0.587644]\n",
            "1203 [D loss: 0.025222, acc.: 99.33%] [G loss: 0.711560]\n",
            "1204 [D loss: 0.043052, acc.: 99.33%] [G loss: 0.828137]\n",
            "1205 [D loss: 0.032483, acc.: 99.67%] [G loss: 0.788962]\n",
            "1206 [D loss: 0.036203, acc.: 99.33%] [G loss: 0.649076]\n",
            "1207 [D loss: 0.032224, acc.: 99.50%] [G loss: 0.750781]\n",
            "1208 [D loss: 0.044752, acc.: 98.50%] [G loss: 0.973599]\n",
            "1209 [D loss: 0.123576, acc.: 96.50%] [G loss: 0.916779]\n",
            "1210 [D loss: 0.016635, acc.: 99.83%] [G loss: 1.152276]\n",
            "1211 [D loss: 0.021173, acc.: 99.67%] [G loss: 0.597569]\n",
            "1212 [D loss: 0.020364, acc.: 99.83%] [G loss: 2.068103]\n",
            "1213 [D loss: 0.019894, acc.: 100.00%] [G loss: 24.321539]\n",
            "1214 [D loss: 0.017515, acc.: 99.67%] [G loss: 12.851692]\n",
            "1215 [D loss: 0.016419, acc.: 99.83%] [G loss: 5.302643]\n",
            "1216 [D loss: 0.023814, acc.: 99.50%] [G loss: 1.588714]\n",
            "1217 [D loss: 0.023537, acc.: 99.67%] [G loss: 1.403443]\n",
            "1218 [D loss: 0.038232, acc.: 99.17%] [G loss: 0.802169]\n",
            "1219 [D loss: 0.025577, acc.: 99.33%] [G loss: 0.995142]\n",
            "1220 [D loss: 0.032648, acc.: 99.33%] [G loss: 0.823906]\n",
            "1221 [D loss: 0.024286, acc.: 99.83%] [G loss: 0.793026]\n",
            "1222 [D loss: 0.033629, acc.: 99.33%] [G loss: 1.154848]\n",
            "1223 [D loss: 0.027384, acc.: 99.50%] [G loss: 0.976957]\n",
            "1224 [D loss: 0.029730, acc.: 99.50%] [G loss: 1.015787]\n",
            "1225 [D loss: 0.025242, acc.: 99.83%] [G loss: 0.576791]\n",
            "1226 [D loss: 0.022431, acc.: 99.33%] [G loss: 1.277005]\n",
            "1227 [D loss: 0.023439, acc.: 99.83%] [G loss: 0.740552]\n",
            "1228 [D loss: 0.021592, acc.: 100.00%] [G loss: 1.200121]\n",
            "1229 [D loss: 0.015593, acc.: 100.00%] [G loss: 0.752336]\n",
            "1230 [D loss: 0.019853, acc.: 99.67%] [G loss: 1.012061]\n",
            "1231 [D loss: 0.038868, acc.: 99.17%] [G loss: 1.046328]\n",
            "1232 [D loss: 0.027180, acc.: 99.67%] [G loss: 0.920082]\n",
            "1233 [D loss: 0.022804, acc.: 99.67%] [G loss: 0.704202]\n",
            "1234 [D loss: 0.020279, acc.: 99.83%] [G loss: 0.735547]\n",
            "1235 [D loss: 0.019155, acc.: 99.83%] [G loss: 0.956058]\n",
            "1236 [D loss: 0.027889, acc.: 99.67%] [G loss: 0.690994]\n",
            "1237 [D loss: 0.017219, acc.: 99.83%] [G loss: 1.042049]\n",
            "1238 [D loss: 0.023819, acc.: 99.67%] [G loss: 0.746464]\n",
            "1239 [D loss: 0.021867, acc.: 99.67%] [G loss: 1.158249]\n",
            "1240 [D loss: 0.023381, acc.: 99.83%] [G loss: 0.704563]\n",
            "1241 [D loss: 0.021440, acc.: 99.83%] [G loss: 0.552348]\n",
            "1242 [D loss: 0.018471, acc.: 99.83%] [G loss: 0.473714]\n",
            "1243 [D loss: 0.017144, acc.: 99.83%] [G loss: 0.832787]\n",
            "1244 [D loss: 0.013062, acc.: 100.00%] [G loss: 0.659430]\n",
            "1245 [D loss: 0.010890, acc.: 100.00%] [G loss: 0.859201]\n",
            "1246 [D loss: 0.022560, acc.: 99.83%] [G loss: 0.627001]\n",
            "1247 [D loss: 0.022491, acc.: 99.83%] [G loss: 1.025139]\n",
            "1248 [D loss: 0.016924, acc.: 99.83%] [G loss: 0.896815]\n",
            "1249 [D loss: 0.013702, acc.: 100.00%] [G loss: 0.819681]\n",
            "1250 [D loss: 0.018970, acc.: 99.83%] [G loss: 0.811134]\n",
            "1251 [D loss: 0.019585, acc.: 99.67%] [G loss: 0.878596]\n",
            "1252 [D loss: 0.027578, acc.: 99.83%] [G loss: 0.731869]\n",
            "1253 [D loss: 0.023223, acc.: 99.83%] [G loss: 0.916233]\n",
            "1254 [D loss: 0.018905, acc.: 99.67%] [G loss: 0.659454]\n",
            "1255 [D loss: 0.020663, acc.: 99.83%] [G loss: 0.975503]\n",
            "1256 [D loss: 0.022231, acc.: 99.83%] [G loss: 0.902539]\n",
            "1257 [D loss: 0.022232, acc.: 99.67%] [G loss: 1.299135]\n",
            "1258 [D loss: 0.026059, acc.: 99.83%] [G loss: 0.631289]\n",
            "1259 [D loss: 0.021410, acc.: 99.33%] [G loss: 0.856351]\n",
            "1260 [D loss: 0.024318, acc.: 99.67%] [G loss: 0.687451]\n",
            "1261 [D loss: 0.017566, acc.: 99.83%] [G loss: 0.884113]\n",
            "1262 [D loss: 0.012049, acc.: 100.00%] [G loss: 0.979644]\n",
            "1263 [D loss: 0.021440, acc.: 99.67%] [G loss: 0.723419]\n",
            "1264 [D loss: 0.016360, acc.: 99.67%] [G loss: 0.778043]\n",
            "1265 [D loss: 0.011980, acc.: 99.83%] [G loss: 0.710556]\n",
            "1266 [D loss: 0.013863, acc.: 99.83%] [G loss: 0.953789]\n",
            "1267 [D loss: 0.021095, acc.: 99.83%] [G loss: 0.756658]\n",
            "1268 [D loss: 0.016868, acc.: 99.83%] [G loss: 0.739712]\n",
            "1269 [D loss: 0.026432, acc.: 99.67%] [G loss: 0.990239]\n",
            "1270 [D loss: 0.012329, acc.: 100.00%] [G loss: 0.455434]\n",
            "1271 [D loss: 0.014359, acc.: 100.00%] [G loss: 0.552016]\n",
            "1272 [D loss: 0.015531, acc.: 99.83%] [G loss: 0.813568]\n",
            "1273 [D loss: 0.010957, acc.: 100.00%] [G loss: 0.838578]\n",
            "1274 [D loss: 0.014203, acc.: 99.67%] [G loss: 0.916860]\n",
            "1275 [D loss: 0.018588, acc.: 99.67%] [G loss: 0.752769]\n",
            "1276 [D loss: 0.024704, acc.: 99.50%] [G loss: 0.690897]\n",
            "1277 [D loss: 0.015052, acc.: 100.00%] [G loss: 0.683794]\n",
            "1278 [D loss: 0.017814, acc.: 100.00%] [G loss: 0.863117]\n",
            "1279 [D loss: 0.015679, acc.: 99.83%] [G loss: 0.706677]\n",
            "1280 [D loss: 0.013079, acc.: 100.00%] [G loss: 0.880768]\n",
            "1281 [D loss: 0.013818, acc.: 100.00%] [G loss: 0.746325]\n",
            "1282 [D loss: 0.018242, acc.: 99.67%] [G loss: 0.846986]\n",
            "1283 [D loss: 0.021276, acc.: 99.67%] [G loss: 0.890663]\n",
            "1284 [D loss: 0.015496, acc.: 100.00%] [G loss: 0.765237]\n",
            "1285 [D loss: 0.017018, acc.: 100.00%] [G loss: 0.849384]\n",
            "1286 [D loss: 0.014155, acc.: 100.00%] [G loss: 0.510575]\n",
            "1287 [D loss: 0.028408, acc.: 99.50%] [G loss: 0.807749]\n",
            "1288 [D loss: 0.030167, acc.: 99.83%] [G loss: 0.784350]\n",
            "1289 [D loss: 0.013432, acc.: 100.00%] [G loss: 0.818722]\n",
            "1290 [D loss: 0.021956, acc.: 99.67%] [G loss: 0.703168]\n",
            "1291 [D loss: 0.025976, acc.: 99.83%] [G loss: 1.044934]\n",
            "1292 [D loss: 0.018895, acc.: 99.83%] [G loss: 0.680518]\n",
            "1293 [D loss: 0.014632, acc.: 100.00%] [G loss: 0.692162]\n",
            "1294 [D loss: 0.015871, acc.: 99.67%] [G loss: 0.641716]\n",
            "1295 [D loss: 0.012222, acc.: 100.00%] [G loss: 0.674053]\n",
            "1296 [D loss: 0.013707, acc.: 99.83%] [G loss: 0.812645]\n",
            "1297 [D loss: 0.012674, acc.: 100.00%] [G loss: 0.793727]\n",
            "1298 [D loss: 0.015890, acc.: 99.83%] [G loss: 0.619643]\n",
            "1299 [D loss: 0.024649, acc.: 99.83%] [G loss: 0.814848]\n",
            "1300 [D loss: 0.017073, acc.: 99.67%] [G loss: 0.734783]\n",
            "1301 [D loss: 0.021971, acc.: 99.83%] [G loss: 0.648795]\n",
            "1302 [D loss: 0.027993, acc.: 99.50%] [G loss: 1.002766]\n",
            "1303 [D loss: 0.023099, acc.: 99.67%] [G loss: 0.821615]\n",
            "1304 [D loss: 0.013762, acc.: 99.83%] [G loss: 0.754019]\n",
            "1305 [D loss: 0.018520, acc.: 99.67%] [G loss: 1.218799]\n",
            "1306 [D loss: 0.012884, acc.: 100.00%] [G loss: 0.848229]\n",
            "1307 [D loss: 0.017797, acc.: 99.83%] [G loss: 0.709018]\n",
            "1308 [D loss: 0.017596, acc.: 99.83%] [G loss: 0.636307]\n",
            "1309 [D loss: 0.014279, acc.: 99.83%] [G loss: 0.894029]\n",
            "1310 [D loss: 0.027261, acc.: 99.67%] [G loss: 0.825046]\n",
            "1311 [D loss: 0.016930, acc.: 100.00%] [G loss: 0.725829]\n",
            "1312 [D loss: 0.026602, acc.: 99.17%] [G loss: 0.842295]\n",
            "1313 [D loss: 0.019120, acc.: 100.00%] [G loss: 0.722262]\n",
            "1314 [D loss: 0.015908, acc.: 99.67%] [G loss: 0.904115]\n",
            "1315 [D loss: 0.026155, acc.: 99.67%] [G loss: 0.766521]\n",
            "1316 [D loss: 0.016289, acc.: 100.00%] [G loss: 0.828743]\n",
            "1317 [D loss: 0.021326, acc.: 99.50%] [G loss: 0.944389]\n",
            "1318 [D loss: 0.012717, acc.: 99.83%] [G loss: 0.644284]\n",
            "1319 [D loss: 0.012326, acc.: 99.83%] [G loss: 0.753158]\n",
            "1320 [D loss: 0.010615, acc.: 100.00%] [G loss: 0.884031]\n",
            "1321 [D loss: 0.012946, acc.: 100.00%] [G loss: 0.799848]\n",
            "1322 [D loss: 0.019424, acc.: 99.83%] [G loss: 0.987314]\n",
            "1323 [D loss: 0.023025, acc.: 99.50%] [G loss: 0.688664]\n",
            "1324 [D loss: 0.017950, acc.: 99.83%] [G loss: 0.870021]\n",
            "1325 [D loss: 0.019678, acc.: 99.50%] [G loss: 0.704278]\n",
            "1326 [D loss: 0.014582, acc.: 99.83%] [G loss: 0.792108]\n",
            "1327 [D loss: 0.010587, acc.: 100.00%] [G loss: 0.716699]\n",
            "1328 [D loss: 0.010875, acc.: 100.00%] [G loss: 0.602824]\n",
            "1329 [D loss: 0.009897, acc.: 100.00%] [G loss: 0.981930]\n",
            "1330 [D loss: 0.009940, acc.: 100.00%] [G loss: 0.613709]\n",
            "1331 [D loss: 0.011617, acc.: 100.00%] [G loss: 0.628453]\n",
            "1332 [D loss: 0.019857, acc.: 99.67%] [G loss: 0.726460]\n",
            "1333 [D loss: 0.017943, acc.: 99.67%] [G loss: 0.794638]\n",
            "1334 [D loss: 0.014336, acc.: 100.00%] [G loss: 0.743520]\n",
            "1335 [D loss: 0.009923, acc.: 100.00%] [G loss: 0.948936]\n",
            "1336 [D loss: 0.006375, acc.: 100.00%] [G loss: 0.983239]\n",
            "1337 [D loss: 0.009222, acc.: 100.00%] [G loss: 0.635545]\n",
            "1338 [D loss: 0.010965, acc.: 100.00%] [G loss: 0.969752]\n",
            "1339 [D loss: 0.017222, acc.: 99.50%] [G loss: 0.925314]\n",
            "1340 [D loss: 0.011411, acc.: 100.00%] [G loss: 0.664335]\n",
            "1341 [D loss: 0.010685, acc.: 100.00%] [G loss: 0.898013]\n",
            "1342 [D loss: 0.008843, acc.: 100.00%] [G loss: 0.901536]\n",
            "1343 [D loss: 0.008848, acc.: 100.00%] [G loss: 0.658316]\n",
            "1344 [D loss: 0.016233, acc.: 99.50%] [G loss: 0.987851]\n",
            "1345 [D loss: 0.029608, acc.: 99.33%] [G loss: 0.684234]\n",
            "1346 [D loss: 0.010474, acc.: 99.83%] [G loss: 0.798131]\n",
            "1347 [D loss: 0.011004, acc.: 99.83%] [G loss: 0.834445]\n",
            "1348 [D loss: 0.011673, acc.: 99.83%] [G loss: 1.165779]\n",
            "1349 [D loss: 0.011992, acc.: 100.00%] [G loss: 0.798998]\n",
            "1350 [D loss: 0.016176, acc.: 99.67%] [G loss: 1.442648]\n",
            "1351 [D loss: 0.011652, acc.: 100.00%] [G loss: 0.729205]\n",
            "1352 [D loss: 0.018324, acc.: 99.50%] [G loss: 0.843135]\n",
            "1353 [D loss: 0.011698, acc.: 100.00%] [G loss: 1.062443]\n",
            "1354 [D loss: 0.021574, acc.: 99.67%] [G loss: 0.796373]\n",
            "1355 [D loss: 0.012957, acc.: 100.00%] [G loss: 0.699292]\n",
            "1356 [D loss: 0.012102, acc.: 100.00%] [G loss: 1.097704]\n",
            "1357 [D loss: 0.014943, acc.: 99.67%] [G loss: 0.990653]\n",
            "1358 [D loss: 0.010069, acc.: 100.00%] [G loss: 0.905375]\n",
            "1359 [D loss: 0.010157, acc.: 100.00%] [G loss: 0.658945]\n",
            "1360 [D loss: 0.008419, acc.: 100.00%] [G loss: 0.553436]\n",
            "1361 [D loss: 0.010824, acc.: 100.00%] [G loss: 0.761079]\n",
            "1362 [D loss: 0.012368, acc.: 99.83%] [G loss: 0.664928]\n",
            "1363 [D loss: 0.012804, acc.: 100.00%] [G loss: 1.015218]\n",
            "1364 [D loss: 0.014174, acc.: 99.83%] [G loss: 0.818878]\n",
            "1365 [D loss: 0.011719, acc.: 100.00%] [G loss: 0.979989]\n",
            "1366 [D loss: 0.008813, acc.: 99.83%] [G loss: 0.824921]\n",
            "1367 [D loss: 0.029046, acc.: 99.50%] [G loss: 0.928289]\n",
            "1368 [D loss: 0.038845, acc.: 99.17%] [G loss: 0.870648]\n",
            "1369 [D loss: 0.007277, acc.: 100.00%] [G loss: 1.006307]\n",
            "1370 [D loss: 0.013126, acc.: 100.00%] [G loss: 0.916024]\n",
            "1371 [D loss: 0.012676, acc.: 99.83%] [G loss: 0.643326]\n",
            "1372 [D loss: 0.006753, acc.: 100.00%] [G loss: 0.834518]\n",
            "1373 [D loss: 0.014004, acc.: 99.67%] [G loss: 0.924382]\n",
            "1374 [D loss: 0.012656, acc.: 99.83%] [G loss: 0.871703]\n",
            "1375 [D loss: 0.018661, acc.: 99.50%] [G loss: 0.645042]\n",
            "1376 [D loss: 0.014427, acc.: 100.00%] [G loss: 0.908061]\n",
            "1377 [D loss: 0.011752, acc.: 100.00%] [G loss: 1.033162]\n",
            "1378 [D loss: 0.007959, acc.: 100.00%] [G loss: 0.546633]\n",
            "1379 [D loss: 0.013229, acc.: 100.00%] [G loss: 0.808878]\n",
            "1380 [D loss: 0.011718, acc.: 100.00%] [G loss: 0.952281]\n",
            "1381 [D loss: 0.013219, acc.: 99.83%] [G loss: 1.234577]\n",
            "1382 [D loss: 0.014785, acc.: 99.83%] [G loss: 1.145426]\n",
            "1383 [D loss: 0.017350, acc.: 99.83%] [G loss: 0.649068]\n",
            "1384 [D loss: 0.016086, acc.: 99.83%] [G loss: 0.887524]\n",
            "1385 [D loss: 0.015209, acc.: 99.83%] [G loss: 0.780781]\n",
            "1386 [D loss: 0.012304, acc.: 100.00%] [G loss: 0.505418]\n",
            "1387 [D loss: 0.012078, acc.: 100.00%] [G loss: 1.029608]\n",
            "1388 [D loss: 0.006990, acc.: 100.00%] [G loss: 0.728777]\n",
            "1389 [D loss: 0.012545, acc.: 99.83%] [G loss: 0.693446]\n",
            "1390 [D loss: 0.011794, acc.: 99.83%] [G loss: 1.304129]\n",
            "1391 [D loss: 0.012137, acc.: 100.00%] [G loss: 0.647502]\n",
            "1392 [D loss: 0.011355, acc.: 100.00%] [G loss: 0.826235]\n",
            "1393 [D loss: 0.010515, acc.: 99.83%] [G loss: 0.837694]\n",
            "1394 [D loss: 0.011927, acc.: 100.00%] [G loss: 0.842265]\n",
            "1395 [D loss: 0.016843, acc.: 99.67%] [G loss: 0.835491]\n",
            "1396 [D loss: 0.010955, acc.: 100.00%] [G loss: 0.907965]\n",
            "1397 [D loss: 0.019828, acc.: 100.00%] [G loss: 0.966617]\n",
            "1398 [D loss: 0.008099, acc.: 100.00%] [G loss: 0.671465]\n",
            "1399 [D loss: 0.017808, acc.: 99.67%] [G loss: 0.862779]\n",
            "1400 [D loss: 0.017830, acc.: 99.50%] [G loss: 0.804784]\n",
            "1401 [D loss: 0.009475, acc.: 99.83%] [G loss: 0.742268]\n",
            "1402 [D loss: 0.010296, acc.: 100.00%] [G loss: 1.230044]\n",
            "1403 [D loss: 0.007544, acc.: 100.00%] [G loss: 0.841318]\n",
            "1404 [D loss: 0.009888, acc.: 99.83%] [G loss: 1.005083]\n",
            "1405 [D loss: 0.010755, acc.: 99.83%] [G loss: 0.704235]\n",
            "1406 [D loss: 0.010022, acc.: 100.00%] [G loss: 0.682080]\n",
            "1407 [D loss: 0.008162, acc.: 100.00%] [G loss: 0.516840]\n",
            "1408 [D loss: 0.009670, acc.: 99.83%] [G loss: 0.764598]\n",
            "1409 [D loss: 0.006473, acc.: 100.00%] [G loss: 0.997964]\n",
            "1410 [D loss: 0.013155, acc.: 100.00%] [G loss: 1.059652]\n",
            "1411 [D loss: 0.013034, acc.: 99.83%] [G loss: 0.834004]\n",
            "1412 [D loss: 0.008891, acc.: 100.00%] [G loss: 0.937439]\n",
            "1413 [D loss: 0.016248, acc.: 100.00%] [G loss: 0.883356]\n",
            "1414 [D loss: 0.024072, acc.: 99.83%] [G loss: 0.788751]\n",
            "1415 [D loss: 0.007786, acc.: 100.00%] [G loss: 0.849993]\n",
            "1416 [D loss: 0.008219, acc.: 100.00%] [G loss: 0.914738]\n",
            "1417 [D loss: 0.014738, acc.: 99.50%] [G loss: 0.705880]\n",
            "1418 [D loss: 0.009339, acc.: 100.00%] [G loss: 0.872208]\n",
            "1419 [D loss: 0.013052, acc.: 100.00%] [G loss: 0.880918]\n",
            "1420 [D loss: 0.008484, acc.: 100.00%] [G loss: 1.159067]\n",
            "1421 [D loss: 0.014780, acc.: 99.50%] [G loss: 1.001894]\n",
            "1422 [D loss: 0.015430, acc.: 99.83%] [G loss: 0.631796]\n",
            "1423 [D loss: 0.030191, acc.: 99.50%] [G loss: 0.908496]\n",
            "1424 [D loss: 0.010998, acc.: 99.83%] [G loss: 0.954274]\n",
            "1425 [D loss: 0.007845, acc.: 100.00%] [G loss: 0.934147]\n",
            "1426 [D loss: 0.010003, acc.: 100.00%] [G loss: 0.778457]\n",
            "1427 [D loss: 0.007765, acc.: 100.00%] [G loss: 0.751742]\n",
            "1428 [D loss: 0.011597, acc.: 99.83%] [G loss: 1.080089]\n",
            "1429 [D loss: 0.009322, acc.: 99.83%] [G loss: 1.110605]\n",
            "1430 [D loss: 0.012688, acc.: 99.83%] [G loss: 0.642600]\n",
            "1431 [D loss: 0.010977, acc.: 99.83%] [G loss: 0.637783]\n",
            "1432 [D loss: 0.017641, acc.: 100.00%] [G loss: 0.682578]\n",
            "1433 [D loss: 0.007844, acc.: 100.00%] [G loss: 0.983531]\n",
            "1434 [D loss: 0.011761, acc.: 100.00%] [G loss: 0.727168]\n",
            "1435 [D loss: 0.012891, acc.: 99.83%] [G loss: 0.780455]\n",
            "1436 [D loss: 0.010845, acc.: 100.00%] [G loss: 0.625735]\n",
            "1437 [D loss: 0.015898, acc.: 99.67%] [G loss: 1.055846]\n",
            "1438 [D loss: 0.010042, acc.: 100.00%] [G loss: 1.048324]\n",
            "1439 [D loss: 0.012614, acc.: 100.00%] [G loss: 0.979642]\n",
            "1440 [D loss: 0.008405, acc.: 100.00%] [G loss: 0.659202]\n",
            "1441 [D loss: 0.011136, acc.: 99.83%] [G loss: 0.707485]\n",
            "1442 [D loss: 0.014045, acc.: 99.83%] [G loss: 0.580846]\n",
            "1443 [D loss: 0.010123, acc.: 99.83%] [G loss: 0.887779]\n",
            "1444 [D loss: 0.013648, acc.: 99.83%] [G loss: 0.900632]\n",
            "1445 [D loss: 0.008025, acc.: 100.00%] [G loss: 0.793565]\n",
            "1446 [D loss: 0.011645, acc.: 99.83%] [G loss: 0.832388]\n",
            "1447 [D loss: 0.011063, acc.: 99.83%] [G loss: 0.703463]\n",
            "1448 [D loss: 0.032803, acc.: 99.33%] [G loss: 1.095158]\n",
            "1449 [D loss: 0.011200, acc.: 99.83%] [G loss: 0.789941]\n",
            "1450 [D loss: 0.010354, acc.: 99.83%] [G loss: 0.865563]\n",
            "1451 [D loss: 0.013410, acc.: 99.67%] [G loss: 0.797956]\n",
            "1452 [D loss: 0.064722, acc.: 97.67%] [G loss: 0.783816]\n",
            "1453 [D loss: 0.019409, acc.: 99.67%] [G loss: 0.783683]\n",
            "1454 [D loss: 0.007373, acc.: 99.83%] [G loss: 0.834485]\n",
            "1455 [D loss: 0.005386, acc.: 100.00%] [G loss: 1.235165]\n",
            "1456 [D loss: 0.005819, acc.: 100.00%] [G loss: 0.778953]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-833f07903546>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0mcgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0mcgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-833f07903546>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, batch_size, sample_interval)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;31m# Train the discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0md_loss_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0md_loss_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgen_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m             \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_loss_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_loss_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   1723\u001b[0m       iterator = data_adapter.single_batch_iterator(self.distribute_strategy, x,\n\u001b[1;32m   1724\u001b[0m                                                     \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1725\u001b[0;31m                                                     class_weight)\n\u001b[0m\u001b[1;32m   1726\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1727\u001b[0m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36msingle_batch_iterator\u001b[0;34m(strategy, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_make_class_weight_map_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m   \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_distribute_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    680\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcomponents\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0melement_spec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    684\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0;31m# Store dataset reference to ensure that dataset is alive when this iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_apply_options\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    397\u001b[0m       dataset = _OptimizeDataset(dataset, graph_rewrites.enabled,\n\u001b[1;32m    398\u001b[0m                                  \u001b[0mgraph_rewrites\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisabled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m                                  graph_rewrites.default, graph_rewrite_configs)\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;31m# (4) Apply stats aggregator options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, optimizations_enabled, optimizations_disabled, optimizations_default, optimization_configs)\u001b[0m\n\u001b[1;32m   4581\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizations_default\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4582\u001b[0m         \u001b[0moptimization_configs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimization_configs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4583\u001b[0;31m         **self._flat_structure)\n\u001b[0m\u001b[1;32m   4584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4585\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_OptimizeDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36moptimize_dataset_v2\u001b[0;34m(input_dataset, optimizations_enabled, optimizations_disabled, optimizations_default, output_types, output_shapes, optimization_configs, name)\u001b[0m\n\u001b[1;32m   4023\u001b[0m         \u001b[0moptimizations_disabled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizations_default\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4024\u001b[0m         \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_shapes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"optimization_configs\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4025\u001b[0;31m         optimization_configs)\n\u001b[0m\u001b[1;32m   4026\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4027\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}