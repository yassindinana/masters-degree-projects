{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Resnet_Folder_Yassin.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbRkOcYm6Ln9",
        "outputId": "a7b78b5b-845d-4a92-c842-553896ae9114"
      },
      "source": [
        "from keras.layers import Input, Lambda, Dense, Flatten\n",
        "from keras.models import Model\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.applications.resnet50 import preprocess_input\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDmL4BKJ6WS8",
        "outputId": "708fb4fd-10ab-4e8c-ccb9-2c12910f04e7"
      },
      "source": [
        "# Path to data\n",
        "data_dir  = '/content/drive/MyDrive/Masters/Semester 2/Data Mining & Visualisation/Assessments/Assessment1/Code/dataset/'\n",
        "\n",
        "# Get the path to the normal and pneumonia sub-directories\n",
        "normal_files_dir = data_dir + 'Normal'        # Locate the Normal Images folder\n",
        "cov19_files_dir = data_dir + 'Covid-19'       # Locate the COVID-19 Image folder\n",
        "\n",
        "train_path = data_dir + 'Train'\n",
        "valid_path = data_dir + 'Test'\n",
        "\n",
        "# Get the list of all the images\n",
        "normal_files = glob(normal_files_dir+'/*.*')      \n",
        "cov19_files = glob(cov19_files_dir+'/*.*')\n",
        "\n",
        "print('Length of covid-19 file is ', len(cov19_files))     # Print the length of the total images\n",
        "print('Length of normal lungs file is ',len(normal_files))    # Print the length of the normal images\n",
        "\n",
        "#  Split the Images data into a 80% (Train) / 20% (Test) process.\n",
        "cov19_train = np.random.choice(cov19_files, size=80, replace=False).tolist() \n",
        "normal_train = np.random.choice(normal_files, size=80, replace=False).tolist() \n",
        "print('Length of covid-19 training set is ',len(cov19_train))\n",
        "print('Length of normal lungs training set is ',len(normal_train))\n",
        "\n",
        "cov19_test = list(set(cov19_files)-set(cov19_train))            # Load only 20 %\n",
        "normal_test = list(set(normal_files)-set(normal_train))         # Load only 20 %\n",
        "\n",
        "print('Length of covid-19 testing set is ',len(cov19_test))\n",
        "print('Length of normal lungs testing set is ',len(normal_test))\n",
        "\n",
        "\n"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of covid-19 file is  100\n",
            "Length of normal lungs file is  100\n",
            "Length of covid-19 training set is  80\n",
            "Length of normal lungs training set is  80\n",
            "Length of covid-19 testing set is  20\n",
            "Length of normal lungs testing set is  20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REC9KyJt6YN5"
      },
      "source": [
        "import shutil, random\n",
        "i = 0\n",
        "\n",
        "for filename in random.sample(normal_files, 80):\n",
        "    i+=1\n",
        "    shutil.copy(filename, train_path+'/Normal'+ str(i))\n",
        "\n",
        "for filename in random.sample(covid_19_files, 80):\n",
        "    i+=1\n",
        "    shutil.copy(filename, train_path+'/Covid-19'+ str(i))"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwWlet5y6bTt",
        "outputId": "638d9f79-2718-4fdd-eec1-f6e2e5308cc8"
      },
      "source": [
        "i = 0\n",
        "# Copy the data from the file for Testing Sets\n",
        "for filename in random.sample(normal_files, 20):\n",
        "    i+=1\n",
        "    shutil.copy(filename, valid_path+'/Normal'+ str(i))\n",
        "\n",
        "for filename in random.sample(covid_19_files, 20):\n",
        "    i+=1\n",
        "    shutil.copy(filename, valid_path+'/covid_19'+ str(i))"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['/content/drive/MyDrive/Masters/Semester 2/Data Mining & Visualisation/Assessments/Assessment1/Code/dataset/Normal/NORMAL2-IM-0890-0001.jpeg']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haNXH8gt6dqj"
      },
      "source": [
        "# re-size all the images to this\n",
        "IMAGE_SIZE = [224, 224]\n",
        "\n",
        "# add preprocessing layer to the front of ResNet50\n",
        "resnet = ResNet50(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)\n",
        "\n",
        "# don't train existing weights - # We freeze the weights of the model by setting trainable as “False”\n",
        "for layer in resnet.layers:\n",
        "  layer.trainable = False\n",
        "    \n",
        "  # useful for getting number of classes\n",
        "folders = glob(train_path+'/*')\n",
        "  \n",
        "# our layers - you can add more if you want\n",
        "x = Flatten()(resnet.output)\n",
        "# x = Dense(1000, activation='relu')(x)\n",
        "prediction = Dense(len(folders), activation='softmax')(x)\n",
        "\n",
        "# create a model object\n",
        "model = Model(inputs=resnet.input, outputs=prediction)\n",
        "\n",
        "# view the structure of the model\n",
        "model.summary()\n",
        "\n",
        "# tell the model what cost and optimization method to use\n",
        "model.compile(\n",
        "  loss='categorical_crossentropy',\n",
        "  optimizer='adam',\n",
        "  metrics=['accuracy']\n",
        ")\n",
        "\n",
        "#  Apply Data Augmentation to Images \n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "\n",
        "\n",
        "training_set = train_datagen.flow_from_directory(directory = train_path,\n",
        "                                                 target_size = (224, 224),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'categorical')\n",
        "\n",
        "test_set = test_datagen.flow_from_directory(directory = valid_path,\n",
        "                                            target_size = (224, 224),\n",
        "                                            batch_size = 32,\n",
        "                                            class_mode = 'categorical')\n",
        "\n",
        "'''r=model.fit_generator(training_set,\n",
        "                         samples_per_epoch = 8000,\n",
        "                         nb_epoch = 5,\n",
        "                         validation_data = test_set,\n",
        "                         nb_val_samples = 2000)'''\n",
        "print(len(training_set))\n",
        "print(len(test_set))\n",
        "\n",
        "\n",
        "# fit the model\n",
        "model_fitting = model.fit_generator(\n",
        "  training_set,\n",
        "  validation_data=test_set,\n",
        "  epochs=100,\n",
        "  steps_per_epoch=len(training_set),\n",
        "  validation_steps=len(test_set)\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JnKY95P6f1N"
      },
      "source": [
        "plt.plot(model_fitting.history['loss'], label='train loss')\n",
        "plt.plot(model_fitting.history['val_loss'], label='val loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig('LossVal_loss')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9_r8sg_6gOa"
      },
      "source": [
        "plt.plot(model_fitting.history['accuracy'], label='train acc')\n",
        "plt.plot(model_fitting.history['val_accuracy'], label='val acc')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig('Accuracy')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}